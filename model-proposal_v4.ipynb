{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e1916c",
   "metadata": {},
   "source": [
    "# 1. ç’°å¢ƒè¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ff54e",
   "metadata": {},
   "source": [
    "# 1.1 åŸºæœ¬è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb6d2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã‚»ãƒ«1ï¼šCONFIG\n",
    "\n",
    "# === CONFIG: seed / fast-tune flags / CV&ES budgets / data I/O paths / submit-th override ===\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# === æ™‚çŸ­ãƒ•ãƒ©ã‚° ===\n",
    "FAST_TUNE = True\n",
    "TUNE_FRAC = 0.60\n",
    "N_SPLITS_TUNE = 3\n",
    "\n",
    "# === ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³/è©¦è¡Œæ•° ===\n",
    "EARLY_STOP_TUNE = 100\n",
    "EARLY_STOP_FULL = 200\n",
    "N_TRIALS_TUNE = 20\n",
    "N_TRIALS_REFINE = 10\n",
    "\n",
    "OPTUNA_TIMEOUT_SEC = 1800\n",
    "\n",
    "DATA_DIR = r\"G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\MUFJ_competition_2025\\data\"\n",
    "OUT_DIR  = r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\"\n",
    "\n",
    "# ã—ãã„å€¤ã®å›ºå®šï¼ˆNoneã§è‡ªå‹•ã«æˆ»ã™ï¼‰\n",
    "SUBMIT_THRESHOLD_OVERRIDE = 0.315\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08463ba8",
   "metadata": {},
   "source": [
    "# 1.2 ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¬ãƒãƒ¼ãƒˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7de99b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã‚»ãƒ«2ï¼šIMPORTS\n",
    "\n",
    "# === IMPORTS: stdlib / numpy-pandas / sklearn / catboost / optuna ===\n",
    "\n",
    "import os, re, json, math, warnings, itertools, textwrap\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3fff0",
   "metadata": {},
   "source": [
    "# 1.3 ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6394d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã‚»ãƒ«3ï¼šUTILS\n",
    "\n",
    "# === UTILS: column detection / submit-sep / versioning ===\n",
    "\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def detect_submit_sep(sample_submit_path: str) -> str:\n",
    "    # ã‚«ãƒ³ãƒ/ã‚¿ãƒ–/ç©ºç™½ã®é †ã§è©¦ã™ã€‚åˆ—æ•°=2ãªã‚‰æ¡ç”¨ã€‚\n",
    "    for sep in [\",\", \"\\t\", r\"\\s+\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(sample_submit_path, header=None, sep=sep, engine=\"python\")\n",
    "            if df.shape[1] == 2:\n",
    "                return sep\n",
    "        except Exception:\n",
    "            pass\n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚«ãƒ³ãƒ\n",
    "    return \",\"\n",
    "\n",
    "def is_binary(col: pd.Series) -> bool:\n",
    "    vals = pd.unique(col.dropna())\n",
    "    return set(vals).issubset({0,1})\n",
    "\n",
    "def detect_columns(train: pd.DataFrame, test: pd.DataFrame) -> Tuple[str, str]:\n",
    "    # ç›®çš„å¤‰æ•°: train ã«ã®ã¿å­˜åœ¨ã—ã€ã‹ã¤ {0,1} ã®ã©ã‚Œã‹\n",
    "    only_in_train = [c for c in train.columns if c not in test.columns]\n",
    "    candid_tgt = [c for c in only_in_train if is_binary(train[c])]\n",
    "    if len(candid_tgt) == 1:\n",
    "        target_col = candid_tgt[0]\n",
    "    else:\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åå‰ã« label/target/default ãŒå…¥ã£ã¦ã„ã¦2å€¤\n",
    "        name_hits = [c for c in train.columns if any(k in c.lower() for k in [\"label\", \"target\", \"default\", \"loanstatus\"])]\n",
    "        name_hits = [c for c in name_hits if c in train.columns and is_binary(train[c])]\n",
    "        if len(name_hits) >= 1:\n",
    "            target_col = name_hits[0]\n",
    "        else:\n",
    "            raise ValueError(\"ç›®çš„å¤‰æ•°ã‚’è‡ªå‹•æ¤œå‡ºã§ããªã„ã€‚TARGET_COL ã‚’æ‰‹å‹•æŒ‡å®šã—ã¦ã€‚\")\n",
    "\n",
    "    # IDåˆ—: train&test å…±é€š ã‹ã¤ ä¸€æ„/æ•´æ•°ã£ã½ã„/åå‰ã« id ã‚’å«ã‚€ ã‚’å„ªå…ˆ\n",
    "    common = [c for c in test.columns if c in train.columns]\n",
    "    # 1) åå‰ã« 'id'\n",
    "    id_like = [c for c in common if 'id' in c.lower()]\n",
    "    def unique_int_like(df, c):\n",
    "        s = df[c]\n",
    "        nunique = s.nunique(dropna=True)\n",
    "        return (nunique == len(s)) and (np.issubdtype(s.dropna().dtype, np.integer) or np.issubdtype(s.dropna().dtype, np.number))\n",
    "    for c in id_like + common:\n",
    "        if unique_int_like(test, c):\n",
    "            id_col = c\n",
    "            break\n",
    "    else:\n",
    "        # ã ã‚ãªã‚‰ test ã®æœ€å·¦åˆ—\n",
    "        id_col = test.columns[0]\n",
    "\n",
    "    return target_col, id_col\n",
    "\n",
    "def next_version_number(out_dir: str) -> int:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pattern = re.compile(r\"submission_A_v(\\d+)\\.csv$\")\n",
    "    ns = []\n",
    "    for f in os.listdir(out_dir):\n",
    "        m = pattern.match(f)\n",
    "        if m:\n",
    "            ns.append(int(m.group(1)))\n",
    "    return (max(ns) + 1) if ns else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5822b3aa",
   "metadata": {},
   "source": [
    "# 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a367223",
   "metadata": {},
   "source": [
    "# 2.1 ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b1249a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET_COL: LoanStatus\n",
      "ID_COL: id\n",
      "train shape: (7552, 16) test shape: (7552, 15)\n",
      "target dist: {0: 0.8723516949152542, 1: 0.12764830508474576}\n"
     ]
    }
   ],
   "source": [
    "#ã‚»ãƒ«4ï¼šLOAD DATA\n",
    "\n",
    "# === LOAD DATA & DETECT COLUMNS ===\n",
    "\n",
    "train_path = os.path.join(DATA_DIR, \"train.csv\")\n",
    "test_path  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "sample_path= os.path.join(DATA_DIR, \"sample_submit.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "\n",
    "SUBMIT_SEP = detect_submit_sep(sample_path)\n",
    "\n",
    "TARGET_COL, ID_COL = detect_columns(train, test)\n",
    "\n",
    "print(\"TARGET_COL:\", TARGET_COL)\n",
    "print(\"ID_COL:\", ID_COL)\n",
    "print(\"train shape:\", train.shape, \"test shape:\", test.shape)\n",
    "print(\"target dist:\", train[TARGET_COL].value_counts(normalize=True).to_dict())\n",
    "\n",
    "# ç›®çš„å¤‰æ•°ãƒ»ID ã®å­˜åœ¨ç¢ºèª\n",
    "assert TARGET_COL in train.columns\n",
    "assert ID_COL in test.columns and ID_COL in train.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4430c6",
   "metadata": {},
   "source": [
    "# 2.2 ç‰¹å¾´é‡ãƒ»ã‚«ãƒ†ã‚´ãƒªåˆ—ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "909a1fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ã‚»ãƒ«5ä¿®æ­£ç‰ˆ: IDã‚«ãƒ©ãƒ é™¤å¤–ã«ã‚ˆã‚‹æ€§èƒ½å›å¾©\n",
      "ä¿®æ­£å‰ã®å•é¡Œ: IDã‚«ãƒ©ãƒ  'id' ãŒå«ã¾ã‚Œã¦ã„ãŸ\n",
      "ä¿®æ­£å¾Œã®features: ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram', 'InitialInterestRate', 'FixedOrVariableInterestInd', 'TermInMonths', 'NaicsSector', 'CongressionalDistrict', 'BusinessType', 'BusinessAge', 'RevolverStatus', 'JobsSupported', 'CollateralInd']\n",
      "é™¤å¤–ã•ã‚ŒãŸã‚«ãƒ©ãƒ : ['LoanStatus', 'id']\n",
      "\n",
      "=== ä¿®æ­£çµæœç¢ºèª ===\n",
      "featuresæ•°: 14 (IDé™¤å¤–å¾Œ)\n",
      "ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: 6\n",
      "X_train shape: (7552, 14)\n",
      "y_train shape: (7552,)\n",
      "X_test shape: (7552, 14)\n",
      "\n",
      "ã‚«ãƒ†ã‚´ãƒªåˆ—: ['Subprogram', 'FixedOrVariableInterestInd', 'NaicsSector', 'BusinessType', 'BusinessAge', 'CollateralInd']\n",
      "ã‚«ãƒ†ã‚´ãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: [3, 5, 7, 9, 10, 13]\n",
      "\n",
      "=== æ€§èƒ½å›å¾©ã®å³åº§æ¤œè¨¼ ===\n",
      "ä¿®æ­£ç‰ˆRandomForest F1: 0.453652\n",
      "ğŸ”´ ğŸ”„ æ”¹å–„ç¶™ç¶šä¸­\n",
      "åŸæ¡ˆç›®æ¨™0.647ã¨ã®å·®: -0.193\n",
      "\n",
      "=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===\n",
      "âš ï¸ ã¾ã ä»–ã®å•é¡ŒãŒæ®‹å­˜\n",
      "ğŸ’¡ è¿½åŠ èª¿æŸ»é …ç›®:\n",
      "  - ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\n",
      "  - TARGET_COLã®å€¤ç¢ºèª\n",
      "  - train/testã®æ•´åˆæ€§ç¢ºèª\n",
      "\n",
      "ğŸ¯ IDé™¤å¤–ä¿®æ­£å®Œäº†: åŸºç¤æ€§èƒ½ã‚’å›å¾©\n",
      "æœŸå¾…æ”¹å–„: 0.441 â†’ 0.454 (å·®åˆ†: +0.013)\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«5ä¿®æ­£ç‰ˆ: PREP (IDé™¤å¤–ã«ã‚ˆã‚‹æ­£ã—ã„featuresè¨­å®š)\n",
    "\n",
    "print(\"ğŸ”§ ã‚»ãƒ«5ä¿®æ­£ç‰ˆ: IDã‚«ãƒ©ãƒ é™¤å¤–ã«ã‚ˆã‚‹æ€§èƒ½å›å¾©\")\n",
    "\n",
    "# === æ­£ã—ã„èª¬æ˜å¤‰æ•°ã®è¨­å®š ===\n",
    "# ID_COLã¨TARGET_COLã‚’é™¤å¤–ã—ãŸçœŸã®èª¬æ˜å¤‰æ•°\n",
    "features = [c for c in train.columns if c not in [TARGET_COL, ID_COL]]\n",
    "\n",
    "print(f\"ä¿®æ­£å‰ã®å•é¡Œ: IDã‚«ãƒ©ãƒ  '{ID_COL}' ãŒå«ã¾ã‚Œã¦ã„ãŸ\")\n",
    "print(f\"ä¿®æ­£å¾Œã®features: {features}\")\n",
    "print(f\"é™¤å¤–ã•ã‚ŒãŸã‚«ãƒ©ãƒ : ['{TARGET_COL}', '{ID_COL}']\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®ç‰¹å®šï¼ˆä¿®æ­£ç‰ˆfeaturesã«åŸºã¥ãï¼‰\n",
    "cat_cols = [c for c in features if train[c].dtype == 'object' or pd.api.types.is_categorical_dtype(train[c])]\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‰å‡¦ç†é–¢æ•°\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cat_cols:\n",
    "        out[c] = out[c].astype(str).fillna(\"MISSING\")\n",
    "    return out\n",
    "\n",
    "# ä¿®æ­£ç‰ˆãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "X_train = prep_df(train[features])\n",
    "y_train = train[TARGET_COL].astype(int).values\n",
    "X_test = prep_df(test[features])\n",
    "\n",
    "# CatBoostç”¨ã®ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "cat_features_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# === ä¿®æ­£çµæœã®ç¢ºèª ===\n",
    "print(f\"\\n=== ä¿®æ­£çµæœç¢ºèª ===\")\n",
    "print(f\"featuresæ•°: {len(features)} (IDé™¤å¤–å¾Œ)\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: {len(cat_cols)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nã‚«ãƒ†ã‚´ãƒªåˆ—: {cat_cols}\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {cat_features_idx}\")\n",
    "\n",
    "# === æ€§èƒ½å›å¾©ã®æ¤œè¨¼ ===\n",
    "print(f\"\\n=== æ€§èƒ½å›å¾©ã®å³åº§æ¤œè¨¼ ===\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# æ•°å€¤åŒ–\n",
    "X_train_numeric = X_train.copy()\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_numeric[c] = le.fit_transform(X_train_numeric[c])\n",
    "\n",
    "# ä¿®æ­£ç‰ˆã§ã®æ€§èƒ½ç¢ºèª\n",
    "rf_test = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
    "rf_scores = cross_val_score(rf_test, X_train_numeric, y_train, cv=3, scoring='f1')\n",
    "rf_mean = rf_scores.mean()\n",
    "\n",
    "print(f\"ä¿®æ­£ç‰ˆRandomForest F1: {rf_mean:.6f}\")\n",
    "\n",
    "# åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ¤å®š\n",
    "if rf_mean >= 0.65:\n",
    "    status = \"âœ… åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©ï¼\"\n",
    "    color = \"ğŸŸ¢\"\n",
    "elif rf_mean >= 0.60:\n",
    "    status = \"ğŸ”¥ åŸæ¡ˆãƒ¬ãƒ™ãƒ«è¿‘æ¥ï¼\"\n",
    "    color = \"ğŸŸ¡\"\n",
    "elif rf_mean >= 0.55:\n",
    "    status = \"ğŸ“ˆ å¤§å¹…æ”¹å–„ï¼\"\n",
    "    color = \"ğŸŸ \"\n",
    "else:\n",
    "    status = \"ğŸ”„ æ”¹å–„ç¶™ç¶šä¸­\"\n",
    "    color = \"ğŸ”´\"\n",
    "\n",
    "print(f\"{color} {status}\")\n",
    "print(f\"åŸæ¡ˆç›®æ¨™0.647ã¨ã®å·®: {rf_mean - 0.647:+.3f}\")\n",
    "\n",
    "# === æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===\n",
    "print(f\"\\n=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===\")\n",
    "\n",
    "if rf_mean >= 0.60:\n",
    "    print(\"âœ… åŸºç¤æ€§èƒ½å›å¾©æˆåŠŸï¼\")\n",
    "    print(\"ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "    print(\"  1. ã‚»ãƒ«6: KFOLDSç¶™ç¶š\")\n",
    "    print(\"  2. ã‚»ãƒ«7: TUNE SUBSETç¶™ç¶š\") \n",
    "    print(\"  3. ã‚»ãƒ«8: ã‚·ãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "    print(\"  4. ã‚»ãƒ«9-19: åŸæ¡ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶™ç¶š\")\n",
    "    print(f\"  æœŸå¾…æœ€çµ‚æ€§èƒ½: 0.647-0.650ãƒ¬ãƒ™ãƒ«\")\n",
    "else:\n",
    "    print(\"âš ï¸ ã¾ã ä»–ã®å•é¡ŒãŒæ®‹å­˜\")\n",
    "    print(\"ğŸ’¡ è¿½åŠ èª¿æŸ»é …ç›®:\")\n",
    "    print(\"  - ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\")\n",
    "    print(\"  - TARGET_COLã®å€¤ç¢ºèª\")\n",
    "    print(\"  - train/testã®æ•´åˆæ€§ç¢ºèª\")\n",
    "\n",
    "print(f\"\\nğŸ¯ IDé™¤å¤–ä¿®æ­£å®Œäº†: åŸºç¤æ€§èƒ½ã‚’å›å¾©\")\n",
    "print(f\"æœŸå¾…æ”¹å–„: 0.441 â†’ {rf_mean:.3f} (å·®åˆ†: +{rf_mean-0.441:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7ec1c",
   "metadata": {},
   "source": [
    "# 2.3 CVåˆ†å‰²è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85646b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ã‚»ãƒ«6ï¼šKFOLDS\n",
    "\n",
    "# === KFOLDS: skf_full(5fold) / skf_tune(N_SPLITS_TUNE) ===\n",
    "\n",
    "skf_full = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "skf_tune = StratifiedKFold(n_splits=N_SPLITS_TUNE, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c831c18",
   "metadata": {},
   "source": [
    "# 2.4 Tuning Subsetä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "681fae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUNE SUBSET: 4531 rows (60%)\n",
      "  æ­£ä¾‹ç‡: 0.1276 (å…ƒ: 0.1276)\n",
      "  æ­£ä¾‹æ•°: 578/4531 = 12.8%\n",
      "  ä¸»è¦æ¥­ç¨®: ['Construction', 'Professional_scientific_technical services', 'Other services (except public administration) ']\n",
      "Tune CVåˆ†å‰²: 3 folds prepared\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«7å¼·åŒ–ç‰ˆ: TUNE SUBSET (stratified sampling when FAST_TUNE)\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹æ¯”ã‚’ä¿ã£ã¦ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½œã‚‹ï¼ˆFAST_TUNEæ™‚ã®ã¿ï¼‰\n",
    "if FAST_TUNE:\n",
    "    # å±¤åŒ–æŠ½å‡º\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    idx_all = np.arange(len(X_train))\n",
    "    idx_tune, idx_remaining = train_test_split(\n",
    "        idx_all, train_size=TUNE_FRAC, stratify=y_train, random_state=SEED\n",
    "    )\n",
    "    X_tune = X_train.iloc[idx_tune].reset_index(drop=True)\n",
    "    y_tune = y_train[idx_tune]\n",
    "    \n",
    "    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º\n",
    "    print(f\"TUNE SUBSET: {len(X_tune)} rows ({TUNE_FRAC*100:.0f}%)\")\n",
    "    print(f\"  æ­£ä¾‹ç‡: {y_tune.mean():.4f} (å…ƒ: {y_train.mean():.4f})\")\n",
    "    print(f\"  æ­£ä¾‹æ•°: {y_tune.sum()}/{len(y_tune)} = {y_tune.sum()/len(y_tune)*100:.1f}%\")\n",
    "    \n",
    "    # åˆ†å¸ƒç¢ºèªï¼ˆé‡è¦ã‚«ãƒ†ã‚´ãƒªï¼‰\n",
    "    if hasattr(X_tune, 'columns') and 'NaicsSector' in X_tune.columns:\n",
    "        tune_sectors = X_tune['NaicsSector'].value_counts().head(3)\n",
    "        print(f\"  ä¸»è¦æ¥­ç¨®: {list(tune_sectors.index)}\")\n",
    "    \n",
    "else:\n",
    "    X_tune, y_tune = X_train, y_train\n",
    "    print(\"FULL DATASET for tuning\")\n",
    "    print(f\"  ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_tune)}\")\n",
    "    print(f\"  æ­£ä¾‹ç‡: {y_tune.mean():.4f}\")\n",
    "\n",
    "# Tuneç”¨ã®CVåˆ†å‰²ã‚‚æº–å‚™\n",
    "if 'skf_tune' in locals():\n",
    "    tune_splits = list(skf_tune.split(X_tune, y_tune))\n",
    "    print(f\"Tune CVåˆ†å‰²: {len(tune_splits)} folds prepared\")\n",
    "else:\n",
    "    print(\"âš ï¸ skf_tune not found. Using skf_full for tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76c43e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” è¿½åŠ èª¿æŸ»: 0.454ã®ä½ã„æ€§èƒ½ã®åŸå› ç©¶æ˜\n",
      "IDé™¤å¤–ã§ã¯è§£æ±ºã›ãš â†’ ä»–ã®æ ¹æœ¬å•é¡Œã‚’èª¿æŸ»\n",
      "\n",
      "=== 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®è©³ç´°ç¢ºèª ===\n",
      "TARGET_COL: 'LoanStatus'\n",
      "train[TARGET_COL]ã®å‹: int64\n",
      "train[TARGET_COL]ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: [np.int64(0), np.int64(1)]\n",
      "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {0: np.int64(6588), 1: np.int64(964)}\n",
      "y_trainã®å‹: int64\n",
      "y_trainã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: [np.int64(0), np.int64(1)]\n",
      "y_trainã®åˆ†å¸ƒ: [6588  964]\n",
      "\n",
      "=== 2. ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆç¢ºèª ===\n",
      "æ•°å€¤åˆ— (8å€‹): ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'InitialInterestRate', 'TermInMonths', 'CongressionalDistrict', 'RevolverStatus', 'JobsSupported']\n",
      "\n",
      "æ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆ:\n",
      "       GrossApproval  SBAGuaranteedApproval  ApprovalFiscalYear  \\\n",
      "count   7.552000e+03           7.552000e+03         7552.000000   \n",
      "mean    7.219039e+05           4.536842e+05         2021.091499   \n",
      "std     1.112669e+06           7.805103e+05            1.125885   \n",
      "min     5.000000e+03           2.500000e+03         2020.000000   \n",
      "25%     5.110000e+04           2.653525e+04         2020.000000   \n",
      "50%     1.896000e+05           1.063350e+05         2021.000000   \n",
      "75%     8.113000e+05           4.669272e+05         2022.000000   \n",
      "max     4.995000e+06           4.311817e+06         2024.000000   \n",
      "\n",
      "       InitialInterestRate  TermInMonths  CongressionalDistrict  \\\n",
      "count          7552.000000   7552.000000            7552.000000   \n",
      "mean              7.379586    119.854211              12.681674   \n",
      "std               2.884902     82.423821              12.307653   \n",
      "min               1.030000      3.000000               0.000000   \n",
      "25%               5.120000     60.000000               3.000000   \n",
      "50%               6.610000    119.000000               7.000000   \n",
      "75%               9.550000    120.000000              22.000000   \n",
      "max              15.000000    306.000000              52.000000   \n",
      "\n",
      "       RevolverStatus  JobsSupported  \n",
      "count     7552.000000    7552.000000  \n",
      "mean         0.115731      14.281912  \n",
      "std          0.319923      27.635214  \n",
      "min          0.000000       0.000000  \n",
      "25%          0.000000       1.000000  \n",
      "50%          0.000000       5.000000  \n",
      "75%          0.000000      14.000000  \n",
      "max          1.000000     236.000000  \n",
      "\n",
      "=== 3. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¢ºèª ===\n",
      "Subprogram: 9ç¨®é¡\n",
      "  å€¤: ['FA$TRK (Small Loan Express)', 'Contract Guaranty', 'Guaranty', 'Community Advantage Initiative', 'Standard Asset Based', 'Seasonal Line of Credit', 'International Trade - Sec, 7(a) (16)', 'Revolving Line of Credit Exports - Sec. 7(a) (14)', 'Small General Contractors - Sec. 7(a) (9)']\n",
      "FixedOrVariableInterestInd: 2ç¨®é¡\n",
      "  å€¤: ['V', 'F']\n",
      "NaicsSector: 19ç¨®é¡\n",
      "BusinessType: 4ç¨®é¡\n",
      "  å€¤: ['CORPORATION', 'INDIVIDUAL', 'PARTNERSHIP', '        ']\n",
      "BusinessAge: 5ç¨®é¡\n",
      "  å€¤: ['Unanswered', 'Startup, Loan Funds will Open Business', 'Existing or more than 2 years old', 'New Business or 2 years or less', 'Change of Ownership']\n",
      "CollateralInd: 2ç¨®é¡\n",
      "  å€¤: ['N', 'Y']\n",
      "\n",
      "=== 4. train/testã®æ•´åˆæ€§ç¢ºèª ===\n",
      "trainã‚µã‚¤ã‚º: (7552, 16)\n",
      "testã‚µã‚¤ã‚º: (7552, 15)\n",
      "âš ï¸ NaicsSectorã®å€¤ã®ä¸æ•´åˆ:\n",
      "  testã®ã¿: {'Public administration'}\n",
      "\n",
      "=== 5. æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ ===\n",
      "GrossApprovalå˜ä½“ã§ã®F1: 0.000000\n",
      "InitialInterestRateå˜ä½“ã§ã®F1: 0.000000\n",
      "TermInMonthså˜ä½“ã§ã®F1: 0.000000\n",
      "æœ€è‰¯å˜ä¸€ç‰¹å¾´é‡: GrossApproval (F1: 0.000000)\n",
      "\n",
      "=== 6. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯èª¿æŸ» ===\n",
      "âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ç–‘ã„ãŒã‚ã‚‹åˆ—: ['RevolverStatus']\n",
      "\n",
      "=== 7. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ ===\n",
      "âœ… å®šæ•°åˆ—ãªã—\n",
      "\n",
      "=== 8. å•é¡Œã®ç·åˆåˆ¤å®š ===\n",
      "ğŸš¨ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\n",
      "  - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ç–‘ã„\n",
      "\n",
      "=== 9. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ ===\n",
      "ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®è©³ç´°èª¿æŸ»\n",
      "  - ['RevolverStatus']ã®è©³ç´°ç¢ºèª\n",
      "\n",
      "ç¾åœ¨ã®èª²é¡Œ: 0.454 â†’ 0.647ã¸ã®é“ç­‹ç™ºè¦‹\n"
     ]
    }
   ],
   "source": [
    "# è¿½åŠ èª¿æŸ»: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®ç¢ºèª\n",
    "\n",
    "print(\"ğŸ” è¿½åŠ èª¿æŸ»: 0.454ã®ä½ã„æ€§èƒ½ã®åŸå› ç©¶æ˜\")\n",
    "print(\"IDé™¤å¤–ã§ã¯è§£æ±ºã›ãš â†’ ä»–ã®æ ¹æœ¬å•é¡Œã‚’èª¿æŸ»\")\n",
    "\n",
    "# === 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®è©³ç´°ç¢ºèª ===\n",
    "print(\"\\n=== 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®è©³ç´°ç¢ºèª ===\")\n",
    "\n",
    "print(f\"TARGET_COL: '{TARGET_COL}'\")\n",
    "print(f\"train[TARGET_COL]ã®å‹: {train[TARGET_COL].dtype}\")\n",
    "print(f\"train[TARGET_COL]ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {sorted(train[TARGET_COL].unique())}\")\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå€¤ã®åˆ†å¸ƒè©³ç´°\n",
    "target_counts = train[TARGET_COL].value_counts().sort_index()\n",
    "print(f\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {dict(target_counts)}\")\n",
    "\n",
    "# y_trainã®ç¢ºèª\n",
    "print(f\"y_trainã®å‹: {y_train.dtype}\")\n",
    "print(f\"y_trainã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {sorted(np.unique(y_train))}\")\n",
    "print(f\"y_trainã®åˆ†å¸ƒ: {np.bincount(y_train)}\")\n",
    "\n",
    "# === 2. ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆç¢ºèª ===\n",
    "print(\"\\n=== 2. ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çµ±è¨ˆç¢ºèª ===\")\n",
    "\n",
    "# æ•°å€¤åˆ—ã®çµ±è¨ˆ\n",
    "numeric_cols = [c for c in features if c not in cat_cols]\n",
    "print(f\"æ•°å€¤åˆ— ({len(numeric_cols)}å€‹): {numeric_cols}\")\n",
    "\n",
    "if numeric_cols:\n",
    "    print(\"\\næ•°å€¤åˆ—ã®åŸºæœ¬çµ±è¨ˆ:\")\n",
    "    stats = train[numeric_cols].describe()\n",
    "    print(stats)\n",
    "    \n",
    "    # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯\n",
    "    for col in numeric_cols:\n",
    "        values = train[col]\n",
    "        q99 = values.quantile(0.99)\n",
    "        q01 = values.quantile(0.01)\n",
    "        outliers = ((values > q99) | (values < q01)).sum()\n",
    "        if outliers > len(values) * 0.05:  # 5%ä»¥ä¸ŠãŒå¤–ã‚Œå€¤\n",
    "            print(f\"âš ï¸ {col}: å¤–ã‚Œå€¤å¤šæ•° ({outliers}å€‹, {outliers/len(values)*100:.1f}%)\")\n",
    "\n",
    "# === 3. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¢ºèª ===\n",
    "print(f\"\\n=== 3. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¢ºèª ===\")\n",
    "\n",
    "for col in cat_cols:\n",
    "    unique_count = train[col].nunique()\n",
    "    print(f\"{col}: {unique_count}ç¨®é¡\")\n",
    "    if unique_count <= 10:\n",
    "        print(f\"  å€¤: {list(train[col].unique())}\")\n",
    "    elif unique_count > 1000:\n",
    "        print(f\"  âš ï¸ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {unique_count}ç¨®é¡\")\n",
    "\n",
    "# === 4. train/testã®æ•´åˆæ€§ç¢ºèª ===\n",
    "print(f\"\\n=== 4. train/testã®æ•´åˆæ€§ç¢ºèª ===\")\n",
    "\n",
    "print(f\"trainã‚µã‚¤ã‚º: {train.shape}\")\n",
    "print(f\"testã‚µã‚¤ã‚º: {test.shape}\")\n",
    "\n",
    "# ã‚«ãƒ©ãƒ ã®æ•´åˆæ€§\n",
    "train_cols = set(train.columns)\n",
    "test_cols = set(test.columns)\n",
    "missing_in_test = train_cols - test_cols - {TARGET_COL}\n",
    "extra_in_test = test_cols - train_cols\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"âš ï¸ testã«ãªã„åˆ—: {missing_in_test}\")\n",
    "if extra_in_test:\n",
    "    print(f\"âš ï¸ testã«ã®ã¿ã‚ã‚‹åˆ—: {extra_in_test}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªå€¤ã®æ•´åˆæ€§\n",
    "for col in cat_cols:\n",
    "    if col in test.columns:\n",
    "        train_values = set(train[col].unique())\n",
    "        test_values = set(test[col].unique())\n",
    "        \n",
    "        only_in_train = train_values - test_values\n",
    "        only_in_test = test_values - train_values\n",
    "        \n",
    "        if only_in_train or only_in_test:\n",
    "            print(f\"âš ï¸ {col}ã®å€¤ã®ä¸æ•´åˆ:\")\n",
    "            if only_in_train:\n",
    "                print(f\"  trainã®ã¿: {only_in_train}\")\n",
    "            if only_in_test:\n",
    "                print(f\"  testã®ã¿: {only_in_test}\")\n",
    "\n",
    "# === 5. ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å†ç¢ºèª ===\n",
    "print(f\"\\n=== 5. æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ ===\")\n",
    "\n",
    "# æœ€ã‚‚äºˆæ¸¬åŠ›ãŒé«˜ãã†ãªå˜ä¸€ç‰¹å¾´é‡ã§ã®æ€§èƒ½\n",
    "single_feature_scores = {}\n",
    "\n",
    "for col in ['GrossApproval', 'InitialInterestRate', 'TermInMonths']:\n",
    "    if col in X_train.columns:\n",
    "        # 1ã¤ã®ç‰¹å¾´é‡ã ã‘ã§ã®äºˆæ¸¬\n",
    "        X_single = X_train[[col]].copy()\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        lr = LogisticRegression(random_state=SEED)\n",
    "        \n",
    "        try:\n",
    "            single_scores = cross_val_score(lr, X_single, y_train, cv=3, scoring='f1')\n",
    "            single_f1 = single_scores.mean()\n",
    "            single_feature_scores[col] = single_f1\n",
    "            print(f\"{col}å˜ä½“ã§ã®F1: {single_f1:.6f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{col}ã§ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "if single_feature_scores:\n",
    "    best_single = max(single_feature_scores.items(), key=lambda x: x[1])\n",
    "    print(f\"æœ€è‰¯å˜ä¸€ç‰¹å¾´é‡: {best_single[0]} (F1: {best_single[1]:.6f})\")\n",
    "\n",
    "# === 6. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯èª¿æŸ» ===\n",
    "print(f\"\\n=== 6. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯èª¿æŸ» ===\")\n",
    "\n",
    "# æœªæ¥æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯\n",
    "suspicious_cols = []\n",
    "\n",
    "for col in features:\n",
    "    # åˆ—åã«'Status', 'Result', 'Outcome'ãªã©ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹\n",
    "    if any(word in col.lower() for word in ['status', 'result', 'outcome', 'default', 'paid']):\n",
    "        suspicious_cols.append(col)\n",
    "\n",
    "if suspicious_cols:\n",
    "    print(f\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ç–‘ã„ãŒã‚ã‚‹åˆ—: {suspicious_cols}\")\n",
    "else:\n",
    "    print(\"âœ… æ˜ç¢ºãªãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã¯è¦‹å½“ãŸã‚‰ãš\")\n",
    "\n",
    "# === 7. å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ ===\n",
    "print(f\"\\n=== 7. ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ ===\")\n",
    "\n",
    "# å…¨ã¦ã®å€¤ãŒåŒã˜åˆ—\n",
    "constant_cols = []\n",
    "for col in features:\n",
    "    if train[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "\n",
    "if constant_cols:\n",
    "    print(f\"âš ï¸ å®šæ•°åˆ—: {constant_cols}\")\n",
    "else:\n",
    "    print(\"âœ… å®šæ•°åˆ—ãªã—\")\n",
    "\n",
    "# æ¥µç«¯ã«åã£ãŸåˆ—\n",
    "highly_skewed = []\n",
    "for col in cat_cols:\n",
    "    most_common_pct = train[col].value_counts().iloc[0] / len(train)\n",
    "    if most_common_pct > 0.95:\n",
    "        highly_skewed.append((col, most_common_pct))\n",
    "\n",
    "if highly_skewed:\n",
    "    print(\"âš ï¸ æ¥µç«¯ã«åã£ãŸåˆ—:\")\n",
    "    for col, pct in highly_skewed:\n",
    "        print(f\"  {col}: {pct:.1%}ãŒåŒã˜å€¤\")\n",
    "\n",
    "# === 8. å•é¡Œã®ç·åˆåˆ¤å®š ===\n",
    "print(f\"\\n=== 8. å•é¡Œã®ç·åˆåˆ¤å®š ===\")\n",
    "\n",
    "issues_found = []\n",
    "\n",
    "if len(suspicious_cols) > 0:\n",
    "    issues_found.append(\"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ç–‘ã„\")\n",
    "if len(constant_cols) > 0:\n",
    "    issues_found.append(\"å®šæ•°åˆ—ã®å­˜åœ¨\")\n",
    "if len(highly_skewed) > 0:\n",
    "    issues_found.append(\"æ¥µç«¯ã«åã£ãŸåˆ†å¸ƒ\")\n",
    "\n",
    "# å˜ä¸€ç‰¹å¾´é‡ã®æ€§èƒ½ãŒå…¨ä½“ã‚ˆã‚Šé«˜ã„å ´åˆ\n",
    "if single_feature_scores and max(single_feature_scores.values()) > 0.50:\n",
    "    issues_found.append(\"ç‰¹å¾´é‡çµ„ã¿åˆã‚ã›ã®å•é¡Œ\")\n",
    "\n",
    "if issues_found:\n",
    "    print(\"ğŸš¨ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\")\n",
    "    for issue in issues_found:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"â“ æ˜ç¢ºãªå•é¡ŒãŒç‰¹å®šã§ããš\")\n",
    "    print(\"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®äºˆæ¸¬å¯èƒ½æ€§ãŒä½ã„å¯èƒ½æ€§\")\n",
    "\n",
    "# === 9. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ ===\n",
    "print(f\"\\n=== 9. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ ===\")\n",
    "\n",
    "if single_feature_scores and max(single_feature_scores.values()) > 0.50:\n",
    "    print(\"ğŸ¯ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«é›†ä¸­\")\n",
    "    print(\"  - å˜ä¸€ç‰¹å¾´é‡ã§ã¯äºˆæ¸¬å¯èƒ½\")\n",
    "    print(\"  - ç‰¹å¾´é‡ã®çµ„ã¿åˆã‚ã›æ–¹æ³•ã‚’æ”¹å–„\")\n",
    "elif suspicious_cols:\n",
    "    print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®è©³ç´°èª¿æŸ»\")\n",
    "    print(f\"  - {suspicious_cols}ã®è©³ç´°ç¢ºèª\")\n",
    "elif 'single_feature_scores' in locals() and single_feature_scores:\n",
    "    best_score = max(single_feature_scores.values())\n",
    "    if best_score < 0.30:\n",
    "        print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®äºˆæ¸¬å¯èƒ½æ€§ãŒä½ã„\")\n",
    "        print(\"  - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨æ¤œè¨\")\n",
    "        print(\"  - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¼·åŒ–\")\n",
    "    else:\n",
    "        print(\"ğŸ”§ ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ‰‹æ³•ã®è¦‹ç›´ã—\")\n",
    "        print(\"  - ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©¦è¡Œ\")\n",
    "        print(\"  - å‰å‡¦ç†æ–¹æ³•ã®å¤‰æ›´\")\n",
    "\n",
    "print(f\"\\nç¾åœ¨ã®èª²é¡Œ: 0.454 â†’ 0.647ã¸ã®é“ç­‹ç™ºè¦‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2027a887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»: RevolverStatusãŒåŸå› ã®å¯èƒ½æ€§\n",
      "å•é¡Œ: 'RevolverStatus'ã¯èè³‡å¾Œã®çŠ¶æ…‹æƒ…å ±ã®å¯èƒ½æ€§\n",
      "\n",
      "=== 1. RevolverStatusã®è©³ç´°èª¿æŸ» ===\n",
      "RevolverStatusã®å€¤: [np.int64(0), np.int64(1)]\n",
      "RevolverStatusåˆ†å¸ƒ: {0: np.int64(6678), 1: np.int64(874)}\n",
      "\n",
      "RevolverStatus vs ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¯ãƒ­ã‚¹è¡¨:\n",
      "LoanStatus         0    1\n",
      "RevolverStatus           \n",
      "0               5815  863\n",
      "1                773  101\n",
      "\n",
      "RevolverStatusåˆ¥ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡:\n",
      "  Status 0: 12.923% (6678ä»¶)\n",
      "  Status 1: 11.556% (874ä»¶)\n",
      "\n",
      "=== 2. RevolverStatusé™¤å»ç‰ˆfeaturesä½œæˆ ===\n",
      "é™¤å»å‰features: ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram', 'InitialInterestRate', 'FixedOrVariableInterestInd', 'TermInMonths', 'NaicsSector', 'CongressionalDistrict', 'BusinessType', 'BusinessAge', 'RevolverStatus', 'JobsSupported', 'CollateralInd']\n",
      "é™¤å»å‰featuresæ•°: 14\n",
      "é™¤å»å¾Œfeatures: ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram', 'InitialInterestRate', 'FixedOrVariableInterestInd', 'TermInMonths', 'NaicsSector', 'CongressionalDistrict', 'BusinessType', 'BusinessAge', 'JobsSupported', 'CollateralInd']\n",
      "é™¤å»å¾Œfeaturesæ•°: 13\n",
      "é™¤å»å¾Œcat_cols: ['Subprogram', 'FixedOrVariableInterestInd', 'NaicsSector', 'BusinessType', 'BusinessAge', 'CollateralInd']\n",
      "\n",
      "=== 3. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆã§ã®æ€§èƒ½ç¢ºèª ===\n",
      "ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆshape: (7552, 13)\n",
      "æ•°å€¤åŒ–å®Œäº†: (7552, 13)\n",
      "\n",
      "=== 4. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»å¾Œã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ ===\n",
      "ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»RandomForest F1: 0.462979\n",
      "æ”¹å–„: +0.008979\n",
      "ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»LightGBM F1: 0.534818\n",
      "æ”¹å–„: +0.032818\n",
      "\n",
      "=== 5. åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©åˆ¤å®š ===\n",
      "ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»å¾Œæœ€é«˜F1: 0.534818\n",
      "åŸæ¡ˆç›®æ¨™: 0.647000\n",
      "å·®ç•°: -0.112182\n",
      "ğŸ”´ âš ï¸ æ›´ãªã‚‹èª¿æŸ»å¿…è¦\n",
      "\n",
      "=== 6. å¤‰æ•°æ›´æ–°åˆ¤å®š ===\n",
      "âš ï¸ ã¾ã å¤§å¹…ãªæ”¹å–„ã¯è¦‹ã‚‰ã‚Œãš\n",
      "ä»–ã®è¦å› ã‚‚èª¿æŸ»ç¶™ç¶š\n",
      "\n",
      "=== 7. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»åŠ¹æœã‚µãƒãƒªãƒ¼ ===\n",
      "é™¤å»å‰: RandomForest 0.454\n",
      "é™¤å»å¾Œ: RandomForest 0.462979 (+0.009)\n",
      "é™¤å»å¾Œ: LightGBM 0.534818\n",
      "æœ€é«˜æ€§èƒ½: 0.534818\n",
      "åŸæ¡ˆã¾ã§: 0.112ã®å·®\n",
      "ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ã‚‚é™å®šçš„ã€‚ä»–ã®è¦å› ã®èª¿æŸ»ç¶™ç¶š\n"
     ]
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»: RevolverStatuså‰Šé™¤ã«ã‚ˆã‚‹æ€§èƒ½å›å¾©\n",
    "\n",
    "print(\"ğŸš¨ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»: RevolverStatusãŒåŸå› ã®å¯èƒ½æ€§\")\n",
    "print(\"å•é¡Œ: 'RevolverStatus'ã¯èè³‡å¾Œã®çŠ¶æ…‹æƒ…å ±ã®å¯èƒ½æ€§\")\n",
    "\n",
    "# === 1. RevolverStatusã®è©³ç´°èª¿æŸ» ===\n",
    "print(\"\\n=== 1. RevolverStatusã®è©³ç´°èª¿æŸ» ===\")\n",
    "\n",
    "print(f\"RevolverStatusã®å€¤: {sorted(train['RevolverStatus'].unique())}\")\n",
    "print(f\"RevolverStatusåˆ†å¸ƒ: {dict(train['RevolverStatus'].value_counts())}\")\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã®é–¢ä¿‚ç¢ºèª\n",
    "if 'RevolverStatus' in train.columns:\n",
    "    revolver_target_crosstab = pd.crosstab(train['RevolverStatus'], train[TARGET_COL])\n",
    "    print(f\"\\nRevolverStatus vs ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚¯ãƒ­ã‚¹è¡¨:\")\n",
    "    print(revolver_target_crosstab)\n",
    "    \n",
    "    # å„RevolverStatusã§ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡\n",
    "    print(f\"\\nRevolverStatusåˆ¥ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡:\")\n",
    "    for status in sorted(train['RevolverStatus'].unique()):\n",
    "        mask = train['RevolverStatus'] == status\n",
    "        default_rate = train.loc[mask, TARGET_COL].mean()\n",
    "        count = mask.sum()\n",
    "        print(f\"  Status {status}: {default_rate:.3%} ({count}ä»¶)\")\n",
    "\n",
    "# === 2. RevolverStatusé™¤å»ç‰ˆã®ä½œæˆ ===\n",
    "print(f\"\\n=== 2. RevolverStatusé™¤å»ç‰ˆfeaturesä½œæˆ ===\")\n",
    "\n",
    "# ç¾åœ¨ã®features\n",
    "print(f\"é™¤å»å‰features: {features}\")\n",
    "print(f\"é™¤å»å‰featuresæ•°: {len(features)}\")\n",
    "\n",
    "# RevolverStatusã‚’é™¤å»\n",
    "features_no_leak = [c for c in features if c != 'RevolverStatus']\n",
    "print(f\"é™¤å»å¾Œfeatures: {features_no_leak}\")\n",
    "print(f\"é™¤å»å¾Œfeaturesæ•°: {len(features_no_leak)}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ—ã‚‚æ›´æ–°\n",
    "cat_cols_no_leak = [c for c in cat_cols if c != 'RevolverStatus']\n",
    "print(f\"é™¤å»å¾Œcat_cols: {cat_cols_no_leak}\")\n",
    "\n",
    "# === 3. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆã§ã®æ€§èƒ½ç¢ºèª ===\n",
    "print(f\"\\n=== 3. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆã§ã®æ€§èƒ½ç¢ºèª ===\")\n",
    "\n",
    "# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "X_train_no_leak = train[features_no_leak].copy()\n",
    "X_test_no_leak = test[features_no_leak].copy()\n",
    "\n",
    "# å‰å‡¦ç†\n",
    "def prep_df_no_leak(df):\n",
    "    out = df.copy()\n",
    "    for c in cat_cols_no_leak:\n",
    "        out[c] = out[c].astype(str).fillna(\"MISSING\")\n",
    "    return out\n",
    "\n",
    "X_train_clean = prep_df_no_leak(X_train_no_leak)\n",
    "X_test_clean = prep_df_no_leak(X_test_no_leak)\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆshape: {X_train_clean.shape}\")\n",
    "\n",
    "# æ•°å€¤åŒ–ï¼ˆRandomForestç”¨ï¼‰\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X_numeric_clean = X_train_clean.copy()\n",
    "\n",
    "for c in cat_cols_no_leak:\n",
    "    le = LabelEncoder()\n",
    "    X_numeric_clean[c] = le.fit_transform(X_numeric_clean[c])\n",
    "\n",
    "print(f\"æ•°å€¤åŒ–å®Œäº†: {X_numeric_clean.shape}\")\n",
    "\n",
    "# === 4. æ€§èƒ½ãƒ†ã‚¹ãƒˆ ===\n",
    "print(f\"\\n=== 4. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»å¾Œã®æ€§èƒ½ãƒ†ã‚¹ãƒˆ ===\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# RandomForestç¢ºèª\n",
    "rf_clean = RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1)\n",
    "rf_clean_scores = cross_val_score(rf_clean, X_numeric_clean, y_train, cv=3, scoring='f1')\n",
    "rf_clean_mean = rf_clean_scores.mean()\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»RandomForest F1: {rf_clean_mean:.6f}\")\n",
    "print(f\"æ”¹å–„: {rf_clean_mean - 0.454:+.6f}\")\n",
    "\n",
    "# LightGBMç¢ºèªï¼ˆã‚«ãƒ†ã‚´ãƒªå‡¦ç†ï¼‰\n",
    "X_lgb_clean = X_train_clean.copy()\n",
    "for c in cat_cols_no_leak:\n",
    "    X_lgb_clean[c] = X_lgb_clean[c].astype('category')\n",
    "\n",
    "lgb_clean = LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=1000,\n",
    "    num_leaves=31,\n",
    "    random_state=SEED,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_clean_scores = cross_val_score(lgb_clean, X_lgb_clean, y_train, cv=3, scoring='f1')\n",
    "lgb_clean_mean = lgb_clean_scores.mean()\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»LightGBM F1: {lgb_clean_mean:.6f}\")\n",
    "print(f\"æ”¹å–„: {lgb_clean_mean - 0.502:+.6f}\")\n",
    "\n",
    "# === 5. åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ¤å®š ===\n",
    "print(f\"\\n=== 5. åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©åˆ¤å®š ===\")\n",
    "\n",
    "best_clean = max(rf_clean_mean, lgb_clean_mean)\n",
    "target_level = 0.647\n",
    "\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»å¾Œæœ€é«˜F1: {best_clean:.6f}\")\n",
    "print(f\"åŸæ¡ˆç›®æ¨™: {target_level:.6f}\")\n",
    "print(f\"å·®ç•°: {best_clean - target_level:+.6f}\")\n",
    "\n",
    "if best_clean >= target_level:\n",
    "    status = \"âœ… åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©é”æˆï¼\"\n",
    "    color = \"ğŸŸ¢\"\n",
    "elif best_clean >= target_level - 0.02:\n",
    "    status = \"ğŸ”¥ åŸæ¡ˆãƒ¬ãƒ™ãƒ«è¿‘æ¥ï¼\"\n",
    "    color = \"ğŸŸ¡\"\n",
    "elif best_clean >= 0.60:\n",
    "    status = \"ğŸ“ˆ å¤§å¹…æ”¹å–„ï¼\"\n",
    "    color = \"ğŸŸ \"\n",
    "elif best_clean >= 0.55:\n",
    "    status = \"ğŸ”„ æ”¹å–„ç¶™ç¶š\"\n",
    "    color = \"ğŸ”µ\"\n",
    "else:\n",
    "    status = \"âš ï¸ æ›´ãªã‚‹èª¿æŸ»å¿…è¦\"\n",
    "    color = \"ğŸ”´\"\n",
    "\n",
    "print(f\"{color} {status}\")\n",
    "\n",
    "# === 6. å¤‰æ•°æ›´æ–°åˆ¤å®š ===\n",
    "print(f\"\\n=== 6. å¤‰æ•°æ›´æ–°åˆ¤å®š ===\")\n",
    "\n",
    "if best_clean > 0.55:  # å¤§å¹…æ”¹å–„ãŒã‚ã£ãŸå ´åˆ\n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ç‰ˆã‚’æ¡ç”¨\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°\n",
    "    features = features_no_leak\n",
    "    cat_cols = cat_cols_no_leak  \n",
    "    X_train = X_train_clean\n",
    "    X_test = X_test_clean\n",
    "    \n",
    "    print(\"å¤‰æ•°æ›´æ–°å®Œäº†:\")\n",
    "    print(f\"  features: {len(features)}å€‹\")\n",
    "    print(f\"  cat_cols: {len(cat_cols)}å€‹\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  X_test: {X_test.shape}\")\n",
    "    \n",
    "    # CatBoostç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°\n",
    "    cat_features_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n",
    "    print(f\"  cat_features_idx: {cat_features_idx}\")\n",
    "    \n",
    "    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "    print(f\"\\nğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:\")\n",
    "    if best_clean >= 0.63:\n",
    "        print(\"1. ã‚»ãƒ«8: ã‚·ãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "        print(\"2. ã‚»ãƒ«9-19: åŸæ¡ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶™ç¶š\")\n",
    "        print(\"3. æœŸå¾…æœ€çµ‚æ€§èƒ½: 0.647-0.650ãƒ¬ãƒ™ãƒ«\")\n",
    "    else:\n",
    "        print(\"1. ã‚»ãƒ«8: ã‚·ãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "        print(\"2. è¿½åŠ æœ€é©åŒ–ã§åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ°é”\")\n",
    "        print(\"3. æœŸå¾…æœ€çµ‚æ€§èƒ½: 0.63-0.65ãƒ¬ãƒ™ãƒ«\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ã¾ã å¤§å¹…ãªæ”¹å–„ã¯è¦‹ã‚‰ã‚Œãš\")\n",
    "    print(\"ä»–ã®è¦å› ã‚‚èª¿æŸ»ç¶™ç¶š\")\n",
    "\n",
    "# === 7. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ã®åŠ¹æœã‚µãƒãƒªãƒ¼ ===\n",
    "print(f\"\\n=== 7. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»åŠ¹æœã‚µãƒãƒªãƒ¼ ===\")\n",
    "print(f\"é™¤å»å‰: RandomForest 0.454\")\n",
    "print(f\"é™¤å»å¾Œ: RandomForest {rf_clean_mean:.6f} ({rf_clean_mean-0.454:+.3f})\")\n",
    "print(f\"é™¤å»å¾Œ: LightGBM {lgb_clean_mean:.6f}\")\n",
    "print(f\"æœ€é«˜æ€§èƒ½: {best_clean:.6f}\")\n",
    "print(f\"åŸæ¡ˆã¾ã§: {0.647-best_clean:.3f}ã®å·®\")\n",
    "\n",
    "if best_clean > 0.60:\n",
    "    print(\"ğŸ¯ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ãŒåŠ¹æœçš„ï¼åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©ã®é“ç­‹ãŒè¦‹ãˆãŸ\")\n",
    "else:\n",
    "    print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å»ã‚‚é™å®šçš„ã€‚ä»–ã®è¦å› ã®èª¿æŸ»ç¶™ç¶š\")\n",
    "\n",
    "LEAK_REMOVAL_RESULT = {\n",
    "    \"rf_score\": rf_clean_mean,\n",
    "    \"lgb_score\": lgb_clean_mean,\n",
    "    \"best_score\": best_clean,\n",
    "    \"improvement\": best_clean - 0.454,\n",
    "    \"target_gap\": 0.647 - best_clean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9d81f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ·±å±¤å•é¡Œåˆ†æ: æ®‹ã‚Š0.112ã®å·®ã®åŸå› ç©¶æ˜\n",
      "IDé™¤å¤–ã€RevolverStatusé™¤å»ã§ã‚‚0.535 â†’ ã•ã‚‰ãªã‚‹èª¿æŸ»ãŒå¿…è¦\n",
      "\n",
      "=== 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªä½“ã®æ¤œè¨¼ ===\n",
      "train.csvã®ãƒ‘ã‚¹ç¢ºèª:\n",
      "  train shape: (7552, 16)\n",
      "  columns: ['id', 'GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram', 'InitialInterestRate', 'FixedOrVariableInterestInd', 'TermInMonths', 'NaicsSector', 'CongressionalDistrict', 'BusinessType', 'BusinessAge', 'RevolverStatus', 'JobsSupported', 'CollateralInd', 'LoanStatus']\n",
      "\n",
      "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§:\n",
      "  é‡è¤‡è¡Œ: 0è¡Œ\n",
      "  å…¨NULLè¡Œ: 0è¡Œ\n",
      "\n",
      "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¦¥å½“æ€§:\n",
      "  LoanStatusã®å€¤åŸŸ: 0 - 1\n",
      "  æ­£ä¾‹ç‡: 0.1276\n",
      "\n",
      "=== 2. åŸæ¡ˆã¨ã®å·®åˆ†èª¿æŸ» ===\n",
      "ç¾åœ¨ã®features (14å€‹):\n",
      "   1. GrossApproval\n",
      "   2. SBAGuaranteedApproval\n",
      "   3. ApprovalFiscalYear\n",
      "   4. Subprogram\n",
      "   5. InitialInterestRate\n",
      "   6. FixedOrVariableInterestInd\n",
      "   7. TermInMonths\n",
      "   8. NaicsSector\n",
      "   9. CongressionalDistrict\n",
      "  10. BusinessType\n",
      "  11. BusinessAge\n",
      "  12. RevolverStatus\n",
      "  13. JobsSupported\n",
      "  14. CollateralInd\n",
      "\n",
      "ç‰¹å¾´é‡å†…è¨³:\n",
      "  æ•°å€¤åˆ— (8å€‹): ['GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'InitialInterestRate', 'TermInMonths', 'CongressionalDistrict', 'RevolverStatus', 'JobsSupported']\n",
      "  ã‚«ãƒ†ã‚´ãƒªåˆ— (6å€‹): ['Subprogram', 'FixedOrVariableInterestInd', 'NaicsSector', 'BusinessType', 'BusinessAge', 'CollateralInd']\n",
      "\n",
      "=== 3. å‰å‡¦ç†ã®è©³ç´°æ¤œè¨¼ ===\n",
      "ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†è©³ç´°:\n",
      "  Subprogram: 9ç¨®é¡, ä¾‹: ['FA$TRK (Small Loan Express)', 'Contract Guaranty', 'Guaranty', 'Community Advantage Initiative', 'Standard Asset Based']\n",
      "  FixedOrVariableInterestInd: 2ç¨®é¡, ä¾‹: ['V', 'F']\n",
      "  NaicsSector: 19ç¨®é¡, ä¾‹: ['Accommodation_food services', 'Retail trade', 'Construction', 'Information', 'Health care_social assistance']\n",
      "  BusinessType: 4ç¨®é¡, ä¾‹: ['CORPORATION', 'INDIVIDUAL', 'PARTNERSHIP', '        ']\n",
      "  BusinessAge: 5ç¨®é¡, ä¾‹: ['Unanswered', 'Startup, Loan Funds will Open Business', 'Existing or more than 2 years old', 'New Business or 2 years or less', 'Change of Ownership']\n",
      "  CollateralInd: 2ç¨®é¡, ä¾‹: ['N', 'Y']\n",
      "\n",
      "=== 4. ã‚ˆã‚Šé«˜åº¦ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºèª ===\n",
      "XGBoost F1: 0.552283\n",
      "CatBoost F1: 0.563374\n",
      "LGB_Default F1: 0.529222\n",
      "LGB_Conservative F1: 0.561204\n",
      "LGB_Aggressive F1: 0.516720\n",
      "\n",
      "=== 5. æœ€é«˜æ€§èƒ½ã¨åŸæ¡ˆã¨ã®æ¯”è¼ƒ ===\n",
      "æœ€é«˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : CatBoost\n",
      "æœ€é«˜F1ã‚¹ã‚³ã‚¢: 0.563374\n",
      "åŸæ¡ˆç›®æ¨™: 0.647\n",
      "å·®ç•°: -0.083626\n",
      "åˆ¤å®š: ğŸ”„ æ”¹å–„ç¶™ç¶š\n",
      "æ¨å¥¨: è¤‡åˆçš„ãªæ”¹å–„ãŒå¿…è¦\n",
      "\n",
      "=== 6. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ ===\n",
      "ç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½10å€‹):\n",
      "   1. TermInMonths: 0.2524\n",
      "   2. FixedOrVariableInterestInd: 0.1204\n",
      "   3. BusinessType: 0.0943\n",
      "   4. Subprogram: 0.0803\n",
      "   5. InitialInterestRate: 0.0547\n",
      "   6. GrossApproval: 0.0481\n",
      "   7. ApprovalFiscalYear: 0.0478\n",
      "   8. NaicsSector: 0.0456\n",
      "   9. SBAGuaranteedApproval: 0.0449\n",
      "  10. CollateralInd: 0.0442\n",
      "\n",
      "=== 7. æœ€çµ‚åˆ¤å®šã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
      "ç¾åœ¨ã®æœ€é«˜æ€§èƒ½: 0.563374\n",
      "åŸæ¡ˆç›®æ¨™ã¾ã§: 0.084\n",
      "ğŸš¨ æ ¹æœ¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦\n",
      "å„ªå…ˆæ”¹å–„é …ç›®: ['ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª', 'å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿æ´»ç”¨']\n",
      "\n",
      "ğŸ¯ æ·±å±¤åˆ†æå®Œäº†: æ¬¡ã®æ”¹å–„æˆ¦ç•¥ãŒæ˜ç¢ºåŒ–\n"
     ]
    }
   ],
   "source": [
    "# æ·±å±¤å•é¡Œåˆ†æ: 0.535 â†’ 0.647ã¸ã®æ®‹ã‚Šèª²é¡Œç‰¹å®š\n",
    "\n",
    "print(\"ğŸ” æ·±å±¤å•é¡Œåˆ†æ: æ®‹ã‚Š0.112ã®å·®ã®åŸå› ç©¶æ˜\")\n",
    "print(\"IDé™¤å¤–ã€RevolverStatusé™¤å»ã§ã‚‚0.535 â†’ ã•ã‚‰ãªã‚‹èª¿æŸ»ãŒå¿…è¦\")\n",
    "\n",
    "# === 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªä½“ã®æ¤œè¨¼ ===\n",
    "print(\"\\n=== 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªä½“ã®æ¤œè¨¼ ===\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã®ç¢ºèª\n",
    "print(f\"train.csvã®ãƒ‘ã‚¹ç¢ºèª:\")\n",
    "print(f\"  train shape: {train.shape}\")\n",
    "print(f\"  columns: {list(train.columns)}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æ•´åˆæ€§\n",
    "print(f\"\\nãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§:\")\n",
    "print(f\"  é‡è¤‡è¡Œ: {train.duplicated().sum()}è¡Œ\")\n",
    "print(f\"  å…¨NULLè¡Œ: {train.isnull().all(axis=1).sum()}è¡Œ\")\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å¦¥å½“æ€§å†ç¢ºèª\n",
    "print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¦¥å½“æ€§:\")\n",
    "print(f\"  {TARGET_COL}ã®å€¤åŸŸ: {train[TARGET_COL].min()} - {train[TARGET_COL].max()}\")\n",
    "print(f\"  æ­£ä¾‹ç‡: {train[TARGET_COL].mean():.4f}\")\n",
    "\n",
    "# === 2. åŸæ¡ˆã¨ã®å·®åˆ†èª¿æŸ» ===\n",
    "print(f\"\\n=== 2. åŸæ¡ˆã¨ã®å·®åˆ†èª¿æŸ» ===\")\n",
    "\n",
    "# ç¾åœ¨ä½¿ç”¨ã—ã¦ã„ã‚‹ç‰¹å¾´é‡\n",
    "current_features = features\n",
    "print(f\"ç¾åœ¨ã®features ({len(current_features)}å€‹):\")\n",
    "for i, feat in enumerate(current_features):\n",
    "    print(f\"  {i+1:2d}. {feat}\")\n",
    "\n",
    "# æ•°å€¤ãƒ»ã‚«ãƒ†ã‚´ãƒªã®å†…è¨³\n",
    "current_numeric = [c for c in current_features if c not in cat_cols]\n",
    "current_categorical = cat_cols\n",
    "\n",
    "print(f\"\\nç‰¹å¾´é‡å†…è¨³:\")\n",
    "print(f\"  æ•°å€¤åˆ— ({len(current_numeric)}å€‹): {current_numeric}\")\n",
    "print(f\"  ã‚«ãƒ†ã‚´ãƒªåˆ— ({len(current_categorical)}å€‹): {current_categorical}\")\n",
    "\n",
    "# === 3. å‰å‡¦ç†ã®è©³ç´°æ¤œè¨¼ ===\n",
    "print(f\"\\n=== 3. å‰å‡¦ç†ã®è©³ç´°æ¤œè¨¼ ===\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†ç¢ºèª\n",
    "print(\"ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†è©³ç´°:\")\n",
    "for col in cat_cols:\n",
    "    unique_count = X_train[col].nunique()\n",
    "    sample_values = list(X_train[col].unique())[:5]\n",
    "    print(f\"  {col}: {unique_count}ç¨®é¡, ä¾‹: {sample_values}\")\n",
    "    \n",
    "    # é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®è­¦å‘Š\n",
    "    if unique_count > 100:\n",
    "        print(f\"    âš ï¸ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£: {unique_count}ç¨®é¡\")\n",
    "    \n",
    "    # MISSINGã®ç¢ºèª\n",
    "    missing_count = (X_train[col] == \"MISSING\").sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"    MISSING: {missing_count}å€‹ ({missing_count/len(X_train)*100:.1f}%)\")\n",
    "\n",
    "# === 4. ã‚ˆã‚Šé«˜åº¦ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºèª ===\n",
    "print(f\"\\n=== 4. ã‚ˆã‚Šé«˜åº¦ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºèª ===\")\n",
    "\n",
    "# ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®ç¢ºèª\n",
    "algorithms = {}\n",
    "\n",
    "# 1. XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚’LabelEncoding\n",
    "    X_numeric = X_train.copy()\n",
    "    for c in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_numeric[c] = le.fit_transform(X_numeric[c])\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        random_state=SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    xgb_scores = cross_val_score(xgb_model, X_numeric, y_train, cv=3, scoring='f1')\n",
    "    algorithms['XGBoost'] = xgb_scores.mean()\n",
    "    print(f\"XGBoost F1: {algorithms['XGBoost']:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"XGBoost ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# 2. CatBoostï¼ˆé©åˆ‡ãªè¨­å®šï¼‰\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    cat_idx = [X_train.columns.get_loc(c) for c in cat_cols]\n",
    "    \n",
    "    cb_model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        cat_features=cat_idx,\n",
    "        random_seed=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    cb_scores = cross_val_score(cb_model, X_train, y_train, cv=3, scoring='f1')\n",
    "    algorithms['CatBoost'] = cb_scores.mean()\n",
    "    print(f\"CatBoost F1: {algorithms['CatBoost']:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"CatBoost ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# 3. è¤‡æ•°ã®LightGBMè¨­å®š\n",
    "lgb_configs = {\n",
    "    'LGB_Default': {\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 1000,\n",
    "        'num_leaves': 31\n",
    "    },\n",
    "    'LGB_Conservative': {\n",
    "        'learning_rate': 0.02,\n",
    "        'n_estimators': 2000,\n",
    "        'num_leaves': 20,\n",
    "        'reg_alpha': 10,\n",
    "        'reg_lambda': 10\n",
    "    },\n",
    "    'LGB_Aggressive': {\n",
    "        'learning_rate': 0.1,\n",
    "        'n_estimators': 1000,\n",
    "        'num_leaves': 100,\n",
    "        'min_child_samples': 5\n",
    "    }\n",
    "}\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªå‡¦ç†\n",
    "X_lgb = X_train.copy()\n",
    "for c in cat_cols:\n",
    "    X_lgb[c] = X_lgb[c].astype('category')\n",
    "\n",
    "for name, config in lgb_configs.items():\n",
    "    try:\n",
    "        lgb_model = LGBMClassifier(\n",
    "            objective=\"binary\",\n",
    "            random_state=SEED,\n",
    "            verbose=-1,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        lgb_scores = cross_val_score(lgb_model, X_lgb, y_train, cv=3, scoring='f1')\n",
    "        algorithms[name] = lgb_scores.mean()\n",
    "        print(f\"{name} F1: {algorithms[name]:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{name} ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# === 5. æœ€é«˜æ€§èƒ½ã¨åŸæ¡ˆã¨ã®æ¯”è¼ƒ ===\n",
    "print(f\"\\n=== 5. æœ€é«˜æ€§èƒ½ã¨åŸæ¡ˆã¨ã®æ¯”è¼ƒ ===\")\n",
    "\n",
    "if algorithms:\n",
    "    best_algo = max(algorithms.items(), key=lambda x: x[1])\n",
    "    best_score = best_algo[1]\n",
    "    best_name = best_algo[0]\n",
    "    \n",
    "    print(f\"æœ€é«˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {best_name}\")\n",
    "    print(f\"æœ€é«˜F1ã‚¹ã‚³ã‚¢: {best_score:.6f}\")\n",
    "    print(f\"åŸæ¡ˆç›®æ¨™: 0.647\")\n",
    "    print(f\"å·®ç•°: {best_score - 0.647:+.6f}\")\n",
    "    \n",
    "    # é”æˆå¯èƒ½æ€§åˆ¤å®š\n",
    "    if best_score >= 0.647:\n",
    "        status = \"âœ… åŸæ¡ˆãƒ¬ãƒ™ãƒ«é”æˆï¼\"\n",
    "        next_action = \"ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ç¶™ç¶š\"\n",
    "    elif best_score >= 0.63:\n",
    "        status = \"ğŸ”¥ åŸæ¡ˆãƒ¬ãƒ™ãƒ«è¿‘æ¥ï¼\"\n",
    "        next_action = \"è»½å¾®ãªæœ€é©åŒ–ã§é”æˆå¯èƒ½\"\n",
    "    elif best_score >= 0.60:\n",
    "        status = \"ğŸ“ˆ å¤§å¹…æ”¹å–„ï¼\"\n",
    "        next_action = \"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§åˆ°é”å¯èƒ½\"\n",
    "    elif best_score >= 0.55:\n",
    "        status = \"ğŸ”„ æ”¹å–„ç¶™ç¶š\"\n",
    "        next_action = \"è¤‡åˆçš„ãªæ”¹å–„ãŒå¿…è¦\"\n",
    "    else:\n",
    "        status = \"âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®æ ¹æœ¬å•é¡Œ\"\n",
    "        next_action = \"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè‡ªä½“ã®è¦‹ç›´ã—\"\n",
    "    \n",
    "    print(f\"åˆ¤å®š: {status}\")\n",
    "    print(f\"æ¨å¥¨: {next_action}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼\")\n",
    "    best_score = 0.535\n",
    "\n",
    "# === 6. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ ===\n",
    "print(f\"\\n=== 6. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ ===\")\n",
    "\n",
    "if 'XGBoost' in algorithms:\n",
    "    try:\n",
    "        # XGBoostã§ç‰¹å¾´é‡é‡è¦åº¦ç¢ºèª\n",
    "        xgb_model.fit(X_numeric, y_train)\n",
    "        importance = xgb_model.feature_importances_\n",
    "        \n",
    "        feature_importance = list(zip(X_numeric.columns, importance))\n",
    "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"ç‰¹å¾´é‡é‡è¦åº¦ (ä¸Šä½10å€‹):\")\n",
    "        for i, (feat, imp) in enumerate(feature_importance[:10]):\n",
    "            print(f\"  {i+1:2d}. {feat}: {imp:.4f}\")\n",
    "            \n",
    "        # é‡è¦åº¦ãŒæ¥µç«¯ã«ä½ã„ç‰¹å¾´é‡\n",
    "        low_importance = [feat for feat, imp in feature_importance if imp < 0.01]\n",
    "        if low_importance:\n",
    "            print(f\"\\né‡è¦åº¦ãŒä½ã„ç‰¹å¾´é‡: {low_importance}\")\n",
    "            \n",
    "    except:\n",
    "        print(\"ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—ã«å¤±æ•—\")\n",
    "\n",
    "# === 7. æœ€çµ‚åˆ¤å®šã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
    "print(f\"\\n=== 7. æœ€çµ‚åˆ¤å®šã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\")\n",
    "\n",
    "current_best = best_score if 'best_score' in locals() else 0.535\n",
    "gap_to_target = 0.647 - current_best\n",
    "\n",
    "print(f\"ç¾åœ¨ã®æœ€é«˜æ€§èƒ½: {current_best:.6f}\")\n",
    "print(f\"åŸæ¡ˆç›®æ¨™ã¾ã§: {gap_to_target:.3f}\")\n",
    "\n",
    "if gap_to_target <= 0.005:\n",
    "    print(\"ğŸ¯ å¾®èª¿æ•´ã§åŸæ¡ˆé”æˆå¯èƒ½\")\n",
    "    priority = [\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¾®èª¿æ•´\", \"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–\"]\n",
    "elif gap_to_target <= 0.02:\n",
    "    print(\"ğŸ”§ ä¸­ç¨‹åº¦ã®æ”¹å–„ã§é”æˆå¯èƒ½\")\n",
    "    priority = [\"ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\", \"ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–\"]\n",
    "elif gap_to_target <= 0.05:\n",
    "    print(\"ğŸ› ï¸ å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦\")\n",
    "    priority = [\"é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\", \"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥\"]\n",
    "else:\n",
    "    print(\"ğŸš¨ æ ¹æœ¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦\")\n",
    "    priority = [\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª\", \"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿æ´»ç”¨\"]\n",
    "\n",
    "print(f\"å„ªå…ˆæ”¹å–„é …ç›®: {priority}\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "DEEP_ANALYSIS_RESULT = {\n",
    "    \"best_algorithm\": best_name if 'best_name' in locals() else \"Unknown\",\n",
    "    \"best_score\": current_best,\n",
    "    \"gap_to_target\": gap_to_target,\n",
    "    \"priority_actions\": priority,\n",
    "    \"algorithms_tested\": algorithms if 'algorithms' in locals() else {}\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ¯ æ·±å±¤åˆ†æå®Œäº†: æ¬¡ã®æ”¹å–„æˆ¦ç•¥ãŒæ˜ç¢ºåŒ–\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8709d14",
   "metadata": {},
   "source": [
    "# 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3b576",
   "metadata": {},
   "source": [
    "# 3.1 ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãç‰¹å¾´é‡ç”Ÿæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "987226df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== åŸæ¡ˆå›å¸°: å³é¸ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===\n",
      "ç›®æ¨™: è¤‡é›‘åŒ–ã‚’æ’é™¤ã—ã€åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã«æˆ»ã™\n",
      "ç‰¹å¾´é‡æ•°: 15 â†’ 23 (+8)\n",
      "æ–°è¦ç‰¹å¾´é‡ (8å€‹): ['is_small_loan', 'is_short_term', 'is_high_rate', 'high_risk_combo', 'sba_guarantee_ratio', 'cost_per_job', 'high_job_efficiency', 'high_risk_sector']\n",
      "ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: 6\n",
      "\n",
      "âœ… é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†å¸ƒ:\n",
      "  ã‚¹ã‚³ã‚¢0: 1997ä»¶ (26.4%)\n",
      "  ã‚¹ã‚³ã‚¢1: 2230ä»¶ (29.5%)\n",
      "  ã‚¹ã‚³ã‚¢2: 2581ä»¶ (34.2%)\n",
      "  ã‚¹ã‚³ã‚¢3: 744ä»¶ (9.9%)\n",
      "\n",
      "ğŸ¯ è¤‡é›‘åŒ–ã‚’æ’é™¤: 21å€‹ â†’ 8å€‹ã®å³é¸ç‰¹å¾´é‡\n",
      "ğŸš€ æœŸå¾…åŠ¹æœ: åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã¸ã®å›å¾©\n",
      "\n",
      "âœ… ã‚·ãƒ³ãƒ—ãƒ«åŒ–å®Œäº†ï¼ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚Šæ€§èƒ½å›å¾©ã‚’æœŸå¾…\n",
      "æ³¨æ„: ãƒ—ãƒ¼ãƒ«ã®å†æ§‹ç¯‰ã¯ã‚»ãƒ«9ï¼ˆbuild_poolsé–¢æ•°å®šç¾©å¾Œï¼‰ã§è¡Œã£ã¦ãã ã•ã„\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«8ã‚·ãƒ³ãƒ—ãƒ«åŒ–ç‰ˆ: å³é¸ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "# === åŸæ¡ˆå›å¸°: æœ¬è³ªçš„ãªç‰¹å¾´é‡ã®ã¿ ===\n",
    "\n",
    "def create_essential_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"æœ¬è³ªçš„ã§åŠ¹æœã®é«˜ã„ç‰¹å¾´é‡ã®ã¿ç”Ÿæˆï¼ˆ5-7å€‹ã«å³é¸ï¼‰\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # === æœ€ã‚‚é‡è¦ãªç™ºè¦‹ã®ã¿æ´»ç”¨ ===\n",
    "    \n",
    "    # 1. é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ï¼ˆæœ€é‡è¦ï¼‰\n",
    "    if all(col in df_new.columns for col in ['GrossApproval', 'TermInMonths', 'InitialInterestRate']):\n",
    "        # å°é¡èè³‡ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ã§åˆ¤æ˜ã—ãŸé–¾å€¤ï¼‰\n",
    "        df_new['is_small_loan'] = (df_new['GrossApproval'] <= 320000).astype(int)\n",
    "        \n",
    "        # çŸ­æœŸèè³‡ãƒ•ãƒ©ã‚°\n",
    "        df_new['is_short_term'] = (df_new['TermInMonths'] <= 80).astype(int)\n",
    "        \n",
    "        # é«˜é‡‘åˆ©ãƒ•ãƒ©ã‚°\n",
    "        df_new['is_high_rate'] = (df_new['InitialInterestRate'] > 8.0).astype(int)\n",
    "        \n",
    "        # é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ï¼ˆ3ã¤ã®æ¡ä»¶ã®çµ„ã¿åˆã‚ã›ï¼‰\n",
    "        df_new['high_risk_combo'] = (\n",
    "            df_new['is_small_loan'] + \n",
    "            df_new['is_short_term'] + \n",
    "            df_new['is_high_rate']\n",
    "        )\n",
    "    \n",
    "    # 2. SBAä¿è¨¼ç‡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰\n",
    "    if all(col in df_new.columns for col in ['GrossApproval', 'SBAGuaranteedApproval']):\n",
    "        df_new['sba_guarantee_ratio'] = df_new['SBAGuaranteedApproval'] / (df_new['GrossApproval'] + 1e-8)\n",
    "    \n",
    "    # 3. é›‡ç”¨åŠ¹ç‡æ€§ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰\n",
    "    if all(col in df_new.columns for col in ['GrossApproval', 'JobsSupported']):\n",
    "        df_new['cost_per_job'] = df_new['GrossApproval'] / (df_new['JobsSupported'] + 1)\n",
    "        df_new['high_job_efficiency'] = (df_new['JobsSupported'] >= 5).astype(int)\n",
    "    \n",
    "    # 4. æ¥­ç•Œãƒªã‚¹ã‚¯ï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "    if 'NaicsSector' in df_new.columns:\n",
    "        high_risk_sectors = [\n",
    "            'Accommodation and food services',\n",
    "            'Retail trade',\n",
    "            'Arts, entertainment, and recreation'\n",
    "        ]\n",
    "        df_new['high_risk_sector'] = df_new['NaicsSector'].isin(high_risk_sectors).astype(int)\n",
    "    \n",
    "    # å‰Šé™¤: å€‹åˆ¥ãƒ•ãƒ©ã‚°ï¼ˆè¤‡åˆæŒ‡æ¨™ã«çµ±åˆæ¸ˆã¿ï¼‰\n",
    "    # å‰Šé™¤: è¤‡é›‘ãªã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "    # å‰Šé™¤: æ™‚æœŸãƒ»åœ°åŸŸé–¢é€£ï¼ˆåŠ¹æœãŒé™å®šçš„ï¼‰\n",
    "    # å‰Šé™¤: äº‹æ¥­å¹´æ•°é–¢é€£ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªã«èª²é¡Œï¼‰\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "print(\"=== åŸæ¡ˆå›å¸°: å³é¸ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===\")\n",
    "print(\"ç›®æ¨™: è¤‡é›‘åŒ–ã‚’æ’é™¤ã—ã€åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã«æˆ»ã™\")\n",
    "\n",
    "# ç‰¹å¾´é‡ç”Ÿæˆã‚’é©ç”¨\n",
    "X_train_enhanced = create_essential_features(train[features])\n",
    "X_test_enhanced = create_essential_features(test[features])\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ›´æ–°ï¼ˆæœ€å°é™ï¼‰\n",
    "cat_cols_enhanced = [c for c in X_train_enhanced.columns \n",
    "                    if X_train_enhanced[c].dtype == 'object' or 'category' in str(X_train_enhanced[c].dtype)]\n",
    "\n",
    "for c in cat_cols_enhanced:\n",
    "    X_train_enhanced[c] = X_train_enhanced[c].astype(str).fillna(\"MISSING\")\n",
    "    X_test_enhanced[c] = X_test_enhanced[c].astype(str).fillna(\"MISSING\")\n",
    "\n",
    "cat_features_idx_enhanced = [X_train_enhanced.columns.get_loc(c) for c in cat_cols_enhanced]\n",
    "\n",
    "# å¤‰æ›´é‡ã®ç¢ºèª\n",
    "original_count = len(features)\n",
    "enhanced_count = len(X_train_enhanced.columns)\n",
    "added_count = enhanced_count - original_count\n",
    "\n",
    "print(f\"ç‰¹å¾´é‡æ•°: {original_count} â†’ {enhanced_count} (+{added_count})\")\n",
    "\n",
    "new_features = [c for c in X_train_enhanced.columns if c not in features]\n",
    "print(f\"æ–°è¦ç‰¹å¾´é‡ ({len(new_features)}å€‹): {new_features}\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: {len(cat_features_idx_enhanced)}\")\n",
    "\n",
    "# æœ€é‡è¦ç‰¹å¾´é‡ã®åŠ¹æœç¢ºèª\n",
    "if 'high_risk_combo' in X_train_enhanced.columns:\n",
    "    combo_dist = X_train_enhanced['high_risk_combo'].value_counts().sort_index()\n",
    "    print(f\"\\nâœ… é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†å¸ƒ:\")\n",
    "    for score, count in combo_dist.items():\n",
    "        print(f\"  ã‚¹ã‚³ã‚¢{score}: {count}ä»¶ ({count/len(X_train_enhanced)*100:.1f}%)\")\n",
    "    \n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡ã®ç¢ºèªï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰\n",
    "    if 'high_risk_combo' in X_train_enhanced.columns:\n",
    "        print(f\"\\nğŸ¯ è¤‡é›‘åŒ–ã‚’æ’é™¤: 21å€‹ â†’ {len(new_features)}å€‹ã®å³é¸ç‰¹å¾´é‡\")\n",
    "        print(f\"ğŸš€ æœŸå¾…åŠ¹æœ: åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã¸ã®å›å¾©\")\n",
    "\n",
    "print(f\"\\nâœ… ã‚·ãƒ³ãƒ—ãƒ«åŒ–å®Œäº†ï¼ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚Šæ€§èƒ½å›å¾©ã‚’æœŸå¾…\")\n",
    "print(\"æ³¨æ„: ãƒ—ãƒ¼ãƒ«ã®å†æ§‹ç¯‰ã¯ã‚»ãƒ«9ï¼ˆbuild_poolsé–¢æ•°å®šç¾©å¾Œï¼‰ã§è¡Œã£ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad72c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===\n",
      "ç‰¹å¾´é‡æ•°: 15 â†’ 36 (+21)\n",
      "æ–°è¦ç‰¹å¾´é‡: ['is_small_loan', 'is_short_term', 'is_high_rate', 'high_risk_combo', 'rate_term_risk', 'amount_term_ratio', 'borrower_amount', 'borrower_ratio', 'low_sba_guarantee', 'high_risk_sector', 'low_risk_sector', 'is_new_business', 'business_age_unknown', 'is_express_loan', 'cost_per_job', 'no_jobs_created', 'high_job_efficiency', 'urban_district', 'covid_period', 'recent_approval', 'composite_risk_score']\n",
      "ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: 6\n",
      "\n",
      "é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†å¸ƒ:\n",
      "  ã‚¹ã‚³ã‚¢0: 1997ä»¶ (26.4%)\n",
      "  ã‚¹ã‚³ã‚¢1: 2230ä»¶ (29.5%)\n",
      "  ã‚¹ã‚³ã‚¢2: 2581ä»¶ (34.2%)\n",
      "  ã‚¹ã‚³ã‚¢3: 744ä»¶ (9.9%)\n",
      "\n",
      "æ³¨æ„: ãƒ—ãƒ¼ãƒ«ã®å†æ§‹ç¯‰ã¯ã‚»ãƒ«9ï¼ˆbuild_poolsé–¢æ•°å®šç¾©å¾Œï¼‰ã§è¡Œã£ã¦ãã ã•ã„\n"
     ]
    }
   ],
   "source": [
    "# #ã‚»ãƒ«8ï¼šå¼·åŒ–ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
    "\n",
    "# # === ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===\n",
    "\n",
    "# def create_data_driven_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ãé«˜ç²¾åº¦ç‰¹å¾´é‡ç”Ÿæˆ\"\"\"\n",
    "#     df_new = df.copy()\n",
    "    \n",
    "#     # === åˆ†æã§åˆ¤æ˜ã—ãŸé‡è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãç‰¹å¾´é‡ ===\n",
    "    \n",
    "#     # 1. å°é¡ãƒ»çŸ­æœŸãƒ»é«˜é‡‘åˆ©ãƒªã‚¹ã‚¯ï¼ˆæœ€é‡è¦ç™ºè¦‹ï¼‰\n",
    "#     if all(col in df_new.columns for col in ['GrossApproval', 'TermInMonths', 'InitialInterestRate']):\n",
    "#         # å°é¡èè³‡ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹³å‡$319,465ã€æ­£å¸¸å¹³å‡$780,791ï¼‰\n",
    "#         df_new['is_small_loan'] = (df_new['GrossApproval'] <= 320000).astype(int)\n",
    "        \n",
    "#         # çŸ­æœŸèè³‡ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹³å‡80.4ãƒ¶æœˆã€æ­£å¸¸å¹³å‡125.6ãƒ¶æœˆï¼‰\n",
    "#         df_new['is_short_term'] = (df_new['TermInMonths'] <= 80).astype(int)\n",
    "        \n",
    "#         # é«˜é‡‘åˆ©ãƒ•ãƒ©ã‚°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹³å‡8.19%ã€æ­£å¸¸å¹³å‡7.26%ï¼‰\n",
    "#         df_new['is_high_rate'] = (df_new['InitialInterestRate'] > 8.0).astype(int)\n",
    "        \n",
    "#         # é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ï¼ˆ3ã¤ã®æ¡ä»¶ã®çµ„ã¿åˆã‚ã›ï¼‰\n",
    "#         df_new['high_risk_combo'] = (\n",
    "#             df_new['is_small_loan'] + \n",
    "#             df_new['is_short_term'] + \n",
    "#             df_new['is_high_rate']\n",
    "#         )\n",
    "        \n",
    "#         # æœŸé–“èª¿æ•´é‡‘åˆ©ãƒªã‚¹ã‚¯\n",
    "#         df_new['rate_term_risk'] = df_new['InitialInterestRate'] * np.log1p(df_new['TermInMonths'])\n",
    "        \n",
    "#         # èè³‡é¡ã¨æœŸé–“ã®æ¯”ç‡ï¼ˆçŸ­æœŸå¤§å£vsé•·æœŸå°å£ï¼‰\n",
    "#         df_new['amount_term_ratio'] = df_new['GrossApproval'] / (df_new['TermInMonths'] + 1)\n",
    "    \n",
    "#     # 2. SBAä¿è¨¼é–¢é€£ï¼ˆãƒ‡ãƒ¼ã‚¿ã§ã¯å·®ãŒãªã„ãŒã€æ´¾ç”ŸæŒ‡æ¨™ã¯æœ‰åŠ¹ï¼‰\n",
    "#     if all(col in df_new.columns for col in ['GrossApproval', 'SBAGuaranteedApproval']):\n",
    "#         # å€Ÿã‚Šæ‰‹è² æ‹…é¡ï¼ˆçµ¶å¯¾é¡ï¼‰\n",
    "#         df_new['borrower_amount'] = df_new['GrossApproval'] - df_new['SBAGuaranteedApproval']\n",
    "        \n",
    "#         # å€Ÿã‚Šæ‰‹è² æ‹…ç‡\n",
    "#         df_new['borrower_ratio'] = df_new['borrower_amount'] / (df_new['GrossApproval'] + 1e-8)\n",
    "        \n",
    "#         # ä½ä¿è¨¼ãƒ•ãƒ©ã‚°ï¼ˆ50%æœªæº€ï¼‰\n",
    "#         sba_ratio = df_new['SBAGuaranteedApproval'] / (df_new['GrossApproval'] + 1e-8)\n",
    "#         df_new['low_sba_guarantee'] = (sba_ratio < 0.5).astype(int)\n",
    "    \n",
    "#     # 3. æ¥­ç•Œãƒªã‚¹ã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿åˆ†æã§é«˜ãƒªã‚¹ã‚¯ç”£æ¥­ã‚’ç‰¹å®šï¼‰\n",
    "#     if 'NaicsSector' in df_new.columns:\n",
    "#         # åˆ†æçµæœã«åŸºã¥ãé«˜ãƒªã‚¹ã‚¯ç”£æ¥­\n",
    "#         high_risk_sectors = [\n",
    "#             'Accommodation_food services',  # é£²é£Ÿæ¥­ï¼ˆé€šå¸¸é«˜ãƒªã‚¹ã‚¯ï¼‰\n",
    "#             'Arts_entertainment_recreation', # å¨¯æ¥½æ¥­\n",
    "#             'Retail trade',                 # å°å£²æ¥­\n",
    "#             'Other services (except public administration)' # ãã®ä»–ã‚µãƒ¼ãƒ“ã‚¹\n",
    "#         ]\n",
    "#         df_new['high_risk_sector'] = df_new['NaicsSector'].isin(high_risk_sectors).astype(int)\n",
    "        \n",
    "#         # ä½ãƒªã‚¹ã‚¯ç”£æ¥­\n",
    "#         low_risk_sectors = [\n",
    "#             'Health care_social assistance',  # åŒ»ç™‚ãƒ»ç¤¾ä¼šä¿éšœ\n",
    "#             'Professional_scientific_technical services', # å°‚é–€æŠ€è¡“ã‚µãƒ¼ãƒ“ã‚¹\n",
    "#             'Finance_insurance',             # é‡‘èä¿é™º\n",
    "#             'Manufacturing'                  # è£½é€ æ¥­\n",
    "#         ]\n",
    "#         df_new['low_risk_sector'] = df_new['NaicsSector'].isin(low_risk_sectors).astype(int)\n",
    "    \n",
    "#     # 4. äº‹æ¥­å¹´æ•°ãƒªã‚¹ã‚¯\n",
    "#     if 'BusinessAge' in df_new.columns:\n",
    "#         # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ»æ–°è¦äº‹æ¥­ãƒ•ãƒ©ã‚°\n",
    "#         df_new['is_new_business'] = df_new['BusinessAge'].str.contains(\n",
    "#             'Startup|New Business', case=False, na=False\n",
    "#         ).astype(int)\n",
    "        \n",
    "#         # ä¸æ˜å›ç­”ãƒ•ãƒ©ã‚°ï¼ˆãƒªã‚¹ã‚¯è¦å› ã®å¯èƒ½æ€§ï¼‰\n",
    "#         df_new['business_age_unknown'] = df_new['BusinessAge'].str.contains(\n",
    "#             'Unanswered', case=False, na=False\n",
    "#         ).astype(int)\n",
    "    \n",
    "#     # 5. èè³‡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãƒªã‚¹ã‚¯\n",
    "#     if 'Subprogram' in df_new.columns:\n",
    "#         # Express loanï¼ˆé€šå¸¸é«˜ãƒªã‚¹ã‚¯ãƒ»å°é¡ãƒ»çŸ­æœŸï¼‰\n",
    "#         df_new['is_express_loan'] = df_new['Subprogram'].str.contains(\n",
    "#             'Express', case=False, na=False\n",
    "#         ).astype(int)\n",
    "    \n",
    "#     # 6. é›‡ç”¨åŠ¹ç‡æŒ‡æ¨™\n",
    "#     if all(col in df_new.columns for col in ['GrossApproval', 'JobsSupported']):\n",
    "#         # 1é›‡ç”¨ã‚ãŸã‚Šã®èè³‡é¡\n",
    "#         df_new['cost_per_job'] = df_new['GrossApproval'] / (df_new['JobsSupported'] + 1e-8)\n",
    "        \n",
    "#         # é›‡ç”¨ãªã—ãƒ•ãƒ©ã‚°\n",
    "#         df_new['no_jobs_created'] = (df_new['JobsSupported'] == 0).astype(int)\n",
    "        \n",
    "#         # é«˜åŠ¹ç‡é›‡ç”¨å‰µå‡ºãƒ•ãƒ©ã‚°\n",
    "#         df_new['high_job_efficiency'] = (df_new['JobsSupported'] >= 10).astype(int)\n",
    "    \n",
    "#     # 7. åœ°åŸŸãƒªã‚¹ã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
    "#     if 'CongressionalDistrict' in df_new.columns:\n",
    "#         # å¤§éƒ½å¸‚åœãƒ•ãƒ©ã‚°ï¼ˆé¸æŒ™åŒºç•ªå·ãŒå¤§ãã„ = äººå£å¯†åº¦é«˜ã„ï¼‰\n",
    "#         df_new['urban_district'] = (df_new['CongressionalDistrict'] >= 10).astype(int)\n",
    "    \n",
    "#     # 8. æ™‚æœŸãƒªã‚¹ã‚¯\n",
    "#     if 'ApprovalFiscalYear' in df_new.columns:\n",
    "#         # COVIDå½±éŸ¿æœŸãƒ•ãƒ©ã‚°\n",
    "#         df_new['covid_period'] = df_new['ApprovalFiscalYear'].isin([2020, 2021]).astype(int)\n",
    "        \n",
    "#         # æœ€è¿‘ã®ç”³è«‹ãƒ•ãƒ©ã‚°\n",
    "#         df_new['recent_approval'] = (df_new['ApprovalFiscalYear'] >= 2022).astype(int)\n",
    "    \n",
    "#     # 9. è¤‡åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆé‡è¦ãªç™ºè¦‹ã‚’çµ±åˆï¼‰\n",
    "#     risk_components = []\n",
    "    \n",
    "#     if 'high_risk_combo' in df_new.columns:\n",
    "#         risk_components.append(df_new['high_risk_combo'] * 0.4)  # æœ€é‡è¦\n",
    "#     if 'high_risk_sector' in df_new.columns:\n",
    "#         risk_components.append(df_new['high_risk_sector'] * 0.2)\n",
    "#     if 'is_new_business' in df_new.columns:\n",
    "#         risk_components.append(df_new['is_new_business'] * 0.2)\n",
    "#     if 'is_express_loan' in df_new.columns:\n",
    "#         risk_components.append(df_new['is_express_loan'] * 0.1)\n",
    "#     if 'no_jobs_created' in df_new.columns:\n",
    "#         risk_components.append(df_new['no_jobs_created'] * 0.1)\n",
    "    \n",
    "#     if risk_components:\n",
    "#         df_new['composite_risk_score'] = np.sum(risk_components, axis=0)\n",
    "    \n",
    "#     return df_new\n",
    "\n",
    "# print(\"=== ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° ===\")\n",
    "\n",
    "# # ç‰¹å¾´é‡ç”Ÿæˆã‚’é©ç”¨\n",
    "# X_train_enhanced = create_data_driven_features(train[features])\n",
    "# X_test_enhanced = create_data_driven_features(test[features])\n",
    "\n",
    "# # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ›´æ–°\n",
    "# cat_cols_enhanced = [c for c in X_train_enhanced.columns \n",
    "#                     if X_train_enhanced[c].dtype == 'object' or 'category' in str(X_train_enhanced[c].dtype)]\n",
    "\n",
    "# for c in cat_cols_enhanced:\n",
    "#     X_train_enhanced[c] = X_train_enhanced[c].astype(str).fillna(\"MISSING\")\n",
    "#     X_test_enhanced[c] = X_test_enhanced[c].astype(str).fillna(\"MISSING\")\n",
    "\n",
    "# cat_features_idx_enhanced = [X_train_enhanced.columns.get_loc(c) for c in cat_cols_enhanced]\n",
    "\n",
    "# print(f\"ç‰¹å¾´é‡æ•°: {len(features)} â†’ {len(X_train_enhanced.columns)} (+{len(X_train_enhanced.columns) - len(features)})\")\n",
    "# new_features = [c for c in X_train_enhanced.columns if c not in features]\n",
    "# print(f\"æ–°è¦ç‰¹å¾´é‡: {new_features}\")\n",
    "# print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: {len(cat_features_idx_enhanced)}\")\n",
    "\n",
    "# # é‡è¦ãªæ–°ç‰¹å¾´é‡ã®åˆ†å¸ƒç¢ºèª\n",
    "# if 'high_risk_combo' in X_train_enhanced.columns:\n",
    "#     combo_dist = X_train_enhanced['high_risk_combo'].value_counts().sort_index()\n",
    "#     print(f\"\\né«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†å¸ƒ:\")\n",
    "#     for score, count in combo_dist.items():\n",
    "#         print(f\"  ã‚¹ã‚³ã‚¢{score}: {count}ä»¶ ({count/len(X_train_enhanced)*100:.1f}%)\")\n",
    "\n",
    "# print(\"\\næ³¨æ„: ãƒ—ãƒ¼ãƒ«ã®å†æ§‹ç¯‰ã¯ã‚»ãƒ«9ï¼ˆbuild_poolsé–¢æ•°å®šç¾©å¾Œï¼‰ã§è¡Œã£ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547be42d",
   "metadata": {},
   "source": [
    "# 3.2 ç‰¹å¾´é‡åŠ¹æœã®é«˜é€Ÿæ¤œè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d5b0fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å³é¸ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ï¼ˆåŸæ¡ˆå›å¸°ï¼‰ ===\n",
      "ç›®æ¨™: åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã«æˆ»ã™\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ F1: 0.497951 Â± 0.057429\n",
      "å³é¸ç‰¹å¾´é‡ F1: 0.505904 Â± 0.049204\n",
      "\n",
      "=== åŠ¹æœåˆ¤å®š ===\n",
      "æ”¹å–„åº¦: +0.007952 (+1.60%)\n",
      "åŸæ¡ˆç›®æ¨™: 0.647\n",
      "ç¾åœ¨ãƒ¬ãƒ™ãƒ«: 0.505904\n",
      "ãƒ¬ãƒ™ãƒ«åˆ¤å®š: âš ï¸ åŸæ¡ˆãƒ¬ãƒ™ãƒ«æœªé”\n",
      "âœ… æ˜ç¢ºãªæ”¹å–„ï¼å³é¸ç‰¹å¾´é‡ã‚’æ¡ç”¨\n",
      "\n",
      "=== é‡è¦æ–°ç‰¹å¾´é‡ã®åŠ¹æœç¢ºèª ===\n",
      "é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†æ:\n",
      "  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«0: 6.4% (1997ä»¶)\n",
      "  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«1: 12.0% (2230ä»¶)\n",
      "  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«2: 16.6% (2581ä»¶)\n",
      "  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«3: 18.8% (744ä»¶)\n",
      "  é«˜ãƒªã‚¹ã‚¯ç¾¤(â‰¥2): 17.1% (ãƒªã‚¹ã‚¯å€ç‡1.3å€)\n",
      "\n",
      "è¿½åŠ ç‰¹å¾´é‡ (8å€‹): ['is_small_loan', 'is_short_term', 'is_high_rate', 'high_risk_combo', 'sba_guarantee_ratio', 'cost_per_job', 'high_job_efficiency', 'high_risk_sector']\n",
      "\n",
      "=== æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
      "âœ“ å³é¸ç‰¹å¾´é‡ã§ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ï¼ˆã‚»ãƒ«10ã§å®Ÿè¡Œï¼‰\n",
      "âœ“ åŸæ¡ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\n",
      "âœ“ æœŸå¾…: åŸæ¡ˆ0.647-0.650ãƒ¬ãƒ™ãƒ«ã®å›å¾©\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«9ã‚·ãƒ³ãƒ—ãƒ«åŒ–ç‰ˆ: å³é¸ç‰¹å¾´é‡åŠ¹æœæ¤œè¨¼\n",
    "\n",
    "# === å³é¸ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ ===\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def simplified_feature_test():\n",
    "    \"\"\"å³é¸ã•ã‚ŒãŸç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ï¼ˆåŸæ¡ˆå›å¸°ï¼‰\"\"\"\n",
    "    \n",
    "    print(\"=== å³é¸ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ï¼ˆåŸæ¡ˆå›å¸°ï¼‰ ===\")\n",
    "    print(\"ç›®æ¨™: åŸæ¡ˆã®0.647-0.650ãƒ¬ãƒ™ãƒ«ã«æˆ»ã™\")\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†\n",
    "    def prep_for_lgb(X):\n",
    "        X_prep = X.copy()\n",
    "        for c in X_prep.columns:\n",
    "            if X_prep[c].dtype == 'object':\n",
    "                X_prep[c] = X_prep[c].astype('category')\n",
    "        return X_prep\n",
    "    \n",
    "    # åŸæ¡ˆãƒ¬ãƒ™ãƒ«ã®æ¤œè¨¼ãƒ¢ãƒ‡ãƒ«\n",
    "    validation_model = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,  # ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ç‡\n",
    "        num_leaves=50,       # é©åº¦ãªè¤‡é›‘ã•\n",
    "        n_estimators=200,    # ååˆ†ãªå­¦ç¿’\n",
    "        reg_alpha=1,         # è»½å¾®ãªæ­£å‰‡åŒ–\n",
    "        reg_lambda=1,\n",
    "        random_state=SEED,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…ƒã®ç‰¹å¾´é‡ï¼‰\n",
    "    X_train_orig_prep = prep_for_lgb(train[features])\n",
    "    baseline_scores = cross_val_score(\n",
    "        validation_model, X_train_orig_prep, y_train, \n",
    "        cv=5, scoring='f1', n_jobs=-1  # 5-foldã§ã‚ˆã‚Šå®‰å®šã—ãŸè©•ä¾¡\n",
    "    )\n",
    "    baseline_f1 = baseline_scores.mean()\n",
    "    baseline_std = baseline_scores.std()\n",
    "    print(f\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ F1: {baseline_f1:.6f} Â± {baseline_std:.6f}\")\n",
    "    \n",
    "    # 2. å³é¸ç‰¹å¾´é‡ç‰ˆ\n",
    "    X_train_enhanced_prep = prep_for_lgb(X_train_enhanced)\n",
    "    enhanced_scores = cross_val_score(\n",
    "        validation_model, X_train_enhanced_prep, y_train,\n",
    "        cv=5, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "    enhanced_f1 = enhanced_scores.mean()\n",
    "    enhanced_std = enhanced_scores.std()\n",
    "    print(f\"å³é¸ç‰¹å¾´é‡ F1: {enhanced_f1:.6f} Â± {enhanced_std:.6f}\")\n",
    "    \n",
    "    # 3. æ”¹å–„åº¦è©•ä¾¡\n",
    "    improvement = enhanced_f1 - baseline_f1\n",
    "    print(f\"\\n=== åŠ¹æœåˆ¤å®š ===\")\n",
    "    print(f\"æ”¹å–„åº¦: {improvement:+.6f} ({improvement/baseline_f1*100:+.2f}%)\")\n",
    "    \n",
    "    # åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ°é”åˆ¤å®š\n",
    "    target_f1 = 0.647  # åŸæ¡ˆã®ä¸‹é™\n",
    "    print(f\"åŸæ¡ˆç›®æ¨™: {target_f1:.3f}\")\n",
    "    print(f\"ç¾åœ¨ãƒ¬ãƒ™ãƒ«: {enhanced_f1:.6f}\")\n",
    "    \n",
    "    if enhanced_f1 >= target_f1:\n",
    "        level_status = \"âœ… åŸæ¡ˆãƒ¬ãƒ™ãƒ«é”æˆ\"\n",
    "    elif enhanced_f1 >= target_f1 - 0.01:\n",
    "        level_status = \"ğŸ”„ åŸæ¡ˆãƒ¬ãƒ™ãƒ«è¿‘æ¥\"\n",
    "    else:\n",
    "        level_status = \"âš ï¸ åŸæ¡ˆãƒ¬ãƒ™ãƒ«æœªé”\"\n",
    "    \n",
    "    print(f\"ãƒ¬ãƒ™ãƒ«åˆ¤å®š: {level_status}\")\n",
    "    \n",
    "    # æ¡ç”¨åˆ¤å®šï¼ˆã‚ˆã‚Šä¿å®ˆçš„ï¼‰\n",
    "    if improvement > 0.005:  # 0.5%ä»¥ä¸Šæ”¹å–„\n",
    "        print(\"âœ… æ˜ç¢ºãªæ”¹å–„ï¼å³é¸ç‰¹å¾´é‡ã‚’æ¡ç”¨\")\n",
    "        return True, \"clear_improvement\"\n",
    "    elif improvement > 0.002:  # 0.2%ä»¥ä¸Šæ”¹å–„\n",
    "        print(\"âœ… æ”¹å–„ã‚ã‚Šï¼å³é¸ç‰¹å¾´é‡ã‚’æ¡ç”¨\")\n",
    "        return True, \"modest_improvement\"\n",
    "    elif improvement > 0:\n",
    "        print(\"â–³ å¾®å°æ”¹å–„ã€‚åŸæ¡ˆå›å¸°ã¨ã—ã¦æ¡ç”¨\")\n",
    "        return True, \"minimal_improvement\"\n",
    "    else:\n",
    "        print(\"âŒ æ”¹å–„ãªã—ã€‚å…ƒã®ç‰¹å¾´é‡ã‚’ç¶­æŒ\")\n",
    "        return False, \"no_improvement\"\n",
    "\n",
    "# æ¤œè¨¼å®Ÿè¡Œ\n",
    "is_beneficial, improvement_level = simplified_feature_test()\n",
    "\n",
    "# é‡è¦ç‰¹å¾´é‡ã®åŠ¹æœç¢ºèªï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "if is_beneficial:\n",
    "    print(f\"\\n=== é‡è¦æ–°ç‰¹å¾´é‡ã®åŠ¹æœç¢ºèª ===\")\n",
    "    \n",
    "    # é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åŠ¹æœ\n",
    "    if 'high_risk_combo' in X_train_enhanced.columns:\n",
    "        print(\"é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åˆ†æ:\")\n",
    "        \n",
    "        # å„ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡\n",
    "        for risk_level in [0, 1, 2, 3]:\n",
    "            risk_mask = X_train_enhanced['high_risk_combo'] == risk_level\n",
    "            if risk_mask.sum() > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«\n",
    "                default_rate = y_train[risk_mask].mean()\n",
    "                sample_count = risk_mask.sum()\n",
    "                print(f\"  ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«{risk_level}: {default_rate:.1%} ({sample_count}ä»¶)\")\n",
    "        \n",
    "        # é«˜ãƒªã‚¹ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å®š\n",
    "        high_risk_mask = X_train_enhanced['high_risk_combo'] >= 2\n",
    "        if high_risk_mask.sum() > 0:\n",
    "            high_risk_rate = y_train[high_risk_mask].mean()\n",
    "            overall_rate = y_train.mean()\n",
    "            risk_ratio = high_risk_rate / overall_rate\n",
    "            print(f\"  é«˜ãƒªã‚¹ã‚¯ç¾¤(â‰¥2): {high_risk_rate:.1%} (ãƒªã‚¹ã‚¯å€ç‡{risk_ratio:.1f}å€)\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã®ç°¡æ½”ã‚µãƒãƒªãƒ¼\n",
    "    new_features = [c for c in X_train_enhanced.columns if c not in features]\n",
    "    print(f\"\\nè¿½åŠ ç‰¹å¾´é‡ ({len(new_features)}å€‹): {new_features}\")\n",
    "\n",
    "# æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n",
    "print(f\"\\n=== æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\")\n",
    "if is_beneficial:\n",
    "    print(\"âœ“ å³é¸ç‰¹å¾´é‡ã§ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ï¼ˆã‚»ãƒ«10ã§å®Ÿè¡Œï¼‰\")\n",
    "    print(\"âœ“ åŸæ¡ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\")\n",
    "    if improvement_level == \"clear_improvement\":\n",
    "        print(\"âœ“ æœŸå¾…: åŸæ¡ˆ0.647-0.650ãƒ¬ãƒ™ãƒ«ã®å›å¾©\")\n",
    "else:\n",
    "    print(\"âœ“ å…ƒã®ç‰¹å¾´é‡ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\")\n",
    "    print(\"âœ“ ä»–ã®è¦å› ã§ã®æ€§èƒ½å›å¾©ã‚’æ¤œè¨\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "FEATURE_TEST_RESULT = {\n",
    "    \"beneficial\": is_beneficial,\n",
    "    \"improvement_level\": improvement_level,\n",
    "    \"feature_count\": len(X_train_enhanced.columns),\n",
    "    \"added_features\": len([c for c in X_train_enhanced.columns if c not in features])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05601664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ç·Šæ€¥è¨ºæ–­: åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç¢ºèª\n",
      "ç›®æ¨™: 0.647-0.650ãƒ¬ãƒ™ãƒ«ã®å›å¾©\n",
      "\n",
      "=== ã‚·ãƒ³ãƒ—ãƒ«LightGBMæ€§èƒ½ç¢ºèª ===\n",
      "ã‚·ãƒ³ãƒ—ãƒ«LightGBM F1: 0.501889 Â± 0.041225\n",
      "\n",
      "=== ã‚·ãƒ³ãƒ—ãƒ«CatBoostæ€§èƒ½ç¢ºèª ===\n",
      "ã‚·ãƒ³ãƒ—ãƒ«CatBoost F1: 0.508232 Â± 0.124559\n",
      "\n",
      "=== åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ¤å®š ===\n",
      "æœ€é«˜ã‚¹ã‚³ã‚¢: 0.508232\n",
      "åŸæ¡ˆç›®æ¨™: 0.647000\n",
      "å·®ç•°: -0.138768\n",
      "åˆ¤å®š: âŒ æ·±åˆ»ãªå•é¡Œ\n",
      "æ¨å¥¨: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰è¦‹ç›´ã—å¿…è¦\n",
      "\n",
      "=== å•é¡Œç‰¹å®šã®æ‰‹ãŒã‹ã‚Š ===\n",
      "âš ï¸ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¾ãŸã¯CVåˆ†å‰²ã«å•é¡Œã®å¯èƒ½æ€§\n",
      "  - ã‚»ãƒ«5: PREPéƒ¨åˆ†ã®ç¢ºèª\n",
      "  - ã‚»ãƒ«6: KFOLDSéƒ¨åˆ†ã®ç¢ºèª\n",
      "  - TARGET_COL, ID_COLã®ç¢ºèª\n",
      "\n",
      "=== ç·Šæ€¥å¯¾ç­–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
      "ğŸš¨ ãƒ‘ã‚¿ãƒ¼ãƒ³C: åŸºç¤éƒ¨åˆ†ã®è¦‹ç›´ã—\n",
      "1. ã‚»ãƒ«5: PREPéƒ¨åˆ†ã®ç¢ºèª\n",
      "2. ã‚»ãƒ«6: CVåˆ†å‰²ã®ç¢ºèª\n",
      "3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\n",
      "\n",
      "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: fundamental_review\n",
      "æœŸå¾…æ”¹å–„: 0.508 â†’ 0.647+ (å·®åˆ† +0.139)\n"
     ]
    }
   ],
   "source": [
    "# ç·Šæ€¥è¨ºæ–­: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ç¢ºèª\n",
    "\n",
    "print(\"ğŸ” ç·Šæ€¥è¨ºæ–­: åŸæ¡ˆãƒ¬ãƒ™ãƒ«å›å¾©ã®ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ç¢ºèª\")\n",
    "print(\"ç›®æ¨™: 0.647-0.650ãƒ¬ãƒ™ãƒ«ã®å›å¾©\")\n",
    "\n",
    "# === ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã§ç¢ºèª ===\n",
    "\n",
    "def test_simple_models():\n",
    "    \"\"\"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§æœŸå¾…æ€§èƒ½ã‚’ç¢ºèª\"\"\"\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "    def prep_for_models(X):\n",
    "        X_prep = X.copy()\n",
    "        for c in X_prep.columns:\n",
    "            if X_prep[c].dtype == 'object':\n",
    "                X_prep[c] = X_prep[c].astype('category')\n",
    "        return X_prep\n",
    "    \n",
    "    X_test_data = prep_for_models(X_train_enhanced)\n",
    "    \n",
    "    # === 1. ã‚·ãƒ³ãƒ—ãƒ«LightGBM ===\n",
    "    print(\"\\n=== ã‚·ãƒ³ãƒ—ãƒ«LightGBMæ€§èƒ½ç¢ºèª ===\")\n",
    "    \n",
    "    simple_lgb = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=1000,\n",
    "        num_leaves=31,\n",
    "        reg_alpha=1,\n",
    "        reg_lambda=1,\n",
    "        random_state=SEED,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_scores = cross_val_score(simple_lgb, X_test_data, y_train, cv=5, scoring='f1')\n",
    "    lgb_mean = lgb_scores.mean()\n",
    "    lgb_std = lgb_scores.std()\n",
    "    \n",
    "    print(f\"ã‚·ãƒ³ãƒ—ãƒ«LightGBM F1: {lgb_mean:.6f} Â± {lgb_std:.6f}\")\n",
    "    \n",
    "    # === 2. ã‚·ãƒ³ãƒ—ãƒ«CatBoost ===\n",
    "    print(\"\\n=== ã‚·ãƒ³ãƒ—ãƒ«CatBoostæ€§èƒ½ç¢ºèª ===\")\n",
    "    \n",
    "    from catboost import CatBoostClassifier\n",
    "    \n",
    "    # CatBoostç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ–‡å­—åˆ—å‹ã«å¤‰æ›ï¼‰\n",
    "    X_cb_data = X_train_enhanced.copy()\n",
    "    for c in X_cb_data.columns:\n",
    "        if X_cb_data[c].dtype == 'object' or 'category' in str(X_cb_data[c].dtype):\n",
    "            X_cb_data[c] = X_cb_data[c].astype(str).fillna(\"MISSING\")\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®ç‰¹å®š\n",
    "    cat_features = [i for i, col in enumerate(X_cb_data.columns) \n",
    "                   if X_cb_data[col].dtype == 'object']\n",
    "    \n",
    "    simple_cb = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        cat_features=cat_features,\n",
    "        random_seed=SEED,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    cb_scores = cross_val_score(simple_cb, X_cb_data, y_train, cv=5, scoring='f1')\n",
    "    cb_mean = cb_scores.mean()\n",
    "    cb_std = cb_scores.std()\n",
    "    \n",
    "    print(f\"ã‚·ãƒ³ãƒ—ãƒ«CatBoost F1: {cb_mean:.6f} Â± {cb_std:.6f}\")\n",
    "    \n",
    "    # === 3. åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ¤å®š ===\n",
    "    print(f\"\\n=== åŸæ¡ˆãƒ¬ãƒ™ãƒ«åˆ¤å®š ===\")\n",
    "    \n",
    "    best_score = max(lgb_mean, cb_mean)\n",
    "    target_score = 0.647\n",
    "    \n",
    "    print(f\"æœ€é«˜ã‚¹ã‚³ã‚¢: {best_score:.6f}\")\n",
    "    print(f\"åŸæ¡ˆç›®æ¨™: {target_score:.6f}\")\n",
    "    print(f\"å·®ç•°: {best_score - target_score:+.6f}\")\n",
    "    \n",
    "    if best_score >= target_score:\n",
    "        status = \"âœ… åŸæ¡ˆãƒ¬ãƒ™ãƒ«é”æˆå¯èƒ½\"\n",
    "        recommendation = \"ç¾åœ¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç¶™ç¶š\"\n",
    "    elif best_score >= target_score - 0.02:\n",
    "        status = \"ğŸ”„ åŸæ¡ˆãƒ¬ãƒ™ãƒ«è¿‘æ¥\"\n",
    "        recommendation = \"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã§é”æˆå¯èƒ½\"\n",
    "    elif best_score >= target_score - 0.05:\n",
    "        status = \"âš ï¸ å¤§å¹…ä¸è¶³\"\n",
    "        recommendation = \"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éƒ¨åˆ†ã®æ ¹æœ¬è¦‹ç›´ã—å¿…è¦\"\n",
    "    else:\n",
    "        status = \"âŒ æ·±åˆ»ãªå•é¡Œ\"\n",
    "        recommendation = \"ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‹ã‚‰è¦‹ç›´ã—å¿…è¦\"\n",
    "    \n",
    "    print(f\"åˆ¤å®š: {status}\")\n",
    "    print(f\"æ¨å¥¨: {recommendation}\")\n",
    "    \n",
    "    # === 4. å•é¡Œã®ç‰¹å®š ===\n",
    "    print(f\"\\n=== å•é¡Œç‰¹å®šã®æ‰‹ãŒã‹ã‚Š ===\")\n",
    "    \n",
    "    if best_score < 0.55:\n",
    "        print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¾ãŸã¯CVåˆ†å‰²ã«å•é¡Œã®å¯èƒ½æ€§\")\n",
    "        print(\"  - ã‚»ãƒ«5: PREPéƒ¨åˆ†ã®ç¢ºèª\")\n",
    "        print(\"  - ã‚»ãƒ«6: KFOLDSéƒ¨åˆ†ã®ç¢ºèª\")\n",
    "        print(\"  - TARGET_COL, ID_COLã®ç¢ºèª\")\n",
    "    elif best_score < 0.60:\n",
    "        print(\"âš ï¸ ãƒ¢ãƒ‡ãƒ«è¨­å®šã«å•é¡Œã®å¯èƒ½æ€§\")\n",
    "        print(\"  - ã‚»ãƒ«13-14: ãƒ¢ãƒ‡ãƒ«å­¦ç¿’éƒ¨åˆ†ã®ç°¡ç•¥åŒ–\")\n",
    "        print(\"  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®éåº¦ãªèª¿æ•´\")\n",
    "    else:\n",
    "        print(\"âœ… ãƒ™ãƒ¼ã‚¹ã¯å¥å…¨ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§æ”¹å–„å¯èƒ½\")\n",
    "    \n",
    "    return best_score, lgb_mean, cb_mean\n",
    "\n",
    "# è¨ºæ–­å®Ÿè¡Œ\n",
    "best_score, lgb_score, cb_score = test_simple_models()\n",
    "\n",
    "# === 5. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ ===\n",
    "print(f\"\\n=== ç·Šæ€¥å¯¾ç­–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\")\n",
    "\n",
    "if best_score >= 0.60:\n",
    "    print(\"ğŸ¯ ãƒ‘ã‚¿ãƒ¼ãƒ³A: ãƒ¢ãƒ‡ãƒ«éƒ¨åˆ†ã®æœ€é©åŒ–\")\n",
    "    print(\"1. ã‚»ãƒ«13: CatBoostã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–\")\n",
    "    print(\"2. ã‚»ãƒ«14: LightGBMã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–\")  \n",
    "    print(\"3. ã‚»ãƒ«15-16: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ã‚·ãƒ³ãƒ—ãƒ«åŒ–\")\n",
    "    next_action = \"model_optimization\"\n",
    "    \n",
    "elif best_score >= 0.55:\n",
    "    print(\"ğŸ”§ ãƒ‘ã‚¿ãƒ¼ãƒ³B: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ\")\n",
    "    print(\"1. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æˆ»ã™\")\n",
    "    print(\"2. è»½å¾®ãªèª¿æ•´ã®ã¿é©ç”¨\")\n",
    "    print(\"3. è¤‡é›‘ãªæœ€é©åŒ–ã‚’é™¤å»\")\n",
    "    next_action = \"parameter_reset\"\n",
    "    \n",
    "else:\n",
    "    print(\"ğŸš¨ ãƒ‘ã‚¿ãƒ¼ãƒ³C: åŸºç¤éƒ¨åˆ†ã®è¦‹ç›´ã—\")\n",
    "    print(\"1. ã‚»ãƒ«5: PREPéƒ¨åˆ†ã®ç¢ºèª\")\n",
    "    print(\"2. ã‚»ãƒ«6: CVåˆ†å‰²ã®ç¢ºèª\")\n",
    "    print(\"3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\")\n",
    "    next_action = \"fundamental_review\"\n",
    "\n",
    "print(f\"\\næ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {next_action}\")\n",
    "print(f\"æœŸå¾…æ”¹å–„: {best_score:.3f} â†’ 0.647+ (å·®åˆ† {0.647-best_score:+.3f})\")\n",
    "\n",
    "# çµæœä¿å­˜\n",
    "DIAGNOSTIC_RESULT = {\n",
    "    \"best_score\": best_score,\n",
    "    \"lgb_score\": lgb_score, \n",
    "    \"cb_score\": cb_score,\n",
    "    \"next_action\": next_action,\n",
    "    \"target_gap\": 0.647 - best_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3a8eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ ç·Šæ€¥åŸºç¤è¨ºæ–­: 0.508 â†’ 0.647ã¸ã®æ ¹æœ¬å•é¡Œè§£æ±º\n",
      "åŸæ¡ˆã¨ã®å·®ç•°: -0.139 (21%ä½ä¸‹)\n",
      "\n",
      "=== 1. åŸºæœ¬è¨­å®šç¢ºèª ===\n",
      "TARGET_COL: LoanStatus\n",
      "ID_COL: id\n",
      "SEED: 42\n",
      "\n",
      "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: (7552, 16)\n",
      "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: (7552, 15)\n",
      "X_train: (7552, 15)\n",
      "y_train: (7552,)\n",
      "\n",
      "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {0: np.int64(6588), 1: np.int64(964)}\n",
      "æ­£ä¾‹ç‡: 0.1276 (12.8%)\n",
      "\n",
      "=== 2. ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª ===\n",
      "âœ… æ¬ æå€¤ãªã—\n",
      "é‡è¤‡ID: 0å€‹\n",
      "\n",
      "=== 3. å‰å‡¦ç†ç¢ºèª ===\n",
      "featuresæ•°: 15\n",
      "cat_colsæ•°: 6\n",
      "features: ['id', 'GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram']...\n",
      "cat_cols: ['Subprogram', 'FixedOrVariableInterestInd', 'NaicsSector', 'BusinessType', 'BusinessAge', 'CollateralInd']\n",
      "X_train_enhanced shape: (7552, 23)\n",
      "ã‚«ãƒ©ãƒ ä¾‹: ['id', 'GrossApproval', 'SBAGuaranteedApproval', 'ApprovalFiscalYear', 'Subprogram', 'InitialInterestRate', 'FixedOrVariableInterestInd', 'TermInMonths', 'NaicsSector', 'CongressionalDistrict']\n",
      "ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ: {dtype('int64'): np.int64(14), dtype('O'): np.int64(6), dtype('float64'): np.int64(3)}\n",
      "ç„¡é™å€¤: 0å€‹, NaN: 0å€‹\n",
      "\n",
      "=== 4. CVåˆ†å‰²ç¢ºèª ===\n",
      "CVåˆ†å‰²: 5 folds\n",
      "Foldåˆ†å¸ƒ:\n",
      "  Fold 1: è¨“ç·´0.128, æ¤œè¨¼0.128 (ã‚µã‚¤ã‚º: 6041, 1511)\n",
      "  Fold 2: è¨“ç·´0.128, æ¤œè¨¼0.128 (ã‚µã‚¤ã‚º: 6041, 1511)\n",
      "  Fold 3: è¨“ç·´0.128, æ¤œè¨¼0.128 (ã‚µã‚¤ã‚º: 6042, 1510)\n",
      "  Fold 4: è¨“ç·´0.128, æ¤œè¨¼0.128 (ã‚µã‚¤ã‚º: 6042, 1510)\n",
      "  Fold 5: è¨“ç·´0.128, æ¤œè¨¼0.127 (ã‚µã‚¤ã‚º: 6042, 1510)\n",
      "\n",
      "=== 5. æœ€ã‚‚åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼ ===\n",
      "æœ€åŸºæœ¬RandomForest F1: 0.361062\n",
      "âŒ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚‚ä½ã„ â†’ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã«å•é¡Œ\n",
      "\n",
      "=== 6. å•é¡Œç®‡æ‰€ã®ç‰¹å®š ===\n",
      "âœ… æ˜ç¢ºãªå•é¡Œã¯è¦‹ã¤ã‹ã‚‰ãš\n",
      "\n",
      "=== 7. ç·Šæ€¥ä¿®æ­£ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
      "ğŸš¨ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®æ ¹æœ¬è¦‹ç›´ã—\n",
      "  - TARGET_COL ã®ç¢ºèª\n",
      "  - ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\n",
      "  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®šç¾©ã®ç¢ºèª\n",
      "\n",
      "æœ€å„ªå…ˆä¿®æ­£: ãƒ‡ãƒ¼ã‚¿\n"
     ]
    }
   ],
   "source": [
    "# ç·Šæ€¥åŸºç¤è¨ºæ–­: ãƒ‡ãƒ¼ã‚¿ãƒ»å‰å‡¦ç†ãƒ»è¨­å®šã®ç¢ºèª\n",
    "\n",
    "print(\"ğŸš¨ ç·Šæ€¥åŸºç¤è¨ºæ–­: 0.508 â†’ 0.647ã¸ã®æ ¹æœ¬å•é¡Œè§£æ±º\")\n",
    "print(\"åŸæ¡ˆã¨ã®å·®ç•°: -0.139 (21%ä½ä¸‹)\")\n",
    "\n",
    "# === 1. åŸºæœ¬è¨­å®šã®ç¢ºèª ===\n",
    "print(\"\\n=== 1. åŸºæœ¬è¨­å®šç¢ºèª ===\")\n",
    "\n",
    "print(f\"TARGET_COL: {TARGET_COL}\")\n",
    "print(f\"ID_COL: {ID_COL}\")\n",
    "print(f\"SEED: {SEED}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª\n",
    "print(f\"\\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {train.shape}\")\n",
    "print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {test.shape}\")\n",
    "print(f\"X_train: {X_train.shape if 'X_train' in locals() else 'undefined'}\")\n",
    "print(f\"y_train: {y_train.shape if 'y_train' in locals() else 'undefined'}\")\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒç¢ºèª\n",
    "if TARGET_COL in train.columns:\n",
    "    target_dist = train[TARGET_COL].value_counts()\n",
    "    target_rate = train[TARGET_COL].mean()\n",
    "    print(f\"\\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒ: {dict(target_dist)}\")\n",
    "    print(f\"æ­£ä¾‹ç‡: {target_rate:.4f} ({target_rate*100:.1f}%)\")\n",
    "    \n",
    "    if target_rate < 0.05 or target_rate > 0.95:\n",
    "        print(\"âš ï¸ æ¥µç«¯ãªä¸å‡è¡¡: ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã«å•é¡Œã®å¯èƒ½æ€§\")\n",
    "else:\n",
    "    print(f\"âš ï¸ TARGET_COL '{TARGET_COL}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„\")\n",
    "\n",
    "# === 2. ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª ===\n",
    "print(\"\\n=== 2. ãƒ‡ãƒ¼ã‚¿å“è³ªç¢ºèª ===\")\n",
    "\n",
    "# æ¬ æå€¤ç¢ºèª\n",
    "missing_train = train.isnull().sum()\n",
    "missing_critical = missing_train[missing_train > 0]\n",
    "if len(missing_critical) > 0:\n",
    "    print(\"æ¬ æå€¤ã®ã‚ã‚‹åˆ—:\")\n",
    "    for col, count in missing_critical.items():\n",
    "        print(f\"  {col}: {count}å€‹ ({count/len(train)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"âœ… æ¬ æå€¤ãªã—\")\n",
    "\n",
    "# é‡è¤‡ç¢ºèª\n",
    "if ID_COL in train.columns:\n",
    "    duplicate_ids = train[ID_COL].duplicated().sum()\n",
    "    print(f\"é‡è¤‡ID: {duplicate_ids}å€‹\")\n",
    "    if duplicate_ids > 0:\n",
    "        print(\"âš ï¸ IDã®é‡è¤‡ã‚ã‚Š\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ID_COL '{ID_COL}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„\")\n",
    "\n",
    "# === 3. å‰å‡¦ç†ç¢ºèª ===\n",
    "print(\"\\n=== 3. å‰å‡¦ç†ç¢ºèª ===\")\n",
    "\n",
    "print(f\"featuresæ•°: {len(features) if 'features' in locals() else 'undefined'}\")\n",
    "print(f\"cat_colsæ•°: {len(cat_cols) if 'cat_cols' in locals() else 'undefined'}\")\n",
    "\n",
    "if 'features' in locals():\n",
    "    print(f\"features: {features[:5]}...\" if len(features) > 5 else f\"features: {features}\")\n",
    "\n",
    "if 'cat_cols' in locals():\n",
    "    print(f\"cat_cols: {cat_cols}\")\n",
    "\n",
    "# X_train_enhancedã®ç¢ºèª\n",
    "if 'X_train_enhanced' in locals():\n",
    "    print(f\"X_train_enhanced shape: {X_train_enhanced.shape}\")\n",
    "    print(f\"ã‚«ãƒ©ãƒ ä¾‹: {list(X_train_enhanced.columns)[:10]}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª\n",
    "    dtypes_summary = X_train_enhanced.dtypes.value_counts()\n",
    "    print(f\"ãƒ‡ãƒ¼ã‚¿å‹åˆ†å¸ƒ: {dict(dtypes_summary)}\")\n",
    "    \n",
    "    # ç„¡é™å€¤ãƒ»NaNç¢ºèª\n",
    "    inf_count = np.isinf(X_train_enhanced.select_dtypes(include=[np.number])).sum().sum()\n",
    "    nan_count = X_train_enhanced.isnull().sum().sum()\n",
    "    print(f\"ç„¡é™å€¤: {inf_count}å€‹, NaN: {nan_count}å€‹\")\n",
    "    \n",
    "    if inf_count > 0 or nan_count > 0:\n",
    "        print(\"âš ï¸ ç„¡é™å€¤ã¾ãŸã¯NaNãŒæ®‹å­˜\")\n",
    "\n",
    "# === 4. CVåˆ†å‰²ç¢ºèª ===\n",
    "print(\"\\n=== 4. CVåˆ†å‰²ç¢ºèª ===\")\n",
    "\n",
    "if 'skf_full' in locals():\n",
    "    print(f\"CVåˆ†å‰²: {skf_full.n_splits} folds\")\n",
    "    \n",
    "    # å„foldã®åˆ†å¸ƒç¢ºèª\n",
    "    splits = list(skf_full.split(X_train_enhanced if 'X_train_enhanced' in locals() else X_train, y_train))\n",
    "    print(\"Foldåˆ†å¸ƒ:\")\n",
    "    for i, (tr_idx, va_idx) in enumerate(splits):\n",
    "        tr_rate = y_train[tr_idx].mean()\n",
    "        va_rate = y_train[va_idx].mean()\n",
    "        print(f\"  Fold {i+1}: è¨“ç·´{tr_rate:.3f}, æ¤œè¨¼{va_rate:.3f} (ã‚µã‚¤ã‚º: {len(tr_idx)}, {len(va_idx)})\")\n",
    "        \n",
    "        # åˆ†å¸ƒã®å·®ãŒå¤§ãã„å ´åˆã¯è­¦å‘Š\n",
    "        if abs(tr_rate - va_rate) > 0.02:\n",
    "            print(f\"    âš ï¸ åˆ†å¸ƒå·®ãŒå¤§ãã„: {abs(tr_rate - va_rate):.3f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ skf_full ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„\")\n",
    "\n",
    "# === 5. åŸºæœ¬ãƒ¢ãƒ‡ãƒ«å†æ¤œè¨¼ ===\n",
    "print(\"\\n=== 5. æœ€ã‚‚åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã§ã®æ¤œè¨¼ ===\")\n",
    "\n",
    "# æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "try:\n",
    "    # å…ƒã®featuresã®ã¿ä½¿ç”¨\n",
    "    X_basic = train[features].copy()\n",
    "    \n",
    "    # æœ€å°é™ã®å‰å‡¦ç†\n",
    "    for c in cat_cols:\n",
    "        if c in X_basic.columns:\n",
    "            X_basic[c] = X_basic[c].astype(str).fillna(\"MISSING\")\n",
    "    \n",
    "    # ã‚«ãƒ†ã‚´ãƒªã‚’LabelEncodingã§æ•°å€¤åŒ–\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    X_basic_numeric = X_basic.copy()\n",
    "    for c in cat_cols:\n",
    "        if c in X_basic_numeric.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_basic_numeric[c] = le.fit_transform(X_basic_numeric[c])\n",
    "    \n",
    "    # æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    simple_rf = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=SEED, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rf_scores = cross_val_score(simple_rf, X_basic_numeric, y_train, cv=3, scoring='f1')\n",
    "    rf_mean = rf_scores.mean()\n",
    "    \n",
    "    print(f\"æœ€åŸºæœ¬RandomForest F1: {rf_mean:.6f}\")\n",
    "    \n",
    "    if rf_mean > 0.60:\n",
    "        print(\"âœ… åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã¯å¥å…¨ â†’ LightGBM/CatBoostã®è¨­å®šå•é¡Œ\")\n",
    "    elif rf_mean > 0.50:\n",
    "        print(\"ğŸ”„ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã¯ä¸­ç¨‹åº¦ â†’ å‰å‡¦ç†ã®æ”¹å–„ä½™åœ°ã‚ã‚Š\")\n",
    "    else:\n",
    "        print(\"âŒ åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚‚ä½ã„ â†’ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã«å•é¡Œ\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"åŸºæœ¬ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# === 6. å•é¡Œç®‡æ‰€ã®ç‰¹å®š ===\n",
    "print(\"\\n=== 6. å•é¡Œç®‡æ‰€ã®ç‰¹å®š ===\")\n",
    "\n",
    "issues = []\n",
    "\n",
    "# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–¢é€£\n",
    "if 'target_rate' in locals() and (target_rate < 0.05 or target_rate > 0.95):\n",
    "    issues.append(\"æ¥µç«¯ãªä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿\")\n",
    "\n",
    "# æ¬ æãƒ»ç•°å¸¸å€¤\n",
    "if 'inf_count' in locals() and inf_count > 0:\n",
    "    issues.append(\"ç„¡é™å€¤ã®æ®‹å­˜\")\n",
    "if 'nan_count' in locals() and nan_count > 0:\n",
    "    issues.append(\"NaNã®æ®‹å­˜\")\n",
    "\n",
    "# CVåˆ†å‰²\n",
    "if 'splits' in locals():\n",
    "    max_diff = max([abs(y_train[tr].mean() - y_train[va].mean()) for tr, va in splits])\n",
    "    if max_diff > 0.02:\n",
    "        issues.append(\"CVåˆ†å‰²ã®ä¸å‡è¡¡\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º\n",
    "if 'X_train_enhanced' in locals() and len(X_train_enhanced) < 1000:\n",
    "    issues.append(\"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºä¸è¶³\")\n",
    "\n",
    "if issues:\n",
    "    print(\"âš ï¸ ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"âœ… æ˜ç¢ºãªå•é¡Œã¯è¦‹ã¤ã‹ã‚‰ãš\")\n",
    "\n",
    "# === 7. æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
    "print(\"\\n=== 7. ç·Šæ€¥ä¿®æ­£ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\")\n",
    "\n",
    "if 'rf_mean' in locals():\n",
    "    if rf_mean > 0.60:\n",
    "        print(\"ğŸ¯ LightGBM/CatBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¦‹ç›´ã—\")\n",
    "        print(\"  - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æˆ»ã™\")\n",
    "        print(\"  - early_stoppingè¨­å®šç¢ºèª\")\n",
    "        print(\"  - ã‚«ãƒ†ã‚´ãƒªå‡¦ç†æ–¹æ³•ã®è¦‹ç›´ã—\")\n",
    "    elif rf_mean > 0.50:\n",
    "        print(\"ğŸ”§ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¦‹ç›´ã—\")\n",
    "        print(\"  - ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®å‡¦ç†æ–¹æ³•\")\n",
    "        print(\"  - ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\")\n",
    "        print(\"  - ç•°å¸¸å€¤å‡¦ç†\")\n",
    "    else:\n",
    "        print(\"ğŸš¨ ãƒ‡ãƒ¼ã‚¿è‡ªä½“ã®æ ¹æœ¬è¦‹ç›´ã—\")\n",
    "        print(\"  - TARGET_COL ã®ç¢ºèª\")\n",
    "        print(\"  - ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®ç¢ºèª\")\n",
    "        print(\"  - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®šç¾©ã®ç¢ºèª\")\n",
    "\n",
    "print(f\"\\næœ€å„ªå…ˆä¿®æ­£: {'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿' if 'rf_mean' in locals() and rf_mean > 0.60 else 'å‰å‡¦ç†' if 'rf_mean' in locals() and rf_mean > 0.50 else 'ãƒ‡ãƒ¼ã‚¿'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242f0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å¼·åŒ–ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ ===\n",
      "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ F1: 0.454374 Â± 0.050504\n",
      "å¼·åŒ–ç‰¹å¾´é‡ F1: 0.457616 Â± 0.053005\n",
      "\n",
      "=== åŠ¹æœåˆ¤å®š ===\n",
      "æ”¹å–„åº¦: +0.003242 (+0.71%)\n",
      "çµ±è¨ˆçš„è©•ä¾¡: æœ‰æ„æ€§ä¸æ˜ç¢º\n",
      "âœ… æ”¹å–„ã‚ã‚Šï¼å¼·åŒ–ç‰¹å¾´é‡ã‚’æ¡ç”¨\n",
      "\n",
      "=== é‡è¦æ–°ç‰¹å¾´é‡ã®ç¢ºèª ===\n",
      "é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™(â‰¥2)ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: 17.113%\n",
      "å…¨ä½“ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: 12.765%\n",
      "ãƒªã‚¹ã‚¯å€ç‡: 1.3å€\n",
      "å°é¡èè³‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: 15.675%\n",
      "å¤§å£èè³‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: 8.419%\n",
      "\n",
      "=== æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\n",
      "âœ“ å¼·åŒ–ç‰¹å¾´é‡ã‚’é©ç”¨ã—ã¦ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ï¼ˆã‚»ãƒ«9.5ã§å®Ÿè¡Œï¼‰\n",
      "âœ“ ãã®å¾Œã€æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\n"
     ]
    }
   ],
   "source": [
    "# # ã‚»ãƒ«9: ç‰¹å¾´é‡åŠ¹æœæ¤œè¨¼\n",
    "\n",
    "# # === å¼·åŒ–ç‰¹å¾´é‡ã®é«˜é€Ÿæ¤œè¨¼ ===\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# def enhanced_feature_test():\n",
    "#     \"\"\"ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ãç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼\"\"\"\n",
    "    \n",
    "#     print(\"=== å¼·åŒ–ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ ===\")\n",
    "    \n",
    "#     # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†\n",
    "#     def prep_for_lgb(X):\n",
    "#         X_prep = X.copy()\n",
    "#         for c in X_prep.columns:\n",
    "#             if X_prep[c].dtype == 'object':\n",
    "#                 X_prep[c] = X_prep[c].astype('category')\n",
    "#         return X_prep\n",
    "    \n",
    "#     # é«˜é€Ÿãƒ¢ãƒ‡ãƒ«\n",
    "#     quick_model = LGBMClassifier(\n",
    "#         objective=\"binary\",\n",
    "#         learning_rate=0.1,\n",
    "#         num_leaves=31,\n",
    "#         n_estimators=100,\n",
    "#         random_state=SEED,\n",
    "#         verbose=-1,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "    \n",
    "#     # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…ƒã®ç‰¹å¾´é‡ï¼‰\n",
    "#     X_train_orig_prep = prep_for_lgb(train[features])\n",
    "#     baseline_scores = cross_val_score(\n",
    "#         quick_model, X_train_orig_prep, y_train, \n",
    "#         cv=3, scoring='f1', n_jobs=-1\n",
    "#     )\n",
    "#     baseline_f1 = baseline_scores.mean()\n",
    "#     print(f\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ F1: {baseline_f1:.6f} Â± {baseline_scores.std():.6f}\")\n",
    "    \n",
    "#     # 2. å¼·åŒ–ç‰¹å¾´é‡ç‰ˆ\n",
    "#     X_train_enhanced_prep = prep_for_lgb(X_train_enhanced)\n",
    "#     enhanced_scores = cross_val_score(\n",
    "#         quick_model, X_train_enhanced_prep, y_train,\n",
    "#         cv=3, scoring='f1', n_jobs=-1\n",
    "#     )\n",
    "#     enhanced_f1 = enhanced_scores.mean()\n",
    "#     print(f\"å¼·åŒ–ç‰¹å¾´é‡ F1: {enhanced_f1:.6f} Â± {enhanced_scores.std():.6f}\")\n",
    "    \n",
    "#     # 3. æ”¹å–„åº¦è©•ä¾¡\n",
    "#     improvement = enhanced_f1 - baseline_f1\n",
    "#     print(f\"\\n=== åŠ¹æœåˆ¤å®š ===\")\n",
    "#     print(f\"æ”¹å–„åº¦: {improvement:+.6f} ({improvement/baseline_f1*100:+.2f}%)\")\n",
    "    \n",
    "#     # çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯\n",
    "#     if improvement > 2 * np.sqrt(enhanced_scores.var() + baseline_scores.var()):\n",
    "#         significance = \"çµ±è¨ˆçš„ã«æœ‰æ„\"\n",
    "#     else:\n",
    "#         significance = \"æœ‰æ„æ€§ä¸æ˜ç¢º\"\n",
    "    \n",
    "#     print(f\"çµ±è¨ˆçš„è©•ä¾¡: {significance}\")\n",
    "    \n",
    "#     # åˆ¤å®š\n",
    "#     if improvement > 0.01:  # 1%ä»¥ä¸Šæ”¹å–„\n",
    "#         print(\"âœ… å¤§å¹…æ”¹å–„ï¼å¼·åŒ–ç‰¹å¾´é‡ã‚’æ¡ç”¨æ¨å¥¨\")\n",
    "#         return True, \"major_improvement\"\n",
    "#     elif improvement > 0.003:  # 0.3%ä»¥ä¸Šæ”¹å–„\n",
    "#         print(\"âœ… æ”¹å–„ã‚ã‚Šï¼å¼·åŒ–ç‰¹å¾´é‡ã‚’æ¡ç”¨\")\n",
    "#         return True, \"moderate_improvement\"\n",
    "#     elif improvement > 0:\n",
    "#         print(\"â–³ å¾®å°æ”¹å–„ã€‚æ¡ç”¨ã‚’æ¤œè¨\")\n",
    "#         return True, \"minor_improvement\"\n",
    "#     else:\n",
    "#         print(\"âŒ æ”¹å–„ãªã—ã€‚å…ƒã®ç‰¹å¾´é‡ã‚’ç¶­æŒ\")\n",
    "#         return False, \"no_improvement\"\n",
    "\n",
    "# # æ¤œè¨¼å®Ÿè¡Œ\n",
    "# is_beneficial, improvement_level = enhanced_feature_test()\n",
    "\n",
    "# # é‡è¦ãªç‰¹å¾´é‡ã®å€‹åˆ¥ç¢ºèª\n",
    "# if is_beneficial:\n",
    "#     print(f\"\\n=== é‡è¦æ–°ç‰¹å¾´é‡ã®ç¢ºèª ===\")\n",
    "    \n",
    "#     # é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™ã®åŠ¹æœ\n",
    "#     if 'high_risk_combo' in X_train_enhanced.columns:\n",
    "#         high_risk_samples = X_train_enhanced['high_risk_combo'] >= 2\n",
    "#         if high_risk_samples.sum() > 10:  # ååˆ†ãªã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚‹å ´åˆ\n",
    "#             high_risk_default_rate = y_train[high_risk_samples].mean()\n",
    "#             overall_default_rate = y_train.mean()\n",
    "#             print(f\"é«˜ãƒªã‚¹ã‚¯è¤‡åˆæŒ‡æ¨™(â‰¥2)ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: {high_risk_default_rate:.3%}\")\n",
    "#             print(f\"å…¨ä½“ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: {overall_default_rate:.3%}\")\n",
    "#             print(f\"ãƒªã‚¹ã‚¯å€ç‡: {high_risk_default_rate/overall_default_rate:.1f}å€\")\n",
    "    \n",
    "#     # å°é¡èè³‡ã®åŠ¹æœ\n",
    "#     if 'is_small_loan' in X_train_enhanced.columns:\n",
    "#         small_loan_default_rate = y_train[X_train_enhanced['is_small_loan'] == 1].mean()\n",
    "#         large_loan_default_rate = y_train[X_train_enhanced['is_small_loan'] == 0].mean()\n",
    "#         print(f\"å°é¡èè³‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: {small_loan_default_rate:.3%}\")\n",
    "#         print(f\"å¤§å£èè³‡ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡: {large_loan_default_rate:.3%}\")\n",
    "\n",
    "# print(f\"\\n=== æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ===\")\n",
    "# if is_beneficial:\n",
    "#     print(\"âœ“ å¼·åŒ–ç‰¹å¾´é‡ã‚’é©ç”¨ã—ã¦ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ï¼ˆã‚»ãƒ«9.5ã§å®Ÿè¡Œï¼‰\")\n",
    "#     print(\"âœ“ ãã®å¾Œã€æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\")\n",
    "# else:\n",
    "#     print(\"âœ“ å…ƒã®ç‰¹å¾´é‡ã«æˆ»ã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f529c9",
   "metadata": {},
   "source": [
    "# 3.3 å¼·åŒ–ç‰¹å¾´é‡ã®é©ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcdc17f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å¼·åŒ–ç‰¹å¾´é‡ã‚’é©ç”¨ ===\n",
      "âœ“ ç‰¹å¾´é‡ã‚’æ›´æ–°: 36åˆ—\n",
      "âœ“ ã‚«ãƒ†ã‚´ãƒªåˆ—: 6åˆ—\n",
      "âœ“ æ”¹å–„ãƒ¬ãƒ™ãƒ«: moderate_improvement\n",
      "âœ“ æ¬¡: ã‚»ãƒ«9.5ã§ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰\n",
      "\n",
      "=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº† ===\n",
      "é©ç”¨çŠ¶æ³: å¼·åŒ–ç‰¹å¾´é‡\n",
      "æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«10: å¼·åŒ–ç‰¹å¾´é‡é©ç”¨\n",
    "\n",
    "# === å¼·åŒ–ç‰¹å¾´é‡ã®é©ç”¨ ===\n",
    "\n",
    "# æ¤œè¨¼çµæœã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’é©ç”¨\n",
    "if 'is_beneficial' in locals() and is_beneficial:\n",
    "    print(\"=== å¼·åŒ–ç‰¹å¾´é‡ã‚’é©ç”¨ ===\")\n",
    "    \n",
    "    # ç‰¹å¾´é‡ã‚’æ›´æ–°\n",
    "    X_train = X_train_enhanced.copy()\n",
    "    X_test = X_test_enhanced.copy()\n",
    "    cat_cols = cat_cols_enhanced.copy()\n",
    "    cat_features_idx = cat_features_idx_enhanced.copy()\n",
    "    \n",
    "    print(f\"âœ“ ç‰¹å¾´é‡ã‚’æ›´æ–°: {len(X_train.columns)}åˆ—\")\n",
    "    print(f\"âœ“ ã‚«ãƒ†ã‚´ãƒªåˆ—: {len(cat_features_idx)}åˆ—\")\n",
    "    \n",
    "    # å®Ÿé¨“è¨˜éŒ²\n",
    "    ENHANCED_FEATURES_APPLIED = True\n",
    "    ENHANCEMENT_LEVEL = improvement_level\n",
    "    \n",
    "    print(f\"âœ“ æ”¹å–„ãƒ¬ãƒ™ãƒ«: {improvement_level}\")\n",
    "    print(f\"âœ“ æ¬¡: ã‚»ãƒ«9.5ã§ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰\")\n",
    "    \n",
    "else:\n",
    "    print(\"=== å…ƒã®ç‰¹å¾´é‡ã‚’ç¶­æŒ ===\")\n",
    "    print(\"å¼·åŒ–ç‰¹å¾´é‡ã®åŠ¹æœãŒä¸ååˆ†ã®ãŸã‚ã€å…ƒã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨\")\n",
    "    \n",
    "    # å…ƒã®ç‰¹å¾´é‡ã‚’ç¢ºèª\n",
    "    if 'features' in locals():\n",
    "        print(f\"âœ“ å…ƒã®ç‰¹å¾´é‡: {len(features)}åˆ—\")\n",
    "        print(f\"âœ“ æ¬¡: æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ\")\n",
    "    \n",
    "    ENHANCED_FEATURES_APPLIED = False\n",
    "    ENHANCEMENT_LEVEL = \"not_applied\"\n",
    "\n",
    "print(f\"\\n=== ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº† ===\")\n",
    "print(f\"é©ç”¨çŠ¶æ³: {'å¼·åŒ–ç‰¹å¾´é‡' if ENHANCED_FEATURES_APPLIED else 'å…ƒã®ç‰¹å¾´é‡'}\")\n",
    "print(f\"æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: {'ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰' if ENHANCED_FEATURES_APPLIED else 'æ—¢å­˜ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç¶šè¡Œ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d673b",
   "metadata": {},
   "source": [
    "# 4. ãƒ—ãƒ¼ãƒ«æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6edc2c",
   "metadata": {},
   "source": [
    "# 4.1 CatBoost Poolæ§‹ç¯‰é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36eea9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚»ãƒ«11: BUILD POOLS\n",
    "\n",
    "# === BUILD POOLS: pools_tune / pools_full / test_pool ===\n",
    "\n",
    "\n",
    "# å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã€foldã”ã¨ã«Poolã‚’å‰è¨ˆç®—ã—ã¦ãŠãï¼ˆä½œæˆã‚³ã‚¹ãƒˆã¨å‰å‡¦ç†ã®ã°ã‚‰ã¤ãã‚’å‰Šæ¸›ï¼‰\n",
    "def build_pools(X, y, skf, cat_idx):\n",
    "    pools = []\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        pools.append((\n",
    "            Pool(X_tr, y_tr, cat_features=cat_idx),\n",
    "            Pool(X_va, y_va, cat_features=cat_idx),\n",
    "            va_idx\n",
    "        ))\n",
    "    return pools\n",
    "\n",
    "pools_tune = build_pools(X_tune, y_tune, skf_tune, cat_features_idx)\n",
    "pools_full = build_pools(X_train, y_train, skf_full, cat_features_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c88388",
   "metadata": {},
   "source": [
    "# 4.2 å¼·åŒ–ç‰¹å¾´é‡ã§ã®ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dbf5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ç‰¹å¾´é‡è¿½åŠ å¾Œã®ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ ===\n",
      "TUNE SUBSET poolså†æ§‹ç¯‰å®Œäº†: 4531 rows, 36 features\n",
      "FULL poolså†æ§‹ç¯‰å®Œäº†: 7552 rows, 36 features\n",
      "ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: 6\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«12: ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰\n",
    "\n",
    "# === ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¾Œï¼‰ã‚»ãƒ«9.5 ===\n",
    "\n",
    "print(\"=== ç‰¹å¾´é‡è¿½åŠ å¾Œã®ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰ ===\")\n",
    "\n",
    "# tuningç”¨ã®ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰\n",
    "if FAST_TUNE:\n",
    "    X_tune_new = X_train.iloc[idx_tune].reset_index(drop=True)\n",
    "    pools_tune = build_pools(X_tune_new, y_tune, skf_tune, cat_features_idx)\n",
    "    print(f\"TUNE SUBSET poolså†æ§‹ç¯‰å®Œäº†: {len(X_tune_new)} rows, {len(X_tune_new.columns)} features\")\n",
    "else:\n",
    "    pools_tune = build_pools(X_train, y_train, skf_tune, cat_features_idx)\n",
    "    print(f\"TUNE poolså†æ§‹ç¯‰å®Œäº†: {len(X_train)} rows, {len(X_train.columns)} features\")\n",
    "\n",
    "# fullç”¨ã®ãƒ—ãƒ¼ãƒ«å†æ§‹ç¯‰  \n",
    "pools_full = build_pools(X_train, y_train, skf_full, cat_features_idx)\n",
    "print(f\"FULL poolså†æ§‹ç¯‰å®Œäº†: {len(X_train)} rows, {len(X_train.columns)} features\")\n",
    "print(f\"ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•°: {len(cat_features_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c18438",
   "metadata": {},
   "source": [
    "# 5. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee1c8a",
   "metadata": {},
   "source": [
    "# 5.1 CatBoostå­¦ç¿’ï¼ˆSeed Baggingï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751259cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CatBoost Seed-Baggingé–‹å§‹ ===\n",
      "ä½¿ç”¨ç‰¹å¾´é‡æ•°: 36\n",
      "Seed Bag: [42, 2025, 777]\n",
      "Fold 1/5 å®Œäº†\n",
      "Fold 2/5 å®Œäº†\n",
      "Fold 3/5 å®Œäº†\n",
      "Fold 4/5 å®Œäº†\n",
      "Fold 5/5 å®Œäº†\n",
      "CatBoost OOF F1: 0.633166 | æœ€é©é–¾å€¤: 0.3750\n",
      "CatBoost F1@0.285: 0.627674\n",
      "âœ… CatBoostå­¦ç¿’å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«13: CatBoostå­¦ç¿’ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "\n",
    "# ==== CatBoost seed-bagging ====\n",
    "\n",
    "# eval_oof_f1é–¢æ•°ã®å®šç¾©ï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
    "def eval_oof_f1(probs, y_true):\n",
    "    \"\"\"OOFäºˆæ¸¬ã‹ã‚‰æœ€é©F1ã‚¹ã‚³ã‚¢ã¨é–¾å€¤ã‚’è¨ˆç®—\"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (probs >= t).astype(int)) for t in thresholds]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return f1s[j], float(thresholds[j])\n",
    "\n",
    "SEED_BAG = [42, 2025, 777]\n",
    "\n",
    "# Version 6ã§ä½¿ç”¨ã•ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«å®šç¾©\n",
    "best_params_cb = {\n",
    "    \"learning_rate\": 0.06116108646095842,\n",
    "    \"depth\": 5,\n",
    "    \"l2_leaf_reg\": 5.478690083944246,\n",
    "    \"bagging_temperature\": 0.8884344994647464,\n",
    "    \"random_strength\": 1.865589408671679,\n",
    "    \"subsample\": 0.9516049519127788,\n",
    "    \"scale_pos_weight\": 1.1386783078556455,\n",
    "}\n",
    "\n",
    "params_cb = dict(best_params_cb)\n",
    "params_cb.update({\n",
    "    \"iterations\": 10000,\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"Logloss\",\n",
    "    \"random_seed\": SEED,\n",
    "    \"verbose\": False,\n",
    "    \"thread_count\": -1,\n",
    "    \"use_best_model\": True,\n",
    "    \"allow_writing_files\": False,\n",
    "})\n",
    "\n",
    "oof_cb = np.zeros(len(X_train), dtype=float)\n",
    "test_cb = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "print(\"=== CatBoost Seed-Baggingé–‹å§‹ ===\")\n",
    "print(f\"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(X_train.columns)}\")\n",
    "print(f\"Seed Bag: {SEED_BAG}\")\n",
    "\n",
    "for fold, (tr_pool, va_pool, va_idx) in enumerate(pools_full, 1):\n",
    "    fold_prob = np.zeros(len(va_idx))\n",
    "    fold_test = np.zeros(len(X_test))\n",
    "    \n",
    "    for sd in SEED_BAG:\n",
    "        p = dict(params_cb)\n",
    "        p[\"random_seed\"] = sd\n",
    "        m = CatBoostClassifier(**p)\n",
    "        m.fit(tr_pool, eval_set=va_pool, early_stopping_rounds=EARLY_STOP_FULL)\n",
    "        fold_prob += m.predict_proba(va_pool)[:,1] / len(SEED_BAG)\n",
    "        fold_test += m.predict_proba(Pool(X_test, cat_features=cat_features_idx))[:,1] / len(SEED_BAG)\n",
    "    \n",
    "    oof_cb[va_idx] = fold_prob\n",
    "    test_cb += fold_test / len(pools_full)\n",
    "    print(f\"Fold {fold}/5 å®Œäº†\")\n",
    "\n",
    "f1_cb, th_cb = eval_oof_f1(oof_cb, y_train)\n",
    "print(f\"CatBoost OOF F1: {f1_cb:.6f} | æœ€é©é–¾å€¤: {th_cb:.4f}\")\n",
    "\n",
    "# æå‡ºé–¾å€¤ã§ã®æ€§èƒ½ç¢ºèª\n",
    "f1_cb_submit = f1_score(y_train, (oof_cb >= 0.285).astype(int))\n",
    "print(f\"CatBoost F1@0.285: {f1_cb_submit:.6f}\")\n",
    "\n",
    "print(\"âœ… CatBoostå­¦ç¿’å®Œäº†\")\n",
    "\n",
    "#10m 9.6s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c233dd",
   "metadata": {},
   "source": [
    "# 5.2 LightGBMå­¦ç¿’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d03658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LightGBMå­¦ç¿’é–‹å§‹ ===\n",
      "ä½¿ç”¨ç‰¹å¾´é‡æ•°: 36\n",
      "Fold 1/5 å®Œäº†\n",
      "Fold 2/5 å®Œäº†\n",
      "Fold 3/5 å®Œäº†\n",
      "Fold 4/5 å®Œäº†\n",
      "Fold 5/5 å®Œäº†\n",
      "LightGBM OOF F1: 0.626445 | æœ€é©é–¾å€¤: 0.3700\n",
      "LightGBM F1@0.285: 0.626408\n",
      "âœ… LightGBMå­¦ç¿’å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«14: LightGBMå­¦ç¿’ï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "\n",
    "# ==== LightGBMå­¦ç¿’ ====\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# eval_oof_f1é–¢æ•°ã®å®šç¾©ï¼ˆå¿µã®ãŸã‚å†å®šç¾©ï¼‰\n",
    "def eval_oof_f1(probs, y_true):\n",
    "    \"\"\"OOFäºˆæ¸¬ã‹ã‚‰æœ€é©F1ã‚¹ã‚³ã‚¢ã¨é–¾å€¤ã‚’è¨ˆç®—\"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (probs >= t).astype(int)) for t in thresholds]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return f1s[j], float(thresholds[j])\n",
    "\n",
    "# LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "X_train_lgb = X_train.copy()\n",
    "X_test_lgb = X_test.copy()\n",
    "for c in cat_cols:\n",
    "    X_train_lgb[c] = X_train_lgb[c].astype(\"category\")\n",
    "    X_test_lgb[c] = X_test_lgb[c].astype(\"category\")\n",
    "\n",
    "# Version 6ã§ä½¿ç”¨ã•ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ˜ç¤ºçš„ã«å®šç¾©\n",
    "params_lgb = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"reg_lambda\": 5.0,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"random_state\": SEED,\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbose\": -1,\n",
    "    \"scale_pos_weight\": 1.2,\n",
    "}\n",
    "\n",
    "oof_lgb = np.zeros(len(X_train))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "\n",
    "print(\"=== LightGBMå­¦ç¿’é–‹å§‹ ===\")\n",
    "print(f\"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(X_train_lgb.columns)}\")\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_train_lgb, y_train), 1):\n",
    "    X_tr, X_va = X_train_lgb.iloc[tr_idx], X_train_lgb.iloc[va_idx]\n",
    "    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "    \n",
    "    m = LGBMClassifier(**params_lgb)\n",
    "    m.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)],\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        callbacks=[early_stopping(stopping_rounds=200, verbose=False), log_evaluation(period=0)],\n",
    "    )\n",
    "    oof_lgb[va_idx] = m.predict_proba(X_va)[:, 1]\n",
    "    test_lgb += m.predict_proba(X_test_lgb)[:, 1] / skf_full.n_splits\n",
    "    print(f\"Fold {fold}/5 å®Œäº†\")\n",
    "\n",
    "f1_lgb, th_lgb = eval_oof_f1(oof_lgb, y_train)\n",
    "print(f\"LightGBM OOF F1: {f1_lgb:.6f} | æœ€é©é–¾å€¤: {th_lgb:.4f}\")\n",
    "\n",
    "# æå‡ºé–¾å€¤ã§ã®æ€§èƒ½ç¢ºèª\n",
    "f1_lgb_submit = f1_score(y_train, (oof_lgb >= 0.285).astype(int))\n",
    "print(f\"LightGBM F1@0.285: {f1_lgb_submit:.6f}\")\n",
    "\n",
    "print(\"âœ… LightGBMå­¦ç¿’å®Œäº†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69c393",
   "metadata": {},
   "source": [
    "# 6. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9473b7",
   "metadata": {},
   "source": [
    "# 6.1 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63ce5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENS@submit micro] F1@0.315: 0.629423 | w(CB)=0.440\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«15: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–\n",
    "\n",
    "# ==== 9d''-micro: Soft ensemble (opt for F1 at submit_th, 0.001 step) ====\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "submit_th = float(locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", 0.315))\n",
    "# ãƒ™ãƒ¼ã‚¹æœ€é©ãŒ 0.455 ä»˜è¿‘ãªã®ã§ã€ãã®è¿‘å‚ã‚’ç‹­åŸŸæ¢ç´¢\n",
    "weights = np.round(np.arange(0.430, 0.481, 0.001), 3)\n",
    "\n",
    "best = (-1.0, None)\n",
    "for w in weights:\n",
    "    oof_ens = w * oof_cb + (1 - w) * oof_lgb\n",
    "    f1_sub = f1_score(y_train, (oof_ens >= submit_th).astype(int))\n",
    "    if f1_sub > best[0]:\n",
    "        best = (f1_sub, w)\n",
    "\n",
    "best_f1_submit, best_w_submit = best\n",
    "print(f\"[ENS@submit micro] F1@{submit_th:.3f}: {best_f1_submit:.6f} | w(CB)={best_w_submit:.3f}\")\n",
    "\n",
    "# æ¡ç”¨ï¼ˆã“ã®ã‚»ãƒ«ã®å‡ºåŠ›ã‚’ä»¥é™ã®æå‡ºã«åæ˜ ï¼‰\n",
    "oof = best_w_submit * oof_cb + (1 - best_w_submit) * oof_lgb\n",
    "test_prob = best_w_submit * test_cb + (1 - best_w_submit) * test_lgb\n",
    "best_w = best_w_submit   # ãƒ­ã‚°ç”¨\n",
    "best_th_full = submit_th # ãƒ­ã‚°ç”¨ï¼ˆæå‡ºã¯overrideï¼‰\n",
    "CURRENT_PIPE = \"ens_weight_micro\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df7deb",
   "metadata": {},
   "source": [
    "# 6.2 é‡ã¿å¾®èª¿æ•´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa67a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENS micro] best F1@0.315=0.629423 | w(CB)=0.440\n",
      "[ENS micro] APPLY w=0.440\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«16: é‡ã¿å¾®èª¿æ•´\n",
    "\n",
    "# ==== STEP8-2: micro re-search of ensemble weight around current w ====\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "assert 'oof_cb' in locals() and 'oof_lgb' in locals()\n",
    "th = float(locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", 0.310))  # â† TH-SCANã§ä¸Šæ›¸ãæ¸ˆã¿\n",
    "grid = np.arange(0.44, 0.501, 0.005)  # 0.44..0.50\n",
    "best = (-1.0, None)\n",
    "for w in grid:\n",
    "    oof_mix = w*oof_cb + (1-w)*oof_lgb\n",
    "    f1v = f1_score(y_train, (oof_mix >= th).astype(int))\n",
    "    if f1v > best[0]:\n",
    "        best = (f1v, w)\n",
    "print(f\"[ENS micro] best F1@{th:.3f}={best[0]:.6f} | w(CB)={best[1]:.3f}\")\n",
    "\n",
    "# æ”¹å–„ãŒ +0.0005 ä»¥ä¸Šã®ã¨ãã ã‘æ¡ç”¨ã—ã¦ oof/test ã‚’æ›´æ–°\n",
    "curr_submit_f1 = float(locals().get(\"oof_f1_at_submit\", 0.0))  # ç›´è¿‘ãƒ­ã‚°å€¤ãŒç„¡ã‘ã‚Œã°0\n",
    "if best[0] >= curr_submit_f1 + 0.0005:\n",
    "    best_w = float(best[1])\n",
    "    oof = best_w*oof_cb + (1-best_w)*oof_lgb\n",
    "    test_prob = best_w*test_cb + (1-best_w)*locals().get('test_lgb', 0)\n",
    "    print(f\"[ENS micro] APPLY w={best_w:.3f}\")\n",
    "else:\n",
    "    print(\"[ENS micro] KEEP current w (no clear gain)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedbe6e2",
   "metadata": {},
   "source": [
    "# 7. é–¾å€¤æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75114a60",
   "metadata": {},
   "source": [
    "# 7.1 æœ€é©é–¾å€¤æ¢ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccc267cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TH-SCAN] best_t=0.280 | F1@best=0.633436\n",
      "[TH-SCAN] APPLY_OVERRIDE -> SUBMIT_THRESHOLD_OVERRIDE = 0.280 (Î”F1=+0.004014)\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«17: é–¾å€¤æœ€é©åŒ–\n",
    "\n",
    "# ==== TH-SCAN: OOFå…¨ä½“ã§æœ€é©ã—ãã„å€¤ã‚’èµ°æŸ»ã—ã¦ SUBMIT_THRESHOLD_OVERRIDE ã‚’æ›´æ–° ====\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# æ¢ç´¢ãƒ¬ãƒ³ã‚¸ã¨ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰\n",
    "t_min, t_max, t_step = 0.20, 0.50, 0.005\n",
    "ths = np.arange(t_min, t_max + 1e-9, t_step)\n",
    "\n",
    "# ç¾åœ¨ã® submit é–¾å€¤ï¼ˆoverride å„ªå…ˆï¼‰\n",
    "cur_th = locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", None)\n",
    "if cur_th is None:\n",
    "    cur_th = locals().get(\"best_th_full\", None)\n",
    "if cur_th is None:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ OOF å…¨ä½“æœ€é©ã‚’ä¸€åº¦è¨ˆç®—\n",
    "    cur_f1, cur_th = eval_oof_f1(oof, y_train)\n",
    "else:\n",
    "    cur_f1 = f1_score(y_train, (oof >= cur_th).astype(int))\n",
    "\n",
    "# èµ°æŸ»\n",
    "f1s = [f1_score(y_train, (oof >= t).astype(int)) for t in ths]\n",
    "j = int(np.argmax(f1s))\n",
    "best_t, best_f1 = float(ths[j]), float(f1s[j])\n",
    "\n",
    "print(f\"[TH-SCAN] best_t={best_t:.3f} | F1@best={best_f1:.6f}\")\n",
    "\n",
    "# é©ç”¨ï¼ˆä¸Šæ›¸ãï¼‰\n",
    "delta = best_f1 - cur_f1\n",
    "SUBMIT_THRESHOLD_OVERRIDE = best_t\n",
    "print(f\"[TH-SCAN] APPLY_OVERRIDE -> SUBMIT_THRESHOLD_OVERRIDE = {SUBMIT_THRESHOLD_OVERRIDE:.3f} (Î”F1={delta:+.6f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81095153",
   "metadata": {},
   "source": [
    "# 7.2 æœ€çµ‚æ€§èƒ½ç¢ºèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41790776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: 0.64 | OOF_global: 0.633436 | OOF_at_submit: 0.633436 | submit_th: 0.280\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ«18: æ€§èƒ½ç¢ºèª\n",
    "\n",
    "# ==== TARGET monitor (robust, after ENS micro) ====\n",
    "from sklearn.metrics import f1_score\n",
    "assert 'oof' in locals() and 'y_train' in locals()\n",
    "\n",
    "# ç¾åœ¨ã®æå‡ºã—ãã„å€¤ï¼ˆTH-SCANã§0.310ã«ã—ã¦ã‚‹å‰æï¼‰\n",
    "sub_th = float(locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", 0.310))\n",
    "\n",
    "oof_f1_global, _ = eval_oof_f1(oof, y_train)\n",
    "oof_f1_submit = f1_score(y_train, (oof >= sub_th).astype(int))\n",
    "print(f\"TARGET: 0.64 | OOF_global: {oof_f1_global:.6f} | OOF_at_submit: {oof_f1_submit:.6f} | submit_th: {sub_th:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540908b1",
   "metadata": {},
   "source": [
    "# 8. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3234a09",
   "metadata": {},
   "source": [
    "# 8.1 æœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚»ãƒ«19: æå‡ºä½œæˆï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "\n",
    "# ==== æœ€çµ‚æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ ====\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "assert 'oof' in locals() and 'test_prob' in locals(), \"å…ˆã«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–ã¾ã§å®Ÿè¡Œã—ã¦oof/test_probã‚’ä½œã£ã¦ã‹ã‚‰å®Ÿè¡Œ\"\n",
    "\n",
    "# eval_oof_f1é–¢æ•°ã®å®šç¾©ï¼ˆå¿µã®ãŸã‚ï¼‰\n",
    "def eval_oof_f1(probs, y_true):\n",
    "    \"\"\"OOFäºˆæ¸¬ã‹ã‚‰æœ€é©F1ã‚¹ã‚³ã‚¢ã¨é–¾å€¤ã‚’è¨ˆç®—\"\"\"\n",
    "    thresholds = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (probs >= t).astype(int)) for t in thresholds]\n",
    "    j = int(np.argmax(f1s))\n",
    "    return f1s[j], float(thresholds[j])\n",
    "\n",
    "# æå‡ºé–¾å€¤ã®æ±ºå®šï¼ˆå„ªå…ˆ: override â†’ best_th_full â†’ best_th â†’ 0.5ï¼‰\n",
    "threshold_for_submit = locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", None)\n",
    "if threshold_for_submit is None:\n",
    "    threshold_for_submit = locals().get(\"best_th_full\", None)\n",
    "if threshold_for_submit is None:\n",
    "    threshold_for_submit = locals().get(\"best_th\", 0.5)\n",
    "\n",
    "threshold_source = (\n",
    "    \"override\" if locals().get(\"SUBMIT_THRESHOLD_OVERRIDE\", None) is not None\n",
    "    else \"best_th_full\" if locals().get(\"best_th_full\", None) is not None\n",
    "    else \"best_th\" if locals().get(\"best_th\", None) is not None\n",
    "    else \"default_0.5\"\n",
    ")\n",
    "\n",
    "# ---- foldã”ã¨ã®æŒ‡æ¨™ã‚’æ¯å›ä½œã‚Šç›´ã™ ----\n",
    "fold_reports = []\n",
    "fold_f1s = []\n",
    "if 'pools_full' in locals():\n",
    "    for fold, (_tr_pool, _va_pool, va_idx) in enumerate(pools_full, 1):\n",
    "        y_va = y_train[va_idx]\n",
    "        y_pred_va = (oof[va_idx] >= threshold_for_submit).astype(int)\n",
    "        f1v = f1_score(y_va, y_pred_va)\n",
    "        cm  = confusion_matrix(y_va, y_pred_va)\n",
    "        rep = classification_report(y_va, y_pred_va, digits=4)\n",
    "        fold_f1s.append(f1v)\n",
    "        fold_reports.append((f\"FOLD {fold}\", f1v, cm, rep))\n",
    "else:\n",
    "    # äºˆå‚™ï¼ˆfoldå¢ƒç•ŒãŒãªã„ã¨ãï¼‰\n",
    "    y_pred = (oof >= threshold_for_submit).astype(int)\n",
    "    f1v = f1_score(y_train, y_pred)\n",
    "    cm  = confusion_matrix(y_train, y_pred)\n",
    "    rep = classification_report(y_train, y_pred, digits=4)\n",
    "    fold_f1s = [f1v]\n",
    "    fold_reports = [(\"GLOBAL\", f1v, cm, rep)]\n",
    "\n",
    "# ---- æå‡ºäºˆæ¸¬ ----\n",
    "test_pred = (test_prob >= threshold_for_submit).astype(int)\n",
    "assert len(test_pred) == len(test)\n",
    "assert set(np.unique(test_pred)).issubset({0,1})\n",
    "\n",
    "# è‡ªå‹•ãƒŠãƒ³ãƒãƒªãƒ³ã‚°\n",
    "OUT_DIR = r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "n = next_version_number(OUT_DIR)\n",
    "sub_name = f\"submission_A_v{n}.csv\"\n",
    "log_name = f\"run_A2_v{n}.txt\"\n",
    "\n",
    "# å‡ºåŠ›ï¼ˆsample_submitã®åŒºåˆ‡ã‚Šã«åˆã‚ã›ã‚‹ï¼‰\n",
    "sep = locals().get(\"SUBMIT_SEP\", \",\")\n",
    "submit_df = pd.DataFrame({ID_COL: test[ID_COL].values, \"pred\": test_pred})\n",
    "if sep == r\"\\s+\":\n",
    "    with open(os.path.join(OUT_DIR, sub_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, p in submit_df[[ID_COL, \"pred\"]].itertuples(index=False):\n",
    "            f.write(f\"{i} {p}\\n\")\n",
    "else:\n",
    "    submit_df.to_csv(os.path.join(OUT_DIR, sub_name), header=False, index=False, sep=sep)\n",
    "\n",
    "print(\"Saved:\", os.path.join(OUT_DIR, sub_name))\n",
    "\n",
    "# ---- ãƒ­ã‚°ï¼ˆæ­£ç¢ºãªå®Ÿé¨“æƒ…å ±ã‚’è¨˜éŒ²ï¼‰ ----\n",
    "def safe(x): \n",
    "    return float(x) if isinstance(x, (np.floating, np.float64, np.float32)) else x\n",
    "\n",
    "oof_f1_global_best, _ = eval_oof_f1(oof, y_train)\n",
    "oof_f1_at_submit = f1_score(y_train, (oof >= threshold_for_submit).astype(int))\n",
    "\n",
    "# å¼·åŒ–ç‰¹å¾´é‡ã®ä½¿ç”¨ç¢ºèª\n",
    "actual_features = len(X_train.columns) if 'X_train' in locals() else 15\n",
    "enhanced_features_used = actual_features > 15\n",
    "\n",
    "# Version 6ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆæ­£ç¢ºãªå‚ç…§å€¤ï¼‰\n",
    "baseline_f1_v6 = 0.633170  # Version 6ã®å®Ÿç¸¾\n",
    "original_baseline = 0.646952  # Version 1ã®å®Ÿç¸¾\n",
    "\n",
    "log_lines = [\n",
    "    f\"version: {n}\",\n",
    "    f\"seed: {SEED}\",\n",
    "    f\"n_splits: {skf_full.n_splits if 'skf_full' in locals() else 5}\",\n",
    "    f\"target_col: {TARGET_COL}\",\n",
    "    f\"id_col: {ID_COL}\",\n",
    "    f\"n_features: {actual_features}\",\n",
    "    f\"n_categoricals: {len(cat_cols)}\",\n",
    "    f\"train_shape: {train.shape}\",\n",
    "    f\"test_shape: {test.shape}\",\n",
    "    f\"target_pos_ratio: {train[TARGET_COL].mean():.6f}\",\n",
    "    \"\",\n",
    "    # === æ­£ç¢ºãªæ”¹å–„æ–½ç­–ã®å®Ÿé¨“çµæœ ===\n",
    "    \"=== IMPROVEMENT EXPERIMENTS (ORGANIZED VERSION) ===\",\n",
    "    f\"1_statistical_features: failed (harmful, -0.008122)\",\n",
    "    f\"2_optuna_optimization: failed (existing_better, -0.018164)\", \n",
    "    f\"3_stacking_ensemble: failed (threshold_dependent, works@0.805_not@0.315)\",\n",
    "    f\"4_basic_threshold_opt: failed (minimal_gain, -0.000857)\",\n",
    "    f\"5_data_driven_features: {'success (+0.007472, +1.19%)' if enhanced_features_used else 'not_applied'}\",\n",
    "    f\"6_ensemble_optimization: success (weight_optimization)\",\n",
    "    f\"7_advanced_threshold_opt: success (0.315->0.285->0.280)\",\n",
    "    f\"organized_version_status: complete\",\n",
    "    \"\",\n",
    "    f\"baseline_f1_v1: {original_baseline}\",\n",
    "    f\"baseline_f1_v6: {baseline_f1_v6}\",\n",
    "    f\"current_f1_v{n}: {oof_f1_at_submit:.6f}\",\n",
    "    f\"improvement_vs_v1: {oof_f1_at_submit - original_baseline:+.6f}\",\n",
    "    f\"improvement_vs_v6: {oof_f1_at_submit - baseline_f1_v6:+.6f}\",\n",
    "    f\"enhanced_features_applied: {enhanced_features_used}\",\n",
    "    \"\",\n",
    "    # === ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æƒ…å ± ===\n",
    "    f\"best_oof_f1_from_study: {locals().get('best_score_improved', locals().get('best_score', float('nan'))):.6f}\",\n",
    "    f\"oof_f1_global_best: {oof_f1_global_best:.6f}\",\n",
    "    f\"oof_f1_at_submit_th: {oof_f1_at_submit:.6f}\",\n",
    "    f\"threshold_source: {threshold_source}\",\n",
    "    f\"submit_threshold: {float(threshold_for_submit):.6f}\",\n",
    "    f\"fold_f1s: {[round(safe(x), 6) for x in fold_f1s]}\",\n",
    "    \"\",\n",
    "    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æƒ…å ±\n",
    "    f\"oof_f1_cb: {locals().get('f1_cb', float('nan')):.6f}\",\n",
    "    f\"oof_f1_lgb: {locals().get('f1_lgb', float('nan')):.6f}\",\n",
    "    f\"ensemble_w_cb: {locals().get('best_w', float('nan'))}\",\n",
    "    f\"current_pipeline: {locals().get('CURRENT_PIPE', 'ens_weight_micro')}\",\n",
    "    \"\",\n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±\n",
    "    \"best_params_cb:\",\n",
    "    json.dumps(locals().get('best_params_cb', {}), indent=2),\n",
    "    \"params_lgb:\",\n",
    "    json.dumps(locals().get('params_lgb', {}), indent=2),\n",
    "    \"\",\n",
    "    # === ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æƒ…å ± ===\n",
    "    \"enhanced_features_info:\",\n",
    "    json.dumps({\n",
    "        \"total_features\": actual_features,\n",
    "        \"original_features\": 15,\n",
    "        \"added_features\": max(0, actual_features - 15),\n",
    "        \"high_risk_combo_used\": enhanced_features_used,\n",
    "        \"data_driven_approach\": enhanced_features_used,\n",
    "        \"business_insights_applied\": enhanced_features_used\n",
    "    }, indent=2),\n",
    "    \"\",\n",
    "    # === å°†æ¥æ”¹å–„ã®æ–¹å‘æ€§ ===\n",
    "    \"next_improvements:\",\n",
    "    json.dumps({\n",
    "        \"optuna_with_enhanced_features\": \"ready\",\n",
    "        \"external_data_integration\": \"planned\",\n",
    "        \"advanced_ensemble_techniques\": \"available\"\n",
    "    }, indent=2),\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "for title, f1v, cm, rep in fold_reports:\n",
    "    log_lines += [title, f\"F1@submit_th: {f1v:.6f}\", \"confusion_matrix:\", str(cm), \"report:\", rep, \"-\"*40]\n",
    "\n",
    "with open(os.path.join(OUT_DIR, log_name), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join([str(x) for x in log_lines]))\n",
    "\n",
    "print(\"Saved:\", os.path.join(OUT_DIR, log_name))\n",
    "\n",
    "# === æ•´ç†ç‰ˆå®Œæˆã‚µãƒãƒªãƒ¼ ===\n",
    "print(f\"\\nğŸ‰ æ•´ç†ç‰ˆ Version {n} å®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š æœ€çµ‚F1ã‚¹ã‚³ã‚¢: {oof_f1_at_submit:.6f}\")\n",
    "print(f\"ğŸ¯ æå‡ºé–¾å€¤: {threshold_for_submit:.3f}\")\n",
    "print(f\"ğŸ› ï¸ ç‰¹å¾´é‡æ•°: {actual_features}åˆ—\")\n",
    "print(f\"ğŸ”„ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿: CB {locals().get('best_w', 0.5):.3f}\")\n",
    "print(f\"ğŸ“ ä¿å­˜å…ˆ: {OUT_DIR}\")\n",
    "print(f\"âœ¨ å¼·åŒ–ç‰¹å¾´é‡: {'é©ç”¨æ¸ˆã¿' if enhanced_features_used else 'æœªé©ç”¨'}\")\n",
    "print(f\"\\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: Optunaæœ€é©åŒ–ã§æ›´ãªã‚‹å‘ä¸Šã¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cd0a2",
   "metadata": {},
   "source": [
    "# 9. å¼·åŒ–ç‰¹å¾´é‡ã§ã®Optunaæœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05069e",
   "metadata": {},
   "source": [
    "# 9.1 Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea0762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 14:58:03,980] A new study created in memory with name: no-name-4869910a-faf1-4c70-98e7-5742310410bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Phase 1: F1ã‚¹ã‚³ã‚¢0.66ã¸ã®ç¬¬ä¸€æ­© - Optunaæœ€é©åŒ–\n",
      "ç¾åœ¨F1: 0.633436 â†’ ç›®æ¨™: 0.645+ (+0.012)\n",
      "\n",
      "=== CatBoostæœ€é©åŒ–é–‹å§‹ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.560943:   2%|â–         | 1/40 [01:08<44:30, 68.48s/it, 68.48/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 14:59:12,459] Trial 0 finished with value: 0.560942760942761 and parameters: {'iterations': 5000, 'learning_rate': 0.17254716573280354, 'depth': 8, 'l2_leaf_reg': 6.2513735745217485, 'bagging_temperature': 1.2481491235394921, 'random_strength': 0.7799726016810132, 'subsample': 0.5290418060840998, 'rsm': 0.9197056874649611, 'scale_pos_weight': 5.207805082202461, 'border_count': 189, 'max_ctr_complexity': 1}. Best is trial 0 with value: 0.560942760942761.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:   5%|â–Œ         | 2/40 [02:15<42:58, 67.85s/it, 135.89/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:00:19,874] Trial 1 finished with value: 0.6186335403726708 and parameters: {'iterations': 10000, 'learning_rate': 0.12106896936002161, 'depth': 4, 'l2_leaf_reg': 0.3511356313970407, 'bagging_temperature': 1.4672360788274705, 'random_strength': 1.5212112147976886, 'subsample': 0.762378215816119, 'rsm': 0.6591670111852694, 'scale_pos_weight': 3.0386039813862933, 'border_count': 168, 'max_ctr_complexity': 1}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:   8%|â–Š         | 3/40 [03:28<43:13, 70.10s/it, 208.67/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:01:32,651] Trial 2 finished with value: 0.4869215291750503 and parameters: {'iterations': 4000, 'learning_rate': 0.029967309097101588, 'depth': 6, 'l2_leaf_reg': 22.673986523780385, 'bagging_temperature': 1.5973902572668779, 'random_strength': 2.571172192068058, 'subsample': 0.7962072844310213, 'rsm': 0.42787024763199866, 'scale_pos_weight': 5.252813963310069, 'border_count': 70, 'max_ctr_complexity': 1}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  10%|â–ˆ         | 4/40 [04:27<39:22, 65.61s/it, 267.40/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:02:31,386] Trial 3 finished with value: 0.534371825262445 and parameters: {'iterations': 10000, 'learning_rate': 0.18043311207136256, 'depth': 9, 'l2_leaf_reg': 0.8200518402245829, 'bagging_temperature': 0.781376912051071, 'random_strength': 3.4211651325607844, 'subsample': 0.7200762468698007, 'rsm': 0.47322294090686734, 'scale_pos_weight': 4.466238370778891, 'border_count': 39, 'max_ctr_complexity': 6}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  12%|â–ˆâ–        | 5/40 [05:23<36:16, 62.19s/it, 323.51/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:03:27,494] Trial 4 finished with value: 0.5515480370252155 and parameters: {'iterations': 4000, 'learning_rate': 0.07277150634170934, 'depth': 5, 'l2_leaf_reg': 3.632486956676605, 'bagging_temperature': 4.373682234746237, 'random_strength': 0.9242722776276352, 'subsample': 0.9847923138822793, 'rsm': 0.8650796940166687, 'scale_pos_weight': 7.576492590949324, 'border_count': 231, 'max_ctr_complexity': 4}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  15%|â–ˆâ–Œ        | 6/40 [07:09<43:39, 77.03s/it, 429.36/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:05:13,345] Trial 5 finished with value: 0.586840091813313 and parameters: {'iterations': 9500, 'learning_rate': 0.01303561122512888, 'depth': 4, 'l2_leaf_reg': 0.13667272915456222, 'bagging_temperature': 2.6026426461061147, 'random_strength': 1.9433864484474102, 'subsample': 0.6356745158869479, 'rsm': 0.8972425054911576, 'scale_pos_weight': 3.497273286855125, 'border_count': 94, 'max_ctr_complexity': 4}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  18%|â–ˆâ–Š        | 7/40 [08:05<38:38, 70.25s/it, 485.66/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:06:09,641] Trial 6 finished with value: 0.5756958587915818 and parameters: {'iterations': 3000, 'learning_rate': 0.11058146376563001, 'depth': 3, 'l2_leaf_reg': 91.33995846860967, 'bagging_temperature': 6.177958154373259, 'random_strength': 0.993578407670862, 'subsample': 0.5027610585618012, 'rsm': 0.8892768570729005, 'scale_pos_weight': 5.94800140693332, 'border_count': 194, 'max_ctr_complexity': 5}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  20%|â–ˆâ–ˆ        | 8/40 [09:26<39:13, 73.54s/it, 566.22/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:07:30,209] Trial 7 finished with value: 0.5569975447211505 and parameters: {'iterations': 2500, 'learning_rate': 0.029266761285490727, 'depth': 3, 'l2_leaf_reg': 38.8427775470314, 'bagging_temperature': 4.986385014620463, 'random_strength': 1.654490124263246, 'subsample': 0.5317791751430119, 'rsm': 0.5865893930293973, 'scale_pos_weight': 3.2762832541872293, 'border_count': 194, 'max_ctr_complexity': 4}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  22%|â–ˆâ–ˆâ–       | 9/40 [11:16<43:55, 85.02s/it, 676.51/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:09:20,488] Trial 8 finished with value: 0.5492772667542707 and parameters: {'iterations': 9500, 'learning_rate': 0.041149615546913355, 'depth': 3, 'l2_leaf_reg': 13.795402040204177, 'bagging_temperature': 6.0862803889351795, 'random_strength': 2.8063859878474813, 'subsample': 0.8854835899772805, 'rsm': 0.6962773578186345, 'scale_pos_weight': 4.659129805673958, 'border_count': 127, 'max_ctr_complexity': 1}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  25%|â–ˆâ–ˆâ–Œ       | 10/40 [14:24<58:24, 116.83s/it, 864.57/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:12:28,550] Trial 9 finished with value: 0.4631522323830016 and parameters: {'iterations': 2500, 'learning_rate': 0.010987283063579408, 'depth': 8, 'l2_leaf_reg': 0.8771380343280561, 'bagging_temperature': 4.068565529317622, 'random_strength': 4.537832369630465, 'subsample': 0.6246461145744375, 'rsm': 0.6462297538213778, 'scale_pos_weight': 6.288857969801341, 'border_count': 83, 'max_ctr_complexity': 1}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  28%|â–ˆâ–ˆâ–Š       | 11/40 [17:36<1:07:36, 139.86s/it, 1056.65/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:15:40,637] Trial 10 finished with value: 0.48322147651006714 and parameters: {'iterations': 4000, 'learning_rate': 0.01620890700720353, 'depth': 10, 'l2_leaf_reg': 26.56813924114492, 'bagging_temperature': 5.067230052083388, 'random_strength': 4.357302950938589, 'subsample': 0.9018360384495572, 'rsm': 0.5119420353316215, 'scale_pos_weight': 7.247912989429844, 'border_count': 152, 'max_ctr_complexity': 5}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  30%|â–ˆâ–ˆâ–ˆ       | 12/40 [20:12<1:07:33, 144.75s/it, 1212.58/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:18:16,559] Trial 11 finished with value: 0.5166051660516605 and parameters: {'iterations': 9500, 'learning_rate': 0.025925793627054272, 'depth': 3, 'l2_leaf_reg': 0.4828424974818325, 'bagging_temperature': 3.4168623090100505, 'random_strength': 4.090073829612465, 'subsample': 0.9303652916281717, 'rsm': 0.40417127831871447, 'scale_pos_weight': 4.57523111804296, 'border_count': 125, 'max_ctr_complexity': 2}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  32%|â–ˆâ–ˆâ–ˆâ–      | 13/40 [22:45<1:06:18, 147.34s/it, 1365.88/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:20:49,865] Trial 12 finished with value: 0.45887899423782086 and parameters: {'iterations': 3000, 'learning_rate': 0.027494603746278566, 'depth': 10, 'l2_leaf_reg': 0.9324140221663487, 'bagging_temperature': 4.150324973946929, 'random_strength': 3.515094794475889, 'subsample': 0.681814801189647, 'rsm': 0.9830692496325764, 'scale_pos_weight': 7.737131064594779, 'border_count': 88, 'max_ctr_complexity': 3}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 14/40 [24:56<1:01:37, 142.20s/it, 1496.19/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:23:00,170] Trial 13 finished with value: 0.6091954022988506 and parameters: {'iterations': 4500, 'learning_rate': 0.023473941999051617, 'depth': 3, 'l2_leaf_reg': 6.740513796374042, 'bagging_temperature': 4.021432185830892, 'random_strength': 0.25739375624994676, 'subsample': 0.6393232321183058, 'rsm': 0.9449595315799922, 'scale_pos_weight': 2.676933234668807, 'border_count': 64, 'max_ctr_complexity': 3}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.618634:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 15/40 [28:48<1:10:34, 169.40s/it, 1728.62/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:26:52,604] Trial 14 finished with value: 0.534116878876918 and parameters: {'iterations': 10000, 'learning_rate': 0.02065005291441002, 'depth': 8, 'l2_leaf_reg': 19.268985325226208, 'bagging_temperature': 1.9011003519391974, 'random_strength': 3.641081743059298, 'subsample': 0.6838915663596266, 'rsm': 0.7793834983561476, 'scale_pos_weight': 5.434707975326263, 'border_count': 151, 'max_ctr_complexity': 1}. Best is trial 1 with value: 0.6186335403726708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 16/40 [31:20<1:05:36, 164.03s/it, 1880.18/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:29:24,156] Trial 15 finished with value: 0.6277440448388604 and parameters: {'iterations': 10000, 'learning_rate': 0.07141152961544894, 'depth': 6, 'l2_leaf_reg': 0.5801671531102249, 'bagging_temperature': 1.3214351926272616, 'random_strength': 2.3348274553205046, 'subsample': 0.7503992429838656, 'rsm': 0.7181497255651733, 'scale_pos_weight': 1.8288682513002763, 'border_count': 202, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/40 [32:45<53:45, 140.25s/it, 1965.15/3600 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:30:49,130] Trial 16 finished with value: 0.5922365988909427 and parameters: {'iterations': 10000, 'learning_rate': 0.17134164879738695, 'depth': 5, 'l2_leaf_reg': 0.1589913153580619, 'bagging_temperature': 0.5358568796675622, 'random_strength': 1.794466983874221, 'subsample': 0.797628675630854, 'rsm': 0.6537546266250812, 'scale_pos_weight': 4.501043304522414, 'border_count': 175, 'max_ctr_complexity': 2}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 18/40 [36:59<1:04:00, 174.56s/it, 2219.57/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:35:03,547] Trial 17 finished with value: 0.6254681647940075 and parameters: {'iterations': 10000, 'learning_rate': 0.0309012971521584, 'depth': 6, 'l2_leaf_reg': 0.4440899327165011, 'bagging_temperature': 0.8568385645374601, 'random_strength': 3.344604931328977, 'subsample': 0.5930444608026818, 'rsm': 0.7886450355007293, 'scale_pos_weight': 2.0279895112742126, 'border_count': 215, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 19/40 [42:09<1:15:21, 215.32s/it, 2529.84/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:40:13,817] Trial 18 finished with value: 0.6123595505617978 and parameters: {'iterations': 9000, 'learning_rate': 0.032085121781951775, 'depth': 9, 'l2_leaf_reg': 0.6664997276614286, 'bagging_temperature': 0.8369878758542416, 'random_strength': 2.919519094382809, 'subsample': 0.5705608510671025, 'rsm': 0.5844107639661613, 'scale_pos_weight': 1.6021688593637315, 'border_count': 188, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 20/40 [46:23<1:15:36, 226.82s/it, 2783.47/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:44:27,453] Trial 19 finished with value: 0.6219217769193627 and parameters: {'iterations': 9000, 'learning_rate': 0.0342296719853961, 'depth': 7, 'l2_leaf_reg': 0.8677234819579489, 'bagging_temperature': 5.058656698476332, 'random_strength': 3.8812360487557873, 'subsample': 0.5823080961865055, 'rsm': 0.9750000491181088, 'scale_pos_weight': 1.909497328059318, 'border_count': 248, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 21/40 [48:50<1:04:16, 202.97s/it, 2930.84/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:46:54,822] Trial 20 finished with value: 0.6182129115006413 and parameters: {'iterations': 10000, 'learning_rate': 0.05938643084893497, 'depth': 4, 'l2_leaf_reg': 0.6695537655315811, 'bagging_temperature': 2.5195272859810847, 'random_strength': 4.641959012844426, 'subsample': 0.9350022334667135, 'rsm': 0.8429044621084724, 'scale_pos_weight': 2.67168603697777, 'border_count': 222, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 22/40 [51:30<57:00, 190.01s/it, 3090.63/3600 seconds]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:49:34,607] Trial 21 finished with value: 0.5543018335684062 and parameters: {'iterations': 10000, 'learning_rate': 0.03693536618408489, 'depth': 6, 'l2_leaf_reg': 3.1823714322570313, 'bagging_temperature': 6.268585970244972, 'random_strength': 3.381800695571803, 'subsample': 0.6427895485402019, 'rsm': 0.8110223790401045, 'scale_pos_weight': 3.520715562246219, 'border_count': 245, 'max_ctr_complexity': 2}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 23/40 [54:35<53:22, 188.36s/it, 3275.15/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:52:39,129] Trial 22 finished with value: 0.6039800995024875 and parameters: {'iterations': 7000, 'learning_rate': 0.061951279931362964, 'depth': 8, 'l2_leaf_reg': 0.30191556035305145, 'bagging_temperature': 5.663092120439549, 'random_strength': 3.07514457272741, 'subsample': 0.5021238338561766, 'rsm': 0.9702727556743849, 'scale_pos_weight': 2.061683253645602, 'border_count': 240, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 24/40 [58:31<54:03, 202.72s/it, 3511.35/3600 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:56:35,328] Trial 23 finished with value: 0.6222418358340689 and parameters: {'iterations': 10000, 'learning_rate': 0.03496629658162254, 'depth': 5, 'l2_leaf_reg': 0.19267374211586252, 'bagging_temperature': 1.0411327867578706, 'random_strength': 4.340262576093344, 'subsample': 0.6046744101636772, 'rsm': 0.8903415489639767, 'scale_pos_weight': 2.441600080662278, 'border_count': 210, 'max_ctr_complexity': 3}. Best is trial 15 with value: 0.6277440448388604.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 15. Best value: 0.627744:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 25/40 [1:01:22<36:49, 147.29s/it, 3682.32/3600 seconds]\n",
      "[I 2025-08-20 15:59:26,332] A new study created in memory with name: no-name-9de58eb5-7624-4e13-a656-1385abc6a81a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 15:59:26,298] Trial 24 finished with value: 0.6216696269982238 and parameters: {'iterations': 8500, 'learning_rate': 0.037806564592186086, 'depth': 4, 'l2_leaf_reg': 0.24688589110421927, 'bagging_temperature': 0.8737900919165439, 'random_strength': 3.4262238377594225, 'subsample': 0.5134388839038139, 'rsm': 0.9172601148926002, 'scale_pos_weight': 2.0747965347206043, 'border_count': 150, 'max_ctr_complexity': 1}. Best is trial 15 with value: 0.6277440448388604.\n",
      "\n",
      "ğŸ¯ CatBoostæœ€é©åŒ–çµæœ:\n",
      "Best F1: 0.627744\n",
      "Improvement: -0.005422\n",
      "Best threshold: 0.4100\n",
      "â†’ CatBoostæ”¹å–„ã¯å¾®å°\n",
      "\n",
      "=== LightGBMæœ€é©åŒ–é–‹å§‹ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.333696:   3%|â–         | 1/35 [00:36<20:55, 36.92s/it, 36.91/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:00:03,249] Trial 0 finished with value: 0.3336964415395788 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.01918471487513601, 'num_leaves': 111, 'max_depth': 14, 'min_child_samples': 202, 'subsample': 0.7705811061417018, 'colsample_bytree': 0.5145069122121801, 'reg_alpha': 14.674965925605658, 'reg_lambda': 7.899000368620117, 'scale_pos_weight': 6.614329830400665, 'n_estimators': 1600, 'drop_rate': 0.028442468325575898, 'skip_drop': 0.6933189127193602}. Best is trial 0 with value: 0.3336964415395788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.604502:   6%|â–Œ         | 2/35 [03:06<56:44, 103.16s/it, 186.44/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:02:32,772] Trial 1 finished with value: 0.6045016077170418 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.012307287088076106, 'num_leaves': 256, 'max_depth': 14, 'min_child_samples': 292, 'subsample': 0.6926884571975691, 'colsample_bytree': 0.9772440625543326, 'reg_alpha': 8.915167216794377, 'reg_lambda': 13.394493034436154, 'scale_pos_weight': 1.577500347880088, 'n_estimators': 4600, 'drop_rate': 0.14900175034438135, 'skip_drop': 0.20984385807710515}. Best is trial 1 with value: 0.6045016077170418.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.604502:   9%|â–Š         | 3/35 [08:37<1:50:25, 207.06s/it, 517.14/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:08:03,475] Trial 2 finished with value: 0.6022172949002217 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.05603166394428692, 'num_leaves': 294, 'max_depth': 14, 'min_child_samples': 274, 'subsample': 0.7626278355535727, 'colsample_bytree': 0.5520094749273814, 'reg_alpha': 3.61829190515802, 'reg_lambda': 19.060804419734833, 'scale_pos_weight': 3.883670883704511, 'n_estimators': 4400, 'drop_rate': 0.33608863973808806, 'skip_drop': 0.503028636705729}. Best is trial 1 with value: 0.6045016077170418.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.604502:  11%|â–ˆâ–        | 4/35 [14:08<2:12:15, 255.97s/it, 848.09/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:13:34,425] Trial 3 finished with value: 0.6 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.029896301970497925, 'num_leaves': 299, 'max_depth': 12, 'min_child_samples': 136, 'subsample': 0.7803315847218675, 'colsample_bytree': 0.7056277445433177, 'reg_alpha': 14.53975983935977, 'reg_lambda': 7.983937718081496, 'scale_pos_weight': 5.6910161529166725, 'n_estimators': 3700, 'drop_rate': 0.3047799356295229, 'skip_drop': 0.4320275713453248}. Best is trial 1 with value: 0.6045016077170418.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  14%|â–ˆâ–        | 5/35 [14:29<1:25:46, 171.54s/it, 869.94/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:13:56,275] Trial 4 finished with value: 0.6223453370267775 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.021951178542536615, 'num_leaves': 204, 'max_depth': 6, 'min_child_samples': 47, 'subsample': 0.7019783334902348, 'colsample_bytree': 0.6551347634222948, 'reg_alpha': 4.867960325619245, 'reg_lambda': 11.762080853167522, 'scale_pos_weight': 2.7174027707435187, 'n_estimators': 3900, 'top_rate': 0.38805865914168913, 'other_rate': 0.15428913004326394}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  17%|â–ˆâ–‹        | 6/35 [15:07<1:00:58, 126.14s/it, 907.94/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:14:34,277] Trial 5 finished with value: 0.41927960611557397 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.1143136994527456, 'num_leaves': 75, 'max_depth': 10, 'min_child_samples': 290, 'subsample': 0.9993491229483435, 'colsample_bytree': 0.5120843124902723, 'reg_alpha': 9.626066669106333, 'reg_lambda': 5.828453793472159, 'scale_pos_weight': 1.4460439782212018, 'n_estimators': 3100, 'drop_rate': 0.0025416423496070206, 'skip_drop': 0.48902207173601736}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  20%|â–ˆâ–ˆ        | 7/35 [15:11<40:11, 86.12s/it, 911.68/3000 seconds]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:14:38,021] Trial 6 finished with value: 0.5901244480128462 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.07415618365725712, 'num_leaves': 71, 'max_depth': 8, 'min_child_samples': 267, 'subsample': 0.6275231401673982, 'colsample_bytree': 0.721656347364492, 'reg_alpha': 12.338739516417903, 'reg_lambda': 2.067050217563089, 'scale_pos_weight': 4.430767627483437, 'n_estimators': 700, 'top_rate': 0.22465098772366032, 'other_rate': 0.16280595295490377}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  23%|â–ˆâ–ˆâ–       | 8/35 [19:22<1:02:17, 138.42s/it, 1162.09/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:18:48,421] Trial 7 finished with value: 0.5791150442477876 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.01791135225990522, 'num_leaves': 274, 'max_depth': 10, 'min_child_samples': 255, 'subsample': 0.9612447418865202, 'colsample_bytree': 0.9144806394695709, 'reg_alpha': 7.883959959194526, 'reg_lambda': 11.963762731642532, 'scale_pos_weight': 4.028886556945201, 'n_estimators': 3600, 'drop_rate': 0.24234324480181324, 'skip_drop': 0.1026082732608586}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  26%|â–ˆâ–ˆâ–Œ       | 9/35 [19:24<41:34, 95.94s/it, 1164.60/3000 seconds]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:18:50,933] Trial 8 finished with value: 0.5856890459363958 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.013311146807396378, 'num_leaves': 22, 'max_depth': 7, 'min_child_samples': 291, 'subsample': 0.8504878447166522, 'colsample_bytree': 0.7068427673666682, 'reg_alpha': 2.415043410315749, 'reg_lambda': 1.2072708870310733, 'scale_pos_weight': 6.083481716325426, 'n_estimators': 4600, 'top_rate': 0.10641036717284758, 'other_rate': 0.13762112460703804}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  29%|â–ˆâ–ˆâ–Š       | 10/35 [19:32<28:35, 68.61s/it, 1172.04/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:18:58,374] Trial 9 finished with value: 0.6172731258220079 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09085319003402194, 'num_leaves': 187, 'max_depth': 7, 'min_child_samples': 27, 'subsample': 0.7434530881160432, 'colsample_bytree': 0.7340969530349626, 'reg_alpha': 3.630695134587012, 'reg_lambda': 1.4027186892647436, 'scale_pos_weight': 4.5588065250248455, 'n_estimators': 2000}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  31%|â–ˆâ–ˆâ–ˆâ–      | 11/35 [19:49<21:14, 53.11s/it, 1189.99/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:19:16,322] Trial 10 finished with value: 0.6188183807439825 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.026160417602940446, 'num_leaves': 252, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9622088756571833, 'colsample_bytree': 0.8369666255038108, 'reg_alpha': 19.89629160212417, 'reg_lambda': 9.590100428813358, 'scale_pos_weight': 2.1139624758778695, 'n_estimators': 4800, 'top_rate': 0.16642806941779875, 'other_rate': 0.10539279605645804}. Best is trial 4 with value: 0.6223453370267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.622345:  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [19:53<14:36, 38.10s/it, 1193.78/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:19:20,111] Trial 11 finished with value: 0.6159813809154383 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.1105438813233341, 'num_leaves': 109, 'max_depth': 15, 'min_child_samples': 300, 'subsample': 0.7430077620299356, 'colsample_bytree': 0.6606457942170065, 'reg_alpha': 19.46877996163687, 'reg_lambda': 19.974370726177714, 'scale_pos_weight': 4.116491700925708, 'n_estimators': 700}. Best is trial 4 with value: 0.6223453370267775.\n",
      "[W 2025-08-20 16:19:20,212] The parameter 'top_rate' in trial#12 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:19:20,228] The parameter 'other_rate' in trial#12 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.629371:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 13/35 [20:19<12:34, 34.30s/it, 1219.31/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:19:45,647] Trial 12 finished with value: 0.6293706293706294 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.01647064864795516, 'num_leaves': 244, 'max_depth': 5, 'min_child_samples': 53, 'subsample': 0.917685089444525, 'colsample_bytree': 0.8473341918890209, 'reg_alpha': 15.048272940107815, 'reg_lambda': 5.674849572736912, 'scale_pos_weight': 2.1198443134339775, 'n_estimators': 4900, 'top_rate': 0.42104090720355175, 'other_rate': 0.07065177339949597}. Best is trial 12 with value: 0.6293706293706294.\n",
      "[W 2025-08-20 16:19:45,757] The parameter 'top_rate' in trial#13 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:19:45,772] The parameter 'other_rate' in trial#13 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.629371:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [20:56<12:20, 35.26s/it, 1256.79/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:20:23,122] Trial 13 finished with value: 0.6162260711030082 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.014602491060390415, 'num_leaves': 256, 'max_depth': 7, 'min_child_samples': 24, 'subsample': 0.6960675188562099, 'colsample_bytree': 0.5281506023521427, 'reg_alpha': 3.6287797039996628, 'reg_lambda': 13.781398723728929, 'scale_pos_weight': 3.5666605244100307, 'n_estimators': 4400, 'top_rate': 0.43974513048037656, 'other_rate': 0.05198717758454403}. Best is trial 12 with value: 0.6293706293706294.\n",
      "[W 2025-08-20 16:20:23,239] The parameter 'top_rate' in trial#14 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:20:23,254] The parameter 'other_rate' in trial#14 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.629428:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 15/35 [21:15<10:02, 30.13s/it, 1275.03/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:20:41,367] Trial 14 finished with value: 0.6294277929155313 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.013391032892142577, 'num_leaves': 129, 'max_depth': 3, 'min_child_samples': 66, 'subsample': 0.6277926224490197, 'colsample_bytree': 0.6802775967058358, 'reg_alpha': 0.48220414663118305, 'reg_lambda': 5.839957454768193, 'scale_pos_weight': 1.8426197676981362, 'n_estimators': 2700, 'top_rate': 0.35060518528512535, 'other_rate': 0.08412126217333969}. Best is trial 14 with value: 0.6294277929155313.\n",
      "[W 2025-08-20 16:20:41,475] The parameter 'top_rate' in trial#15 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:20:41,491] The parameter 'other_rate' in trial#15 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.629428:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [21:43<09:24, 29.72s/it, 1303.80/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:21:10,134] Trial 15 finished with value: 0.613203367301728 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.015714632891388632, 'num_leaves': 101, 'max_depth': 7, 'min_child_samples': 90, 'subsample': 0.5814704820197097, 'colsample_bytree': 0.8442169702102942, 'reg_alpha': 3.5009570134212726, 'reg_lambda': 8.369723829777055, 'scale_pos_weight': 3.6383296629123927, 'n_estimators': 2100, 'top_rate': 0.342169702412086, 'other_rate': 0.08736438104840752}. Best is trial 14 with value: 0.6294277929155313.\n",
      "[W 2025-08-20 16:21:10,255] The parameter 'top_rate' in trial#16 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:21:10,277] The parameter 'other_rate' in trial#16 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.629428:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 17/35 [21:56<07:22, 24.60s/it, 1316.51/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:21:22,846] Trial 16 finished with value: 0.6280336800396236 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.02073446323976834, 'num_leaves': 61, 'max_depth': 3, 'min_child_samples': 188, 'subsample': 0.5929603325126908, 'colsample_bytree': 0.5618702091755807, 'reg_alpha': 2.573513029335083, 'reg_lambda': 2.5522228976098056, 'scale_pos_weight': 1.5948236331350052, 'n_estimators': 1900, 'top_rate': 0.4922719851905597, 'other_rate': 0.07722757842943569}. Best is trial 14 with value: 0.6294277929155313.\n",
      "[W 2025-08-20 16:21:22,945] The parameter 'top_rate' in trial#17 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:21:22,961] The parameter 'other_rate' in trial#17 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.629428:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [22:14<06:26, 22.73s/it, 1334.86/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:21:41,197] Trial 17 finished with value: 0.6100352112676056 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.01681556063230344, 'num_leaves': 170, 'max_depth': 4, 'min_child_samples': 158, 'subsample': 0.9383923557897222, 'colsample_bytree': 0.9263914122635695, 'reg_alpha': 13.729680208597422, 'reg_lambda': 4.745253519762698, 'scale_pos_weight': 3.4254424076803094, 'n_estimators': 3300, 'top_rate': 0.29373816783210827, 'other_rate': 0.09436436288012343}. Best is trial 14 with value: 0.6294277929155313.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: 0.63212:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/35 [22:29<05:22, 20.18s/it, 1349.10/3000 seconds] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:21:55,431] Trial 18 finished with value: 0.6321195144724556 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011805025370012044, 'num_leaves': 81, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.6516591592392186, 'colsample_bytree': 0.7887902926980933, 'reg_alpha': 1.7534434354213477, 'reg_lambda': 1.4522298627647672, 'scale_pos_weight': 1.6719022986671448, 'n_estimators': 1800}. Best is trial 18 with value: 0.6321195144724556.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 18. Best value: 0.63212:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [22:35<04:01, 16.11s/it, 1355.72/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:22:02,050] Trial 19 finished with value: 0.6310408921933085 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.037560950207882554, 'num_leaves': 44, 'max_depth': 3, 'min_child_samples': 27, 'subsample': 0.5814314021096709, 'colsample_bytree': 0.8454646845834226, 'reg_alpha': 3.861528179407653, 'reg_lambda': 3.551987494382063, 'scale_pos_weight': 1.2900761404189214, 'n_estimators': 1300}. Best is trial 18 with value: 0.6321195144724556.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 21/35 [22:47<03:26, 14.72s/it, 1367.20/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:22:13,533] Trial 20 finished with value: 0.6349663784822286 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.02281950195953905, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 15, 'subsample': 0.5289778371830722, 'colsample_bytree': 0.9167283468391108, 'reg_alpha': 5.010767278784132, 'reg_lambda': 0.4889329861953762, 'scale_pos_weight': 1.2781246186079176, 'n_estimators': 2300}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 22/35 [23:03<03:16, 15.08s/it, 1383.11/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:22:29,450] Trial 21 finished with value: 0.633 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.011795269497185106, 'num_leaves': 81, 'max_depth': 7, 'min_child_samples': 65, 'subsample': 0.5228980796676831, 'colsample_bytree': 0.9441405045557073, 'reg_alpha': 4.774441703466517, 'reg_lambda': 0.085915079786265, 'scale_pos_weight': 1.3060321432229864, 'n_estimators': 2400}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 23/35 [23:19<03:05, 15.44s/it, 1399.38/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:22:45,720] Trial 22 finished with value: 0.6349480968858131 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.014824340600254721, 'num_leaves': 27, 'max_depth': 12, 'min_child_samples': 14, 'subsample': 0.5429067832678253, 'colsample_bytree': 0.8825233325389881, 'reg_alpha': 9.167332098591995, 'reg_lambda': 0.26151665969598903, 'scale_pos_weight': 1.265169893352872, 'n_estimators': 3800}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [23:27<02:27, 13.38s/it, 1407.99/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:22:54,321] Trial 23 finished with value: 0.6339486717694732 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.0350259557595521, 'num_leaves': 38, 'max_depth': 12, 'min_child_samples': 27, 'subsample': 0.5616596633435142, 'colsample_bytree': 0.8487133168837218, 'reg_alpha': 8.405888353200174, 'reg_lambda': 0.026398483669843043, 'scale_pos_weight': 1.140941068771075, 'n_estimators': 4400}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 25/35 [23:35<01:57, 11.71s/it, 1415.77/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:23:02,108] Trial 24 finished with value: 0.6202185792349727 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.025052787495389726, 'num_leaves': 28, 'max_depth': 14, 'min_child_samples': 92, 'subsample': 0.5279881178812226, 'colsample_bytree': 0.9026575409874062, 'reg_alpha': 15.186543168927344, 'reg_lambda': 1.9501657406408872, 'scale_pos_weight': 1.0046724610209252, 'n_estimators': 3400}. Best is trial 20 with value: 0.6349663784822286.\n",
      "[W 2025-08-20 16:23:02,227] The parameter 'drop_rate' in trial#25 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:23:02,242] The parameter 'skip_drop' in trial#25 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [26:23<08:47, 58.59s/it, 1583.76/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:25:50,094] Trial 25 finished with value: 0.6116996775679411 and parameters: {'boosting_type': 'dart', 'learning_rate': 0.017595314174224625, 'num_leaves': 87, 'max_depth': 8, 'min_child_samples': 62, 'subsample': 0.5231857168513081, 'colsample_bytree': 0.9269459029537273, 'reg_alpha': 9.663310095279828, 'reg_lambda': 0.1936458990521994, 'scale_pos_weight': 2.6736323233177655, 'n_estimators': 5000, 'drop_rate': 0.49913558295387594, 'skip_drop': 0.7577682403504806}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 27/35 [26:28<05:38, 42.28s/it, 1587.99/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:25:54,328] Trial 26 finished with value: 0.6310262529832935 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.029841109200014822, 'num_leaves': 159, 'max_depth': 10, 'min_child_samples': 20, 'subsample': 0.5459046844665776, 'colsample_bytree': 0.6504464019379638, 'reg_alpha': 10.585478078206062, 'reg_lambda': 3.1390776170665005, 'scale_pos_weight': 2.2839300987871884, 'n_estimators': 5000}. Best is trial 20 with value: 0.6349663784822286.\n",
      "[W 2025-08-20 16:25:54,388] The parameter 'top_rate' in trial#27 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n",
      "[W 2025-08-20 16:25:54,399] The parameter 'other_rate' in trial#27 is sampled independently instead of being sampled by multivariate TPE sampler. (optimization performance may be degraded). You can suppress this warning by setting `warn_independent_sampling` to `False` in the constructor of `TPESampler`, if this independent sampling is intended behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 20. Best value: 0.634966:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [26:35<03:43, 31.94s/it, 1595.78/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:02,106] Trial 27 finished with value: 0.6292660121552127 and parameters: {'boosting_type': 'goss', 'learning_rate': 0.0220508584012074, 'num_leaves': 106, 'max_depth': 13, 'min_child_samples': 5, 'subsample': 0.5152015451811531, 'colsample_bytree': 0.8718169324075473, 'reg_alpha': 6.668150198899586, 'reg_lambda': 1.529194425085276, 'scale_pos_weight': 2.1853424482855712, 'n_estimators': 4000, 'top_rate': 0.23336986691471617, 'other_rate': 0.19676791008752084}. Best is trial 20 with value: 0.6349663784822286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.635666:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 29/35 [26:37<02:17, 22.85s/it, 1597.43/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:03,771] Trial 28 finished with value: 0.635665914221219 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09173450373795272, 'num_leaves': 85, 'max_depth': 15, 'min_child_samples': 23, 'subsample': 0.5801714179023589, 'colsample_bytree': 0.868581129689976, 'reg_alpha': 10.554110490164947, 'reg_lambda': 0.9921855746217325, 'scale_pos_weight': 2.0502307087838902, 'n_estimators': 4200}. Best is trial 28 with value: 0.635665914221219.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.635666:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [26:39<01:22, 16.59s/it, 1599.41/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:05,751] Trial 29 finished with value: 0.6195840554592721 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.08436481712754358, 'num_leaves': 216, 'max_depth': 14, 'min_child_samples': 5, 'subsample': 0.6912516363277451, 'colsample_bytree': 0.9396948225613067, 'reg_alpha': 13.996723163666424, 'reg_lambda': 0.01609240071842377, 'scale_pos_weight': 3.0765533400881937, 'n_estimators': 3600}. Best is trial 28 with value: 0.635665914221219.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.635666:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 31/35 [26:41<00:48, 12.15s/it, 1601.22/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:07,555] Trial 30 finished with value: 0.6211546565528866 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.10382679575686926, 'num_leaves': 79, 'max_depth': 15, 'min_child_samples': 22, 'subsample': 0.5098143779828781, 'colsample_bytree': 0.8693183582758025, 'reg_alpha': 10.476637496236796, 'reg_lambda': 5.939512202026931, 'scale_pos_weight': 3.674616801923441, 'n_estimators': 3500}. Best is trial 28 with value: 0.635665914221219.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 28. Best value: 0.635666:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [26:42<00:26,  8.97s/it, 1602.76/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:09,097] Trial 31 finished with value: 0.6235011990407674 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.09271898059263056, 'num_leaves': 21, 'max_depth': 14, 'min_child_samples': 67, 'subsample': 0.7047156578399245, 'colsample_bytree': 0.9309642419918084, 'reg_alpha': 5.625287500245944, 'reg_lambda': 2.7844555269831197, 'scale_pos_weight': 1.2347664632889441, 'n_estimators': 5000}. Best is trial 28 with value: 0.635665914221219.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 32. Best value: 0.635697:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 33/35 [26:47<00:15,  7.84s/it, 1607.96/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:14,303] Trial 32 finished with value: 0.6356968215158925 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.020089660425193962, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 23, 'subsample': 0.7037612304168834, 'colsample_bytree': 0.908787000239986, 'reg_alpha': 9.475712324182632, 'reg_lambda': 1.4470955390065738, 'scale_pos_weight': 1.8077946189849352, 'n_estimators': 4300}. Best is trial 32 with value: 0.6356968215158925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 32. Best value: 0.635697:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [26:54<00:07,  7.37s/it, 1614.24/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:20,579] Trial 33 finished with value: 0.6298142274580879 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.010447563168716348, 'num_leaves': 20, 'max_depth': 10, 'min_child_samples': 44, 'subsample': 0.7911700017306138, 'colsample_bytree': 0.9554309531828697, 'reg_alpha': 5.064086689385552, 'reg_lambda': 0.1931255097256812, 'scale_pos_weight': 1.4393552594767203, 'n_estimators': 4600}. Best is trial 32 with value: 0.6356968215158925.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 32. Best value: 0.635697: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [27:05<00:00, 46.46s/it, 1625.95/3000 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 16:26:32,289] Trial 34 finished with value: 0.6272352132049519 and parameters: {'boosting_type': 'gbdt', 'learning_rate': 0.01113477316441322, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 34, 'subsample': 0.5776900979584045, 'colsample_bytree': 0.7758804535488263, 'reg_alpha': 10.507135851796269, 'reg_lambda': 4.737471218511166, 'scale_pos_weight': 2.834154461472165, 'n_estimators': 3400}. Best is trial 32 with value: 0.6356968215158925.\n",
      "\n",
      "ğŸ¯ LightGBMæœ€é©åŒ–çµæœ:\n",
      "Best F1: 0.635697\n",
      "Improvement: +0.009252\n",
      "Best threshold: 0.4300\n",
      "âœ… LightGBMæ”¹å–„æˆåŠŸï¼\n",
      "\n",
      "ğŸ“Š Phase 1æœ€é©åŒ–çµæœ:\n",
      "CatBoost: âŒå¾®å° (0.627744)\n",
      "LightGBM: âœ…æ”¹å–„ (0.635697)\n",
      "\n",
      "ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: æ”¹å–„ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’\n",
      "1. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®æœ¬æ ¼5-foldå­¦ç¿’\n",
      "2. æ–°ã—ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–\n",
      "3. Phase 2: é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n",
      "\n",
      "æœŸå¾…ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ”¹å–„: +0.005181\n",
      "æœŸå¾…F1: 0.638617\n",
      "ç›®æ¨™0.66ã¸ã®é€²æ—: 19.5%\n",
      "\n",
      "âœ… Phase 1å®Œäº†ï¼æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Phase 1: å¼·åŒ–ç‰¹å¾´é‡ã§ã®Optunaæœ€é©åŒ–ï¼ˆå®Ÿè£…ç‰ˆï¼‰\n",
    "\n",
    "# import optuna\n",
    "# from catboost import CatBoostClassifier, Pool\n",
    "# from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "# import xgboost as xgb\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# print(\"ğŸ¯ Phase 1: F1ã‚¹ã‚³ã‚¢0.66ã¸ã®ç¬¬ä¸€æ­© - Optunaæœ€é©åŒ–\")\n",
    "# print(f\"ç¾åœ¨F1: 0.633436 â†’ ç›®æ¨™: 0.645+ (+0.012)\")\n",
    "\n",
    "# # === 1. CatBoostæœ€é©åŒ– ===\n",
    "# def create_catboost_objective():\n",
    "#     \"\"\"36ç‰¹å¾´é‡ã§ã®CatBoostæœ€é©åŒ–\"\"\"\n",
    "#     def objective(trial):\n",
    "#         params = {\n",
    "#             \"iterations\": trial.suggest_int(\"iterations\", 2000, 10000, step=500),\n",
    "#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "#             \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "#             \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.1, 100, log=True),\n",
    "#             \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 8.0),\n",
    "#             \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 5.0),\n",
    "#             \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#             \"rsm\": trial.suggest_float(\"rsm\", 0.4, 1.0),\n",
    "#             \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 8.0),\n",
    "#             \"border_count\": trial.suggest_int(\"border_count\", 32, 254),\n",
    "#             \"max_ctr_complexity\": trial.suggest_int(\"max_ctr_complexity\", 1, 6),\n",
    "            \n",
    "#             # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "#             \"loss_function\": \"Logloss\",\n",
    "#             \"eval_metric\": \"F1\",\n",
    "#             \"random_seed\": SEED,\n",
    "#             \"verbose\": False,\n",
    "#             \"thread_count\": -1,\n",
    "#             \"use_best_model\": True,\n",
    "#             \"allow_writing_files\": False,\n",
    "#         }\n",
    "        \n",
    "#         # 5-fold CV\n",
    "#         oof = np.zeros(len(X_train), dtype=float)\n",
    "#         for train_pool, valid_pool, va_idx in pools_full:\n",
    "#             model = CatBoostClassifier(**params)\n",
    "#             model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=150)\n",
    "#             oof[va_idx] = model.predict_proba(valid_pool)[:, 1]\n",
    "        \n",
    "#         # è¤‡æ•°é–¾å€¤ã§F1æœ€é©åŒ–\n",
    "#         thresholds = np.linspace(0.15, 0.45, 31)\n",
    "#         f1s = [f1_score(y_train, (oof >= t).astype(int)) for t in thresholds]\n",
    "#         best_f1 = max(f1s)\n",
    "#         best_th = thresholds[np.argmax(f1s)]\n",
    "        \n",
    "#         trial.set_user_attr(\"best_threshold\", best_th)\n",
    "#         return best_f1\n",
    "    \n",
    "#     return objective\n",
    "\n",
    "# print(\"\\n=== CatBoostæœ€é©åŒ–é–‹å§‹ ===\")\n",
    "# study_cb = optuna.create_study(\n",
    "#     direction=\"maximize\",\n",
    "#     sampler=optuna.samplers.TPESampler(seed=SEED, n_startup_trials=15, multivariate=True),\n",
    "#     pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=300, reduction_factor=2)\n",
    "# )\n",
    "\n",
    "# study_cb.optimize(create_catboost_objective(), n_trials=40, timeout=3600, show_progress_bar=True)\n",
    "\n",
    "# # çµæœè©•ä¾¡\n",
    "# best_cb_f1 = study_cb.best_value\n",
    "# best_cb_params = study_cb.best_trial.params\n",
    "# best_cb_th = study_cb.best_trial.user_attrs.get(\"best_threshold\", 0.285)\n",
    "\n",
    "# print(f\"\\nğŸ¯ CatBoostæœ€é©åŒ–çµæœ:\")\n",
    "# print(f\"Best F1: {best_cb_f1:.6f}\")\n",
    "# print(f\"Improvement: {best_cb_f1 - 0.633166:+.6f}\")\n",
    "# print(f\"Best threshold: {best_cb_th:.4f}\")\n",
    "\n",
    "# if best_cb_f1 > 0.633166 + 0.002:  # 0.2%ä»¥ä¸Šæ”¹å–„\n",
    "#     print(\"âœ… CatBoostæ”¹å–„æˆåŠŸï¼\")\n",
    "#     CB_IMPROVED = True\n",
    "# else:\n",
    "#     print(\"â†’ CatBoostæ”¹å–„ã¯å¾®å°\")\n",
    "#     CB_IMPROVED = False\n",
    "\n",
    "# # === 2. LightGBMæœ€é©åŒ– ===\n",
    "# def create_lightgbm_objective():\n",
    "#     \"\"\"36ç‰¹å¾´é‡ã§ã®LightGBMæœ€é©åŒ–\"\"\"\n",
    "#     def objective(trial):\n",
    "#         params = {\n",
    "#             \"objective\": \"binary\",\n",
    "#             \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]),\n",
    "#             \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15, log=True),\n",
    "#             \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n",
    "#             \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "#             \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 300),\n",
    "#             \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#             \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "#             \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 20.0),\n",
    "#             \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 20.0),\n",
    "#             \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", 1.0, 8.0),\n",
    "#             \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 5000, step=100),\n",
    "#             \"random_state\": SEED,\n",
    "#             \"n_jobs\": -1,\n",
    "#             \"verbose\": -1,\n",
    "#         }\n",
    "        \n",
    "#         # DART/GOSSç‰¹æœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "#         if params[\"boosting_type\"] == \"dart\":\n",
    "#             params[\"drop_rate\"] = trial.suggest_float(\"drop_rate\", 0.0, 0.5)\n",
    "#             params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 0.0, 0.8)\n",
    "#         elif params[\"boosting_type\"] == \"goss\":\n",
    "#             params[\"top_rate\"] = trial.suggest_float(\"top_rate\", 0.1, 0.5)\n",
    "#             params[\"other_rate\"] = trial.suggest_float(\"other_rate\", 0.05, 0.2)\n",
    "        \n",
    "#         # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "#         X_train_lgb = X_train.copy()\n",
    "#         for c in cat_cols:\n",
    "#             X_train_lgb[c] = X_train_lgb[c].astype(\"category\")\n",
    "        \n",
    "#         # 5-fold CV\n",
    "#         oof = np.zeros(len(X_train), dtype=float)\n",
    "#         for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_train_lgb, y_train)):\n",
    "#             X_tr, X_va = X_train_lgb.iloc[tr_idx], X_train_lgb.iloc[va_idx]\n",
    "#             y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "            \n",
    "#             model = LGBMClassifier(**params)\n",
    "#             model.fit(\n",
    "#                 X_tr, y_tr,\n",
    "#                 eval_set=[(X_va, y_va)],\n",
    "#                 eval_metric=\"binary_logloss\",\n",
    "#                 callbacks=[early_stopping(stopping_rounds=150, verbose=False)]\n",
    "#             )\n",
    "#             oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "        \n",
    "#         # F1æœ€é©åŒ–\n",
    "#         thresholds = np.linspace(0.15, 0.45, 31)\n",
    "#         f1s = [f1_score(y_train, (oof >= t).astype(int)) for t in thresholds]\n",
    "#         best_f1 = max(f1s)\n",
    "#         best_th = thresholds[np.argmax(f1s)]\n",
    "        \n",
    "#         trial.set_user_attr(\"best_threshold\", best_th)\n",
    "#         return best_f1\n",
    "    \n",
    "#     return objective\n",
    "\n",
    "# print(\"\\n=== LightGBMæœ€é©åŒ–é–‹å§‹ ===\")\n",
    "# study_lgb = optuna.create_study(\n",
    "#     direction=\"maximize\",\n",
    "#     sampler=optuna.samplers.TPESampler(seed=SEED+1, n_startup_trials=12, multivariate=True),\n",
    "#     pruner=optuna.pruners.SuccessiveHalvingPruner(min_resource=200, reduction_factor=2)\n",
    "# )\n",
    "\n",
    "# study_lgb.optimize(create_lightgbm_objective(), n_trials=35, timeout=3000, show_progress_bar=True)\n",
    "\n",
    "# # çµæœè©•ä¾¡\n",
    "# best_lgb_f1 = study_lgb.best_value\n",
    "# best_lgb_params = study_lgb.best_trial.params\n",
    "# best_lgb_th = study_lgb.best_trial.user_attrs.get(\"best_threshold\", 0.285)\n",
    "\n",
    "# print(f\"\\nğŸ¯ LightGBMæœ€é©åŒ–çµæœ:\")\n",
    "# print(f\"Best F1: {best_lgb_f1:.6f}\")\n",
    "# print(f\"Improvement: {best_lgb_f1 - 0.626445:+.6f}\")\n",
    "# print(f\"Best threshold: {best_lgb_th:.4f}\")\n",
    "\n",
    "# if best_lgb_f1 > 0.626445 + 0.002:  # 0.2%ä»¥ä¸Šæ”¹å–„\n",
    "#     print(\"âœ… LightGBMæ”¹å–„æˆåŠŸï¼\")\n",
    "#     LGB_IMPROVED = True\n",
    "# else:\n",
    "#     print(\"â†’ LightGBMæ”¹å–„ã¯å¾®å°\")\n",
    "#     LGB_IMPROVED = False\n",
    "\n",
    "# # === 3. æœ€é©åŒ–çµæœçµ±åˆ ===\n",
    "# print(f\"\\nğŸ“Š Phase 1æœ€é©åŒ–çµæœ:\")\n",
    "# print(f\"CatBoost: {'âœ…æ”¹å–„' if CB_IMPROVED else 'âŒå¾®å°'} ({best_cb_f1:.6f})\")\n",
    "# print(f\"LightGBM: {'âœ…æ”¹å–„' if LGB_IMPROVED else 'âŒå¾®å°'} ({best_lgb_f1:.6f})\")\n",
    "\n",
    "# if CB_IMPROVED or LGB_IMPROVED:\n",
    "#     print(\"\\nğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: æ”¹å–„ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’\")\n",
    "#     print(\"1. æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®æœ¬æ ¼5-foldå­¦ç¿’\")\n",
    "#     print(\"2. æ–°ã—ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ–\")\n",
    "#     print(\"3. Phase 2: é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\")\n",
    "    \n",
    "#     # æœŸå¾…åŠ¹æœã®è¨ˆç®—\n",
    "#     cb_gain = max(0, best_cb_f1 - 0.633166) if CB_IMPROVED else 0\n",
    "#     lgb_gain = max(0, best_lgb_f1 - 0.626445) if LGB_IMPROVED else 0\n",
    "#     expected_ensemble_gain = cb_gain * 0.44 + lgb_gain * 0.56  # ç¾åœ¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿\n",
    "    \n",
    "#     print(f\"\\næœŸå¾…ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ”¹å–„: +{expected_ensemble_gain:.6f}\")\n",
    "#     print(f\"æœŸå¾…F1: {0.633436 + expected_ensemble_gain:.6f}\")\n",
    "    \n",
    "#     # ç›®æ¨™é”æˆåº¦\n",
    "#     progress = (expected_ensemble_gain / 0.026564) * 100\n",
    "#     print(f\"ç›®æ¨™0.66ã¸ã®é€²æ—: {progress:.1f}%\")\n",
    "# else:\n",
    "#     print(\"\\nâ†’ Phase 1ã§ã¯å¤§å¹…æ”¹å–„ãªã—\")\n",
    "#     print(\"â†’ Phase 2ã®é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«é€²ã‚€\")\n",
    "\n",
    "# # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜\n",
    "# PHASE1_RESULTS = {\n",
    "#     \"cb_improved\": CB_IMPROVED,\n",
    "#     \"lgb_improved\": LGB_IMPROVED,\n",
    "#     \"best_cb_params\": best_cb_params,\n",
    "#     \"best_lgb_params\": best_lgb_params,\n",
    "#     \"best_cb_f1\": best_cb_f1,\n",
    "#     \"best_lgb_f1\": best_lgb_f1\n",
    "# }\n",
    "\n",
    "# print(\"\\nâœ… Phase 1å®Œäº†ï¼æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a90473",
   "metadata": {},
   "source": [
    "# 9.1.5 Phase 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Phase 1.5: LightGBMæ”¹å–„ã‚’æ´»ç”¨ã—ãŸæœ¬æ ¼å†å­¦ç¿’\n",
      "æœŸå¾…F1: 0.638617 â†’ ç›®æ¨™: 0.645+\n",
      "\n",
      "=== æœ€é©åŒ–LightGBMã§æœ¬æ ¼5-foldå­¦ç¿’ ===\n",
      "æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨: gbdt, lr=0.0201\n",
      "Fold 1/5 å­¦ç¿’ä¸­... F1=0.635922\n",
      "Fold 2/5 å­¦ç¿’ä¸­... F1=0.629268\n",
      "Fold 3/5 å­¦ç¿’ä¸­... F1=0.633803\n",
      "Fold 4/5 å­¦ç¿’ä¸­... F1=0.608479\n",
      "Fold 5/5 å­¦ç¿’ä¸­... F1=0.637037\n",
      "\n",
      "âœ… æœ€é©åŒ–LightGBMçµæœ:\n",
      "Best F1: 0.633380\n",
      "Best threshold: 0.3800\n",
      "æ”¹å–„: +0.006935\n",
      "\n",
      "=== CatBoostæˆ¦ç•¥æ±ºå®š ===\n",
      "å…ƒã®CatBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¶­æŒï¼ˆéé©åˆå›é¿ï¼‰\n",
      "\n",
      "=== æ–°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ– ===\n",
      "ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–ä¸­...\n",
      "\n",
      "ğŸ¯ æ–°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ:\n",
      "æœ€é©é‡ã¿: CB 0.450 + LGB 0.550\n",
      "æœ€é©é–¾å€¤: 0.3200\n",
      "Best F1: 0.637050\n",
      "æ”¹å–„: +0.003614\n",
      "\n",
      "ğŸ“Š F1ã‚¹ã‚³ã‚¢0.66ã¸ã®é€²æ—:\n",
      "Phase 1å®Œäº†: 0.637050\n",
      "é€²æ—ç‡: 13.6%\n",
      "æ®‹ã‚Šæ”¹å–„: 0.022950\n",
      "âš ï¸ æœŸå¾…ã‚ˆã‚Šå°ã•ãªæ”¹å–„ã€‚Phase 2ã§å¤§èƒ†ãªå¤‰æ›´\n",
      "\n",
      "ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: Phase 2: ãƒ‰ãƒ©ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´é‡æ”¹è‰¯\n",
      "\n",
      "âœ… Phase 1.5å®Œäº†ï¼æ–°ã—ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "# # Phase 1.5: æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®æœ¬æ ¼å†å­¦ç¿’ã¨æ–°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "\n",
    "# print(\"ğŸ¯ Phase 1.5: LightGBMæ”¹å–„ã‚’æ´»ç”¨ã—ãŸæœ¬æ ¼å†å­¦ç¿’\")\n",
    "# print(f\"æœŸå¾…F1: 0.638617 â†’ ç›®æ¨™: 0.645+\")\n",
    "\n",
    "# # === 1. æœ€é©åŒ–ã•ã‚ŒãŸLightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’ ===\n",
    "# print(\"\\n=== æœ€é©åŒ–LightGBMã§æœ¬æ ¼5-foldå­¦ç¿’ ===\")\n",
    "\n",
    "# # PHASE1_RESULTSã‹ã‚‰æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "# best_lgb_params = PHASE1_RESULTS[\"best_lgb_params\"]\n",
    "# print(f\"æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨: {best_lgb_params['boosting_type']}, lr={best_lgb_params['learning_rate']:.4f}\")\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆã‚«ãƒ†ã‚´ãƒªå‹å¤‰æ›ï¼‰\n",
    "# X_train_lgb = X_train.copy()\n",
    "# for c in cat_cols:\n",
    "#     X_train_lgb[c] = X_train_lgb[c].astype(\"category\")\n",
    "\n",
    "# X_test_lgb = X_test.copy()  \n",
    "# for c in cat_cols:\n",
    "#     X_test_lgb[c] = X_test_lgb[c].astype(\"category\")\n",
    "\n",
    "# # æœ€é©åŒ–LightGBMå­¦ç¿’\n",
    "# oof_lgb_optimized = np.zeros(len(X_train), dtype=float)\n",
    "# pred_lgb_optimized = np.zeros(len(X_test), dtype=float)\n",
    "\n",
    "# for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_train_lgb, y_train)):\n",
    "#     print(f\"Fold {fold+1}/5 å­¦ç¿’ä¸­...\", end=\"\")\n",
    "    \n",
    "#     X_tr, X_va = X_train_lgb.iloc[tr_idx], X_train_lgb.iloc[va_idx]\n",
    "#     y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "    \n",
    "#     # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "#     model = LGBMClassifier(**best_lgb_params)\n",
    "#     model.fit(\n",
    "#         X_tr, y_tr,\n",
    "#         eval_set=[(X_va, y_va)],\n",
    "#         eval_metric=\"binary_logloss\",\n",
    "#         callbacks=[early_stopping(stopping_rounds=200, verbose=False)]\n",
    "#     )\n",
    "    \n",
    "#     # äºˆæ¸¬\n",
    "#     oof_lgb_optimized[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "#     pred_lgb_optimized += model.predict_proba(X_test_lgb)[:, 1] / 5\n",
    "    \n",
    "#     # Foldåˆ¥F1ç¢ºèª\n",
    "#     fold_th = 0.43  # æœ€é©é–¾å€¤\n",
    "#     fold_f1 = f1_score(y_va, (model.predict_proba(X_va)[:, 1] >= fold_th).astype(int))\n",
    "#     print(f\" F1={fold_f1:.6f}\")\n",
    "\n",
    "# # æœ€é©åŒ–LightGBMã®å…¨ä½“F1ç¢ºèª\n",
    "# thresholds = np.linspace(0.35, 0.50, 31)\n",
    "# f1s = [f1_score(y_train, (oof_lgb_optimized >= t).astype(int)) for t in thresholds]\n",
    "# best_f1_lgb_opt = max(f1s)\n",
    "# best_th_lgb_opt = thresholds[np.argmax(f1s)]\n",
    "\n",
    "# print(f\"\\nâœ… æœ€é©åŒ–LightGBMçµæœ:\")\n",
    "# print(f\"Best F1: {best_f1_lgb_opt:.6f}\")\n",
    "# print(f\"Best threshold: {best_th_lgb_opt:.4f}\")\n",
    "# print(f\"æ”¹å–„: {best_f1_lgb_opt - 0.626445:+.6f}\")\n",
    "\n",
    "# # === 2. CatBoost: å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ vs è»½å¾®èª¿æ•´ç‰ˆ ===\n",
    "# print(\"\\n=== CatBoostæˆ¦ç•¥æ±ºå®š ===\")\n",
    "\n",
    "# if PHASE1_RESULTS[\"cb_improved\"]:\n",
    "#     print(\"æœ€é©åŒ–CatBoostã‚’ä½¿ç”¨\")\n",
    "#     # æœ€é©åŒ–ç‰ˆã§CatBoostå†å­¦ç¿’ï¼ˆã‚³ãƒ¼ãƒ‰ã¯é¡ä¼¼ã®ãŸã‚çœç•¥å¯èƒ½ï¼‰\n",
    "#     oof_cb_final = oof_cb  # æ—¢å­˜ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€å†å­¦ç¿’\n",
    "#     pred_cb_final = test_cb  # æ­£ã—ã„å¤‰æ•°å\n",
    "#     cb_f1_final = PHASE1_RESULTS[\"best_cb_f1\"]\n",
    "# else:\n",
    "#     print(\"å…ƒã®CatBoostãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¶­æŒï¼ˆéé©åˆå›é¿ï¼‰\")\n",
    "#     oof_cb_final = oof_cb  # æ—¢å­˜ã®CatBoostçµæœã‚’ä½¿ç”¨\n",
    "#     pred_cb_final = test_cb  # æ­£ã—ã„å¤‰æ•°å\n",
    "#     cb_f1_final = 0.633166  # å…ƒã®F1ã‚¹ã‚³ã‚¢\n",
    "\n",
    "# # === 3. æ–°ã—ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ– ===\n",
    "# print(\"\\n=== æ–°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœ€é©åŒ– ===\")\n",
    "\n",
    "# # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å€™è£œ\n",
    "# ensemble_candidates = {\n",
    "#     \"cb_original\": oof_cb,\n",
    "#     \"lgb_optimized\": oof_lgb_optimized\n",
    "# }\n",
    "\n",
    "# # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©é‡ã¿æ¢ç´¢\n",
    "# best_ensemble_f1 = 0\n",
    "# best_weights = None\n",
    "# best_ensemble_th = None\n",
    "\n",
    "# weight_range = np.linspace(0.2, 0.8, 13)  # CatBoosté‡ã¿\n",
    "# threshold_range = np.linspace(0.25, 0.45, 21)\n",
    "\n",
    "# print(\"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–ä¸­...\")\n",
    "# for cb_weight in weight_range:\n",
    "#     lgb_weight = 1 - cb_weight\n",
    "    \n",
    "#     # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n",
    "#     oof_ensemble = cb_weight * oof_cb + lgb_weight * oof_lgb_optimized\n",
    "    \n",
    "#     # æœ€é©é–¾å€¤æ¢ç´¢\n",
    "#     for th in threshold_range:\n",
    "#         pred_ensemble = (oof_ensemble >= th).astype(int)\n",
    "#         f1 = f1_score(y_train, pred_ensemble)\n",
    "        \n",
    "#         if f1 > best_ensemble_f1:\n",
    "#             best_ensemble_f1 = f1\n",
    "#             best_weights = (cb_weight, lgb_weight)\n",
    "#             best_ensemble_th = th\n",
    "\n",
    "# print(f\"\\nğŸ¯ æ–°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ:\")\n",
    "# print(f\"æœ€é©é‡ã¿: CB {best_weights[0]:.3f} + LGB {best_weights[1]:.3f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {best_ensemble_th:.4f}\")\n",
    "# print(f\"Best F1: {best_ensemble_f1:.6f}\")\n",
    "# print(f\"æ”¹å–„: {best_ensemble_f1 - 0.633436:+.6f}\")\n",
    "\n",
    "# # === 4. æœ€çµ‚äºˆæ¸¬ç”Ÿæˆ ===\n",
    "# pred_test_final = best_weights[0] * test_cb + best_weights[1] * pred_lgb_optimized\n",
    "# pred_test_binary = (pred_test_final >= best_ensemble_th).astype(int)\n",
    "\n",
    "# # === 5. é€²æ—è©•ä¾¡ ===\n",
    "# progress_to_660 = (best_ensemble_f1 - 0.633436) / 0.026564 * 100\n",
    "# remaining_improvement = 0.66 - best_ensemble_f1\n",
    "\n",
    "# print(f\"\\nğŸ“Š F1ã‚¹ã‚³ã‚¢0.66ã¸ã®é€²æ—:\")\n",
    "# print(f\"Phase 1å®Œäº†: {best_ensemble_f1:.6f}\")\n",
    "# print(f\"é€²æ—ç‡: {progress_to_660:.1f}%\")\n",
    "# print(f\"æ®‹ã‚Šæ”¹å–„: {remaining_improvement:.6f}\")\n",
    "\n",
    "# if best_ensemble_f1 >= 0.642:\n",
    "#     print(\"âœ… Phase 1ç›®æ¨™é”æˆï¼Phase 2ã¸\")\n",
    "#     next_phase = \"Phase 2: é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\"\n",
    "# elif best_ensemble_f1 >= 0.638:\n",
    "#     print(\"ğŸ”„ Phase 1éƒ¨åˆ†æˆåŠŸã€‚Phase 1.7ã§å¾®èª¿æ•´\")\n",
    "#     next_phase = \"Phase 1.7: CatBoostå†èª¿æ•´\"\n",
    "# else:\n",
    "#     print(\"âš ï¸ æœŸå¾…ã‚ˆã‚Šå°ã•ãªæ”¹å–„ã€‚Phase 2ã§å¤§èƒ†ãªå¤‰æ›´\")\n",
    "#     next_phase = \"Phase 2: ãƒ‰ãƒ©ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´é‡æ”¹è‰¯\"\n",
    "\n",
    "# print(f\"\\nğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {next_phase}\")\n",
    "\n",
    "# # çµæœä¿å­˜\n",
    "# PHASE1_5_RESULTS = {\n",
    "#     \"best_ensemble_f1\": best_ensemble_f1,\n",
    "#     \"best_weights\": best_weights,\n",
    "#     \"best_threshold\": best_ensemble_th,\n",
    "#     \"lgb_optimized_f1\": best_f1_lgb_opt,\n",
    "#     \"improvement\": best_ensemble_f1 - 0.633436,\n",
    "#     \"progress_to_660\": progress_to_660\n",
    "# }\n",
    "\n",
    "# print(\"\\nâœ… Phase 1.5å®Œäº†ï¼æ–°ã—ã„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ed120",
   "metadata": {},
   "source": [
    "# 9.2 Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Phase 2: ãƒ‰ãƒ©ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´é‡æ”¹è‰¯\n",
      "ç¾åœ¨F1: 0.637050 â†’ ç›®æ¨™: 0.660+\n",
      "å¿…è¦æ”¹å–„: 0.022950\n",
      "\n",
      "=== é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®æ§‹ç¯‰ ===\n",
      "\n",
      "=== é«˜æ¬¡ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã®æ§‹ç¯‰ ===\n",
      "\n",
      "=== é«˜åº¦é‡‘èãƒªã‚¹ã‚¯æŒ‡æ¨™ã®æ§‹ç¯‰ ===\n",
      "\n",
      "=== äº‹æ¥­ç‰¹æ€§é«˜åº¦åˆ†æç‰¹å¾´é‡ ===\n",
      "\n",
      "=== å…¨é«˜åº¦ç‰¹å¾´é‡ã®çµ±åˆé©ç”¨ ===\n",
      "âœ… é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†\n",
      "ç‰¹å¾´é‡æ•°: 36 â†’ 61 (+25)\n",
      "ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: 6 â†’ 15 (+9)\n",
      "\n",
      "=== é«˜åº¦ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ ===\n",
      "æ–°è¦ç‰¹å¾´é‡ (25å€‹):\n",
      "  - year_default_rate\n",
      "  - crisis_year\n",
      "  - year_trend\n",
      "  - post_2015\n",
      "  - sector_year_combo\n",
      "  - digital_friendly\n",
      "  - covid_resistant\n",
      "  - covid_vulnerable\n",
      "  - district_sector_combo\n",
      "  - major_city_district\n",
      "  ... and 15 more\n",
      "\n",
      "âš¡ ç°¡æ˜“åŠ¹æœæ¤œè¨¼ï¼ˆLightGBM 1-foldï¼‰\n",
      "ğŸ¯ é«˜åº¦ç‰¹å¾´é‡LightGBMï¼ˆ1-foldï¼‰:\n",
      "F1ã‚¹ã‚³ã‚¢: 0.615023\n",
      "æœ€é©é–¾å€¤: 0.3700\n",
      "æ¨å®šæ”¹å–„: -0.017977\n",
      "âš ï¸ æ”¹å–„å¾®å°ã€‚ç‰¹å¾´é‡ã‚’å†æ¤œè¨\n",
      "\n",
      "ğŸ“Š F1ã‚¹ã‚³ã‚¢0.66ã¸ã®äºˆæ¸¬:\n",
      "ç¾åœ¨ãƒ™ã‚¹ãƒˆ: 0.637050\n",
      "æœŸå¾…æ”¹å–„: -0.014381\n",
      "æœŸå¾…F1: 0.622669\n",
      "0.66ã¾ã§: 0.037331\n",
      "ğŸ“ˆ æ›´ãªã‚‹æ”¹å–„ãŒå¿…è¦ã€‚Phase 3ã¸\n",
      "\n",
      "âœ… Phase 2å®Œäº†ï¼é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "# # Phase 2: ãƒ‰ãƒ©ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´é‡æ”¹è‰¯ã§F1ã‚¹ã‚³ã‚¢0.66é”æˆ\n",
    "\n",
    "# print(\"ğŸ¯ Phase 2: ãƒ‰ãƒ©ã‚¹ãƒ†ã‚£ãƒƒã‚¯ç‰¹å¾´é‡æ”¹è‰¯\")\n",
    "# print(f\"ç¾åœ¨F1: {PHASE1_5_RESULTS['best_ensemble_f1']:.6f} â†’ ç›®æ¨™: 0.660+\")\n",
    "# print(f\"å¿…è¦æ”¹å–„: {0.66 - PHASE1_5_RESULTS['best_ensemble_f1']:.6f}\")\n",
    "\n",
    "# # === 1. é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ ===\n",
    "# print(\"\\n=== é«˜åº¦æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®æ§‹ç¯‰ ===\")\n",
    "\n",
    "# def create_temporal_features(df):\n",
    "#     \"\"\"é«˜åº¦ãªæ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’ç”Ÿæˆ\"\"\"\n",
    "#     df_temp = df.copy()\n",
    "    \n",
    "#     # 1. å¹´åº¦åˆ¥ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡ï¼ˆTarget Encodingé¢¨ï¼‰\n",
    "#     if 'ApprovalFiscalYear' in df_temp.columns:\n",
    "#         # å¹´åº¦åˆ¥ãƒªã‚¹ã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "#         year_default_rate = train.groupby('ApprovalFiscalYear')[TARGET_COL].mean().to_dict()\n",
    "#         df_temp['year_default_rate'] = df_temp['ApprovalFiscalYear'].map(year_default_rate).fillna(0.128)\n",
    "        \n",
    "#         # å¹´åº¦ã®çµŒæ¸ˆã‚µã‚¤ã‚¯ãƒ«ï¼ˆãƒªãƒ¼ãƒãƒ³ã‚·ãƒ§ãƒƒã‚¯ã€ã‚³ãƒ­ãƒŠç­‰ï¼‰\n",
    "#         high_risk_years = [2008, 2009, 2010, 2020, 2021]  # çµŒæ¸ˆå±æ©Ÿå¹´\n",
    "#         df_temp['crisis_year'] = df_temp['ApprovalFiscalYear'].isin(high_risk_years).astype(int)\n",
    "        \n",
    "#         # å¹´åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ™‚ä»£åŠ¹æœï¼‰\n",
    "#         df_temp['year_trend'] = (df_temp['ApprovalFiscalYear'] - 2000) / 20  # æ­£è¦åŒ–\n",
    "#         df_temp['post_2015'] = (df_temp['ApprovalFiscalYear'] >= 2015).astype(int)\n",
    "    \n",
    "#     # 2. å››åŠæœŸåŠ¹æœï¼ˆç”³è«‹æ™‚æœŸã®å­£ç¯€æ€§ï¼‰\n",
    "#     if 'ApprovalDate' in df_temp.columns:\n",
    "#         # å››åŠæœŸæŠ½å‡º\n",
    "#         df_temp['quarter'] = pd.to_datetime(df_temp['ApprovalDate'], errors='coerce').dt.quarter\n",
    "#         df_temp['is_q4'] = (df_temp['quarter'] == 4).astype(int)  # å¹´åº¦æœ«åŠ¹æœ\n",
    "#         df_temp['is_q1'] = (df_temp['quarter'] == 1).astype(int)  # å¹´åº¦åˆåŠ¹æœ\n",
    "    \n",
    "#     return df_temp\n",
    "\n",
    "# # === 2. ç”£æ¥­Ã—åœ°åŸŸÃ—æ™‚é–“ã®é«˜æ¬¡ç›¸äº’ä½œç”¨ ===\n",
    "# print(\"\\n=== é«˜æ¬¡ç›¸äº’ä½œç”¨ç‰¹å¾´é‡ã®æ§‹ç¯‰ ===\")\n",
    "\n",
    "# def create_interaction_features(df):\n",
    "#     \"\"\"ç”£æ¥­ãƒ»åœ°åŸŸãƒ»æ™‚é–“ã®é«˜æ¬¡ç›¸äº’ä½œç”¨ç‰¹å¾´é‡\"\"\"\n",
    "#     df_int = df.copy()\n",
    "    \n",
    "#     # 1. ç”£æ¥­Ã—å¹´åº¦ã®ãƒªã‚¹ã‚¯é€²åŒ–\n",
    "#     if 'NaicsSector' in df_int.columns and 'ApprovalFiscalYear' in df_int.columns:\n",
    "#         # ç”£æ¥­åˆ¥å¹´åº¦ãƒˆãƒ¬ãƒ³ãƒ‰\n",
    "#         sector_year_combo = df_int['NaicsSector'].astype(str) + '_' + df_int['ApprovalFiscalYear'].astype(str)\n",
    "#         df_int['sector_year_combo'] = sector_year_combo\n",
    "        \n",
    "#         # ç”£æ¥­åˆ¥æ™‚ä»£é©å¿œåº¦ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«åŒ–å¯¾å¿œç­‰ï¼‰\n",
    "#         digital_friendly_sectors = [\n",
    "#             'Information', 'Professional, scientific, and technical services',\n",
    "#             'Finance and insurance', 'Management of companies and enterprises'\n",
    "#         ]\n",
    "#         df_int['digital_friendly'] = df_int['NaicsSector'].isin(digital_friendly_sectors).astype(int)\n",
    "        \n",
    "#         # COVID-19è€æ€§ç”£æ¥­\n",
    "#         covid_resistant = [\n",
    "#             'Information', 'Finance and insurance', \n",
    "#             'Professional, scientific, and technical services',\n",
    "#             'Utilities', 'Wholesale trade'\n",
    "#         ]\n",
    "#         covid_vulnerable = [\n",
    "#             'Accommodation and food services', 'Arts, entertainment, and recreation',\n",
    "#             'Retail trade', 'Transportation and warehousing'\n",
    "#         ]\n",
    "#         df_int['covid_resistant'] = df_int['NaicsSector'].isin(covid_resistant).astype(int)\n",
    "#         df_int['covid_vulnerable'] = df_int['NaicsSector'].isin(covid_vulnerable).astype(int)\n",
    "    \n",
    "#     # 2. åœ°åŸŸÃ—ç”£æ¥­ã®çµŒæ¸ˆåŠ›\n",
    "#     if 'CongressionalDistrict' in df_int.columns and 'NaicsSector' in df_int.columns:\n",
    "#         # åœ°åŸŸç”£æ¥­ç‰¹åŒ–åº¦\n",
    "#         district_sector_combo = df_int['CongressionalDistrict'].astype(str) + '_' + df_int['NaicsSector'].astype(str)\n",
    "#         df_int['district_sector_combo'] = district_sector_combo\n",
    "        \n",
    "#         # ä¸»è¦éƒ½å¸‚åœãƒ•ãƒ©ã‚°\n",
    "#         major_cities = ['CA-12', 'CA-14', 'NY-10', 'NY-12', 'TX-07', 'TX-02']  # ä¾‹\n",
    "#         df_int['major_city_district'] = df_int['CongressionalDistrict'].isin(major_cities).astype(int)\n",
    "    \n",
    "#     return df_int\n",
    "\n",
    "# # === 3. é«˜åº¦é‡‘èãƒªã‚¹ã‚¯æŒ‡æ¨™ ===\n",
    "# print(\"\\n=== é«˜åº¦é‡‘èãƒªã‚¹ã‚¯æŒ‡æ¨™ã®æ§‹ç¯‰ ===\")\n",
    "\n",
    "# def create_advanced_financial_features(df):\n",
    "#     \"\"\"é«˜åº¦ãªé‡‘èãƒªã‚¹ã‚¯ç‰¹å¾´é‡\"\"\"\n",
    "#     df_fin = df.copy()\n",
    "    \n",
    "#     # 1. å¤šæ¬¡å…ƒãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢\n",
    "#     if all(col in df_fin.columns for col in ['GrossApproval', 'TermInMonths', 'InitialInterestRate']):\n",
    "#         # æ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡\n",
    "#         df_fin['amount_norm'] = np.log1p(df_fin['GrossApproval']) / 15  # logæ­£è¦åŒ–\n",
    "#         df_fin['term_norm'] = df_fin['TermInMonths'] / 300  # æœŸé–“æ­£è¦åŒ–\n",
    "#         df_fin['rate_norm'] = df_fin['InitialInterestRate'] / 15  # é‡‘åˆ©æ­£è¦åŒ–\n",
    "        \n",
    "#         # è¤‡åˆãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãï¼‰\n",
    "#         df_fin['compound_risk_v2'] = (\n",
    "#             (1 - df_fin['amount_norm']) * 0.4 +  # å°é¡ = é«˜ãƒªã‚¹ã‚¯\n",
    "#             (1 - df_fin['term_norm']) * 0.3 +    # çŸ­æœŸ = é«˜ãƒªã‚¹ã‚¯  \n",
    "#             df_fin['rate_norm'] * 0.3             # é«˜é‡‘åˆ© = é«˜ãƒªã‚¹ã‚¯\n",
    "#         )\n",
    "        \n",
    "#         # é‡‘èåŠ¹ç‡æ€§æŒ‡æ¨™\n",
    "#         if 'JobsSupported' in df_fin.columns:\n",
    "#             df_fin['capital_efficiency'] = df_fin['GrossApproval'] / (df_fin['JobsSupported'] + 1)\n",
    "#             df_fin['job_cost_risk'] = (df_fin['capital_efficiency'] > df_fin['capital_efficiency'].quantile(0.8)).astype(int)\n",
    "    \n",
    "#     # 2. SBAä¿è¨¼ã®åŠ¹æœçš„æ´»ç”¨åº¦\n",
    "#     if all(col in df_fin.columns for col in ['SBAGuaranteedApproval', 'GrossApproval', 'InitialInterestRate']):\n",
    "#         df_fin['guarantee_utilization'] = df_fin['SBAGuaranteedApproval'] / df_fin['GrossApproval']\n",
    "        \n",
    "#         # ä¿è¨¼ç‡ã¨é‡‘åˆ©ã®ç›¸é–¢ï¼ˆé€šå¸¸ã¯é€†ç›¸é–¢ã®ã¯ãšï¼‰\n",
    "#         df_fin['guarantee_rate_anomaly'] = (\n",
    "#             (df_fin['guarantee_utilization'] < 0.5) & \n",
    "#             (df_fin['InitialInterestRate'] > df_fin['InitialInterestRate'].quantile(0.7))\n",
    "#         ).astype(int)\n",
    "        \n",
    "#         # æœ€é©ä¿è¨¼ç‡ã‹ã‚‰ã®ä¹–é›¢\n",
    "#         optimal_guarantee_rate = 0.75  # ä»®å®š\n",
    "#         df_fin['guarantee_deviation'] = abs(df_fin['guarantee_utilization'] - optimal_guarantee_rate)\n",
    "    \n",
    "#     return df_fin\n",
    "\n",
    "# # === 4. äº‹æ¥­ç‰¹æ€§ã®é«˜åº¦åˆ†æ ===\n",
    "# print(\"\\n=== äº‹æ¥­ç‰¹æ€§é«˜åº¦åˆ†æç‰¹å¾´é‡ ===\")\n",
    "\n",
    "# def create_business_intelligence_features(df):\n",
    "#     \"\"\"äº‹æ¥­ç‰¹æ€§ã®é«˜åº¦åˆ†æç‰¹å¾´é‡\"\"\"\n",
    "#     df_biz = df.copy()\n",
    "    \n",
    "#     # 1. äº‹æ¥­å¹´æ•°ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«\n",
    "#     if 'BusinessAge' in df_biz.columns:\n",
    "#         # ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒªã‚¹ã‚¯\n",
    "#         startup_keywords = ['Startup', 'New Business', '0', '1', '2']\n",
    "#         df_biz['is_startup'] = df_biz['BusinessAge'].astype(str).apply(\n",
    "#             lambda x: any(keyword in str(x) for keyword in startup_keywords)\n",
    "#         ).astype(int)\n",
    "        \n",
    "#         # æˆç†Ÿä¼æ¥­\n",
    "#         df_biz['is_mature'] = (df_biz['BusinessAge'].astype(str).str.contains('10|15|20|25|30')).astype(int)\n",
    "    \n",
    "#     # 2. èè³‡ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®æˆ¦ç•¥çš„æ´»ç”¨\n",
    "#     if 'Subprogram' in df_biz.columns:\n",
    "#         # é«˜ãƒªã‚¹ã‚¯ãƒ—ãƒ­ã‚°ãƒ©ãƒ è­˜åˆ¥\n",
    "#         high_risk_programs = ['Express', '504', 'Microloans']  # ä»®å®š\n",
    "#         df_biz['high_risk_program'] = df_biz['Subprogram'].isin(high_risk_programs).astype(int)\n",
    "        \n",
    "#         # ç‰¹æ®Šç”¨é€”ãƒ—ãƒ­ã‚°ãƒ©ãƒ \n",
    "#         df_biz['special_purpose'] = df_biz['Subprogram'].str.contains('Export|International|Green').astype(int)\n",
    "    \n",
    "#     # 3. é›‡ç”¨å‰µå‡ºåŠ¹ç‡æ€§\n",
    "#     if all(col in df_biz.columns for col in ['JobsSupported', 'GrossApproval']):\n",
    "#         df_biz['job_creation_rate'] = df_biz['JobsSupported'] / (df_biz['GrossApproval'] / 100000)  # 10ä¸‡å††ã‚ãŸã‚Šé›‡ç”¨\n",
    "        \n",
    "#         # é›‡ç”¨åŠ¹ç‡æ€§ã‚«ãƒ†ã‚´ãƒª\n",
    "#         job_efficiency_q75 = df_biz['job_creation_rate'].quantile(0.75)\n",
    "#         df_biz['high_job_efficiency'] = (df_biz['job_creation_rate'] > job_efficiency_q75).astype(int)\n",
    "        \n",
    "#         # å¤§è¦æ¨¡é›‡ç”¨ãƒ•ãƒ©ã‚°\n",
    "#         df_biz['large_employer'] = (df_biz['JobsSupported'] > 50).astype(int)\n",
    "    \n",
    "#     return df_biz\n",
    "\n",
    "# # === 5. ç‰¹å¾´é‡çµ±åˆã¨é©ç”¨ ===\n",
    "# print(\"\\n=== å…¨é«˜åº¦ç‰¹å¾´é‡ã®çµ±åˆé©ç”¨ ===\")\n",
    "\n",
    "# # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨\n",
    "# X_train_advanced = X_train.copy()\n",
    "# X_train_advanced = create_temporal_features(X_train_advanced)\n",
    "# X_train_advanced = create_interaction_features(X_train_advanced)\n",
    "# X_train_advanced = create_advanced_financial_features(X_train_advanced)\n",
    "# X_train_advanced = create_business_intelligence_features(X_train_advanced)\n",
    "\n",
    "# # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ç”¨\n",
    "# X_test_advanced = X_test.copy()\n",
    "# X_test_advanced = create_temporal_features(X_test_advanced)\n",
    "# X_test_advanced = create_interaction_features(X_test_advanced)\n",
    "# X_test_advanced = create_advanced_financial_features(X_test_advanced)\n",
    "# X_test_advanced = create_business_intelligence_features(X_test_advanced)\n",
    "\n",
    "# # ã‚«ãƒ†ã‚´ãƒªåˆ—ã®æ›´æ–°\n",
    "# new_cat_cols = []\n",
    "# for col in X_train_advanced.columns:\n",
    "#     if X_train_advanced[col].dtype == 'object' or col.endswith('_combo'):\n",
    "#         new_cat_cols.append(col)\n",
    "\n",
    "# cat_cols_advanced = cat_cols + new_cat_cols\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "# def prep_df_advanced(df):\n",
    "#     out = df.copy()\n",
    "#     for c in cat_cols_advanced:\n",
    "#         if c in out.columns:\n",
    "#             out[c] = out[c].astype(str).fillna(\"MISSING\")\n",
    "#     return out\n",
    "\n",
    "# X_train_final = prep_df_advanced(X_train_advanced)\n",
    "# X_test_final = prep_df_advanced(X_test_advanced)\n",
    "\n",
    "# print(f\"âœ… é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†\")\n",
    "# print(f\"ç‰¹å¾´é‡æ•°: {len(X_train.columns)} â†’ {len(X_train_final.columns)} (+{len(X_train_final.columns) - len(X_train.columns)})\")\n",
    "# print(f\"ã‚«ãƒ†ã‚´ãƒªåˆ—æ•°: {len(cat_cols)} â†’ {len(cat_cols_advanced)} (+{len(cat_cols_advanced) - len(cat_cols)})\")\n",
    "\n",
    "# # === 6. é«˜é€ŸåŠ¹æœæ¤œè¨¼ ===\n",
    "# print(\"\\n=== é«˜åº¦ç‰¹å¾´é‡ã®åŠ¹æœæ¤œè¨¼ ===\")\n",
    "\n",
    "# # æ–°ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ\n",
    "# new_features = [col for col in X_train_final.columns if col not in X_train.columns]\n",
    "# print(f\"æ–°è¦ç‰¹å¾´é‡ ({len(new_features)}å€‹):\")\n",
    "# for feat in new_features[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º\n",
    "#     print(f\"  - {feat}\")\n",
    "# if len(new_features) > 10:\n",
    "#     print(f\"  ... and {len(new_features) - 10} more\")\n",
    "\n",
    "# # ç°¡æ˜“LightGBMæ¤œè¨¼\n",
    "# print(\"\\nâš¡ ç°¡æ˜“åŠ¹æœæ¤œè¨¼ï¼ˆLightGBM 1-foldï¼‰\")\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "# X_train_lgb_adv = X_train_final.copy()\n",
    "# for c in cat_cols_advanced:\n",
    "#     if c in X_train_lgb_adv.columns:\n",
    "#         X_train_lgb_adv[c] = X_train_lgb_adv[c].astype(\"category\")\n",
    "\n",
    "# # 1-foldæ¤œè¨¼\n",
    "# tr_idx, va_idx = next(skf_full.split(X_train_lgb_adv, y_train))\n",
    "# X_tr, X_va = X_train_lgb_adv.iloc[tr_idx], X_train_lgb_adv.iloc[va_idx]\n",
    "# y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "\n",
    "# # é«˜åº¦ç‰¹å¾´é‡ç‰ˆLightGBM\n",
    "# model_advanced = LGBMClassifier(\n",
    "#     **PHASE1_RESULTS[\"best_lgb_params\"],\n",
    "#     random_state=SEED,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=-1\n",
    "# )\n",
    "\n",
    "# model_advanced.fit(\n",
    "#     X_tr, y_tr,\n",
    "#     eval_set=[(X_va, y_va)],\n",
    "#     eval_metric=\"binary_logloss\",\n",
    "#     callbacks=[early_stopping(stopping_rounds=100, verbose=False)]\n",
    "# )\n",
    "\n",
    "# # äºˆæ¸¬ã¨è©•ä¾¡\n",
    "# pred_va_adv = model_advanced.predict_proba(X_va)[:, 1]\n",
    "# thresholds = np.linspace(0.25, 0.45, 21)\n",
    "# f1s_adv = [f1_score(y_va, (pred_va_adv >= t).astype(int)) for t in thresholds]\n",
    "# best_f1_adv = max(f1s_adv)\n",
    "# best_th_adv = thresholds[np.argmax(f1s_adv)]\n",
    "\n",
    "# print(f\"ğŸ¯ é«˜åº¦ç‰¹å¾´é‡LightGBMï¼ˆ1-foldï¼‰:\")\n",
    "# print(f\"F1ã‚¹ã‚³ã‚¢: {best_f1_adv:.6f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {best_th_adv:.4f}\")\n",
    "\n",
    "# # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒï¼ˆæ¨å®šï¼‰\n",
    "# baseline_1fold = 0.633  # æ¨å®šå€¤\n",
    "# improvement_estimate = best_f1_adv - baseline_1fold\n",
    "# print(f\"æ¨å®šæ”¹å–„: {improvement_estimate:+.6f}\")\n",
    "\n",
    "# if improvement_estimate > 0.005:\n",
    "#     print(\"âœ… é«˜åº¦ç‰¹å¾´é‡ã§å¤§å¹…æ”¹å–„ï¼æœ¬æ ¼å­¦ç¿’ã¸\")\n",
    "#     proceed_to_full_training = True\n",
    "# elif improvement_estimate > 0.002:\n",
    "#     print(\"ğŸ”„ ä¸­ç¨‹åº¦æ”¹å–„ã€‚æœ¬æ ¼å­¦ç¿’ã§ç¢ºèª\")\n",
    "#     proceed_to_full_training = True\n",
    "# else:\n",
    "#     print(\"âš ï¸ æ”¹å–„å¾®å°ã€‚ç‰¹å¾´é‡ã‚’å†æ¤œè¨\")\n",
    "#     proceed_to_full_training = False\n",
    "\n",
    "# # === 7. F1ã‚¹ã‚³ã‚¢0.66ã¸ã®äºˆæ¸¬ ===\n",
    "# current_best = PHASE1_5_RESULTS['best_ensemble_f1']\n",
    "# expected_improvement = improvement_estimate * 0.8  # ä¿å®ˆçš„è¦‹ç©ã‚‚ã‚Š\n",
    "# expected_f1 = current_best + expected_improvement\n",
    "\n",
    "# print(f\"\\nğŸ“Š F1ã‚¹ã‚³ã‚¢0.66ã¸ã®äºˆæ¸¬:\")\n",
    "# print(f\"ç¾åœ¨ãƒ™ã‚¹ãƒˆ: {current_best:.6f}\")\n",
    "# print(f\"æœŸå¾…æ”¹å–„: {expected_improvement:+.6f}\")\n",
    "# print(f\"æœŸå¾…F1: {expected_f1:.6f}\")\n",
    "# print(f\"0.66ã¾ã§: {0.66 - expected_f1:.6f}\")\n",
    "\n",
    "# if expected_f1 >= 0.66:\n",
    "#     print(\"ğŸ‰ F1ã‚¹ã‚³ã‚¢0.66é”æˆè¦‹è¾¼ã¿ï¼\")\n",
    "# elif expected_f1 >= 0.655:\n",
    "#     print(\"ğŸš€ 0.66ã«è¿‘æ¥ï¼å¾®èª¿æ•´ã§é”æˆå¯èƒ½\")\n",
    "# else:\n",
    "#     print(\"ğŸ“ˆ æ›´ãªã‚‹æ”¹å–„ãŒå¿…è¦ã€‚Phase 3ã¸\")\n",
    "\n",
    "# print(\"\\nâœ… Phase 2å®Œäº†ï¼é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# # çµæœä¿å­˜\n",
    "# PHASE2_RESULTS = {\n",
    "#     \"new_features_count\": len(new_features),\n",
    "#     \"total_features\": len(X_train_final.columns),\n",
    "#     \"advanced_f1_1fold\": best_f1_adv,\n",
    "#     \"estimated_improvement\": improvement_estimate,\n",
    "#     \"expected_ensemble_f1\": expected_f1,\n",
    "#     \"proceed_to_full\": proceed_to_full_training\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f27509",
   "metadata": {},
   "source": [
    "# 9.3 Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2471616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Phase 3: å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66é”æˆ\n",
      "ç¾åœ¨F1: 0.622669\n",
      "ç›®æ¨™F1: 0.660000\n",
      "å¿…è¦æ”¹å–„: 0.037331\n",
      "\n",
      "=== æˆ¦ç•¥1: ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š ===\n",
      "é«˜åº¦ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‹ã‚‰æœ€é©é¸æŠ...\n",
      "ç‰¹å¾´é‡é¸æŠæœ€é©åŒ–ä¸­...\n",
      "  35ç‰¹å¾´é‡: F1=0.576744\n",
      "  45ç‰¹å¾´é‡: F1=0.593301\n",
      "  55ç‰¹å¾´é‡: F1=0.580336\n",
      "âœ… æœ€é©ç‰¹å¾´é‡é¸æŠ: 45å€‹, F1=0.593301\n",
      "\n",
      "=== æˆ¦ç•¥2: é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ ===\n",
      "é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Ÿè¡Œ...\n",
      "  lgb_optimized å­¦ç¿’ä¸­...\n",
      "  lgb_dart å­¦ç¿’ä¸­...\n",
      "  lgb_conservative å­¦ç¿’ä¸­...\n",
      "  rf_ensemble å­¦ç¿’ä¸­...\n",
      "âœ… é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ:\n",
      "F1ã‚¹ã‚³ã‚¢: 0.605956\n",
      "æœ€é©é–¾å€¤: 0.3300\n",
      "\n",
      "=== æˆ¦ç•¥3: é«˜åº¦é–¾å€¤ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ€é©åŒ– ===\n",
      "ç²¾å¯†é–¾å€¤æœ€é©åŒ–: F1=0.608090 @ 0.33400\n",
      "ã‚¯ãƒ©ã‚¹é‡ã¿æ¯”: 6.834\n",
      "\n",
      "=== æˆ¦ç•¥4: é«˜åº¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° ===\n",
      "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾è±¡: 4åˆ—\n",
      "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹å¾´é‡: +3å€‹\n",
      "\n",
      "ğŸ“Š Phase 3ç·åˆçµæœ:\n",
      "ç‰¹å¾´é‡é¸æŠ: 45å€‹é¸æŠ, F1=0.593301\n",
      "é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: F1=0.605956\n",
      "ç²¾å¯†é–¾å€¤æœ€é©åŒ–: F1=0.608090\n",
      "\n",
      "Phase 3æ”¹å–„: -0.014579\n",
      "æœ€çµ‚äºˆæƒ³F1: 0.608090\n",
      "0.66ã¾ã§: 0.051910\n",
      "âš ï¸ æ›´ãªã‚‹æ”¹å–„ãŒå¿…è¦\n",
      "\n",
      "ğŸš€ Phase 4ææ¡ˆ:\n",
      "1. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆçµŒæ¸ˆæŒ‡æ¨™ç­‰ï¼‰\n",
      "2. æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¿½åŠ \n",
      "3. æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
      "\n",
      "âœ… Phase 3å®Œäº†ï¼å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "# # Phase 3: å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66é”æˆ\n",
    "\n",
    "# print(\"ğŸ¯ Phase 3: å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66é”æˆ\")\n",
    "# print(f\"ç¾åœ¨F1: {PHASE2_RESULTS.get('expected_ensemble_f1', 0.637):.6f}\")\n",
    "# print(f\"ç›®æ¨™F1: 0.660000\")\n",
    "# print(f\"å¿…è¦æ”¹å–„: {0.66 - PHASE2_RESULTS.get('expected_ensemble_f1', 0.637):.6f}\")\n",
    "\n",
    "# # === æˆ¦ç•¥1: ç‰¹å¾´é‡é¸æŠã¨æ¬¡å…ƒå‰Šæ¸› ===\n",
    "# print(\"\\n=== æˆ¦ç•¥1: ç‰¹å¾´é‡é¸æŠã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š ===\")\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# def optimize_feature_selection(X, y, n_features_range=[30, 40, 50]):\n",
    "#     \"\"\"æœ€é©ãªç‰¹å¾´é‡æ•°ã‚’æ¢ç´¢\"\"\"\n",
    "#     best_score = 0\n",
    "#     best_features = None\n",
    "#     best_n = None\n",
    "    \n",
    "#     print(\"ç‰¹å¾´é‡é¸æŠæœ€é©åŒ–ä¸­...\")\n",
    "    \n",
    "#     for n_features in n_features_range:\n",
    "#         # Mutual Information ã«ã‚ˆã‚‹é¸æŠ\n",
    "#         selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "#         X_selected = selector.fit_transform(X, y)\n",
    "#         selected_features = X.columns[selector.get_support()].tolist()\n",
    "        \n",
    "#         # ç°¡æ˜“æ¤œè¨¼ (1-fold)\n",
    "#         tr_idx, va_idx = next(skf_full.split(X, y))\n",
    "#         X_tr_sel = X_selected[tr_idx]\n",
    "#         X_va_sel = X_selected[va_idx]\n",
    "#         y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "        \n",
    "#         # LightGBMæ¤œè¨¼\n",
    "#         model = LGBMClassifier(\n",
    "#             n_estimators=500,\n",
    "#             learning_rate=0.05,\n",
    "#             random_state=SEED,\n",
    "#             verbose=-1\n",
    "#         )\n",
    "#         model.fit(X_tr_sel, y_tr)\n",
    "#         pred_va = model.predict_proba(X_va_sel)[:, 1]\n",
    "        \n",
    "#         # F1æœ€é©åŒ–\n",
    "#         thresholds = np.linspace(0.25, 0.45, 21)\n",
    "#         f1s = [f1_score(y_va, (pred_va >= t).astype(int)) for t in thresholds]\n",
    "#         score = max(f1s)\n",
    "        \n",
    "#         print(f\"  {n_features}ç‰¹å¾´é‡: F1={score:.6f}\")\n",
    "        \n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_features = selected_features\n",
    "#             best_n = n_features\n",
    "    \n",
    "#     return best_features, best_n, best_score\n",
    "\n",
    "# # é«˜åº¦ç‰¹å¾´é‡ã‹ã‚‰æœ€é©é¸æŠ\n",
    "# print(\"é«˜åº¦ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã‹ã‚‰æœ€é©é¸æŠ...\")\n",
    "\n",
    "# # æ•°å€¤å¤‰æ›ï¼ˆã‚«ãƒ†ã‚´ãƒªã¯LabelEncodingï¼‰\n",
    "# X_numeric = X_train_final.copy()\n",
    "# for col in cat_cols_advanced:\n",
    "#     if col in X_numeric.columns:\n",
    "#         X_numeric[col] = pd.Categorical(X_numeric[col]).codes\n",
    "\n",
    "# best_features, best_n_features, selection_score = optimize_feature_selection(\n",
    "#     X_numeric, y_train, n_features_range=[35, 45, 55]\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… æœ€é©ç‰¹å¾´é‡é¸æŠ: {best_n_features}å€‹, F1={selection_score:.6f}\")\n",
    "\n",
    "# # === æˆ¦ç•¥2: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ã®å¼·åŒ– ===\n",
    "# print(\"\\n=== æˆ¦ç•¥2: é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ ===\")\n",
    "\n",
    "# def create_diverse_models(X, y, selected_features):\n",
    "#     \"\"\"å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¼·åŒ–\"\"\"\n",
    "    \n",
    "#     # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã§ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "#     X_selected = X[selected_features].copy()\n",
    "    \n",
    "#     # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†ï¼ˆLightGBMç”¨ï¼‰\n",
    "#     X_selected_lgb = X_selected.copy()\n",
    "#     for col in X_selected_lgb.columns:\n",
    "#         if col in cat_cols_advanced:\n",
    "#             X_selected_lgb[col] = X_selected_lgb[col].astype(\"category\")\n",
    "    \n",
    "#     # æ•°å€¤å¤‰æ›ï¼ˆRandomForestç­‰ç”¨ï¼‰\n",
    "#     X_selected_numeric = X_selected.copy()\n",
    "#     for col in X_selected_numeric.columns:\n",
    "#         if col in cat_cols_advanced:\n",
    "#             X_selected_numeric[col] = pd.Categorical(X_selected_numeric[col]).codes\n",
    "    \n",
    "#     models_config = {\n",
    "#         \"lgb_optimized\": {\n",
    "#             \"model\": LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"]),\n",
    "#             \"weight\": 0.4\n",
    "#         },\n",
    "#         \"lgb_dart\": {\n",
    "#             \"model\": LGBMClassifier(\n",
    "#                 boosting_type=\"dart\",\n",
    "#                 learning_rate=0.03,\n",
    "#                 n_estimators=1000,\n",
    "#                 drop_rate=0.1,\n",
    "#                 random_state=SEED,\n",
    "#                 verbose=-1\n",
    "#             ),\n",
    "#             \"weight\": 0.2\n",
    "#         },\n",
    "#         \"lgb_conservative\": {\n",
    "#             \"model\": LGBMClassifier(\n",
    "#                 learning_rate=0.01,\n",
    "#                 n_estimators=2000,\n",
    "#                 num_leaves=30,\n",
    "#                 reg_alpha=10,\n",
    "#                 reg_lambda=10,\n",
    "#                 random_state=SEED,\n",
    "#                 verbose=-1\n",
    "#             ),\n",
    "#             \"weight\": 0.2\n",
    "#         },\n",
    "#         \"rf_ensemble\": {\n",
    "#             \"model\": RandomForestClassifier(\n",
    "#                 n_estimators=500,\n",
    "#                 max_depth=8,\n",
    "#                 min_samples_split=20,\n",
    "#                 class_weight=\"balanced\",\n",
    "#                 random_state=SEED,\n",
    "#                 n_jobs=-1\n",
    "#             ),\n",
    "#             \"weight\": 0.2\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     # 5-fold ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "#     oof_ensemble = np.zeros(len(X_selected))\n",
    "#     model_oofs = {}\n",
    "    \n",
    "#     for model_name, config in models_config.items():\n",
    "#         print(f\"  {model_name} å­¦ç¿’ä¸­...\")\n",
    "#         model_oof = np.zeros(len(X_selected))\n",
    "        \n",
    "#         for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_selected, y)):\n",
    "#             # ãƒ‡ãƒ¼ã‚¿é¸æŠï¼ˆãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦ï¼‰\n",
    "#             if \"lgb\" in model_name:\n",
    "#                 X_tr, X_va = X_selected_lgb.iloc[tr_idx], X_selected_lgb.iloc[va_idx]\n",
    "#             else:\n",
    "#                 X_tr, X_va = X_selected_numeric.iloc[tr_idx], X_selected_numeric.iloc[va_idx]\n",
    "            \n",
    "#             y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "            \n",
    "#             model = config[\"model\"]\n",
    "#             if hasattr(model, 'fit'):\n",
    "#                 if \"lgb\" in model_name:\n",
    "#                     model.fit(\n",
    "#                         X_tr, y_tr,\n",
    "#                         eval_set=[(X_va, y_va)],\n",
    "#                         callbacks=[early_stopping(100, verbose=False)]\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     model.fit(X_tr, y_tr)\n",
    "                \n",
    "#                 model_oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "        \n",
    "#         model_oofs[model_name] = model_oof\n",
    "#         oof_ensemble += model_oof * config[\"weight\"]\n",
    "    \n",
    "#     return oof_ensemble, model_oofs\n",
    "\n",
    "# # é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè¡Œ\n",
    "# print(\"é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Ÿè¡Œ...\")\n",
    "# oof_advanced_ensemble, individual_oofs = create_diverse_models(\n",
    "#     X_train_final, y_train, best_features\n",
    "# )\n",
    "\n",
    "# # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡\n",
    "# thresholds = np.linspace(0.20, 0.50, 31)\n",
    "# f1s_ensemble = [f1_score(y_train, (oof_advanced_ensemble >= t).astype(int)) for t in thresholds]\n",
    "# best_f1_ensemble = max(f1s_ensemble)\n",
    "# best_th_ensemble = thresholds[np.argmax(f1s_ensemble)]\n",
    "\n",
    "# print(f\"âœ… é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ:\")\n",
    "# print(f\"F1ã‚¹ã‚³ã‚¢: {best_f1_ensemble:.6f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {best_th_ensemble:.4f}\")\n",
    "\n",
    "# # === æˆ¦ç•¥3: é–¾å€¤ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®æœ€é©åŒ– ===\n",
    "# print(\"\\n=== æˆ¦ç•¥3: é«˜åº¦é–¾å€¤ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æœ€é©åŒ– ===\")\n",
    "\n",
    "# # ã‚ˆã‚Šç´°ã‹ã„é–¾å€¤æ¢ç´¢\n",
    "# fine_thresholds = np.linspace(best_th_ensemble - 0.05, best_th_ensemble + 0.05, 51)\n",
    "# fine_f1s = [f1_score(y_train, (oof_advanced_ensemble >= t).astype(int)) for t in fine_thresholds]\n",
    "# ultra_fine_f1 = max(fine_f1s)\n",
    "# ultra_fine_th = fine_thresholds[np.argmax(fine_f1s)]\n",
    "\n",
    "# print(f\"ç²¾å¯†é–¾å€¤æœ€é©åŒ–: F1={ultra_fine_f1:.6f} @ {ultra_fine_th:.5f}\")\n",
    "\n",
    "# # ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´ã®åŠ¹æœç¢ºèª\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# weight_ratio = class_weights[1] / class_weights[0]\n",
    "\n",
    "# print(f\"ã‚¯ãƒ©ã‚¹é‡ã¿æ¯”: {weight_ratio:.3f}\")\n",
    "\n",
    "# # === æˆ¦ç•¥4: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¼·åŒ– ===\n",
    "# print(\"\\n=== æˆ¦ç•¥4: é«˜åº¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° ===\")\n",
    "\n",
    "# def advanced_target_encoding(X, y, categorical_cols, cv_folds=5):\n",
    "#     \"\"\"é«˜åº¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\"\"\"\n",
    "#     X_encoded = X.copy()\n",
    "    \n",
    "#     for col in categorical_cols:\n",
    "#         if col in X.columns:\n",
    "#             # CV-based Target Encoding\n",
    "#             encoded_values = np.zeros(len(X))\n",
    "            \n",
    "#             for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X, y)):\n",
    "#                 # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®å¹³å‡è¨ˆç®—\n",
    "#                 col_means = pd.Series(y[tr_idx]).groupby(X[col].iloc[tr_idx]).mean()\n",
    "#                 global_mean = y[tr_idx].mean()\n",
    "                \n",
    "#                 # Smoothingé©ç”¨\n",
    "#                 smoothing = 10\n",
    "#                 col_counts = X[col].iloc[tr_idx].value_counts()\n",
    "#                 smoothed_means = (col_means * col_counts + global_mean * smoothing) / (col_counts + smoothing)\n",
    "                \n",
    "#                 # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨\n",
    "#                 encoded_values[va_idx] = X[col].iloc[va_idx].map(smoothed_means).fillna(global_mean)\n",
    "            \n",
    "#             X_encoded[f\"{col}_target_encoded\"] = encoded_values\n",
    "    \n",
    "#     return X_encoded\n",
    "\n",
    "# # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é©ç”¨\n",
    "# high_cardinality_cols = [col for col in cat_cols_advanced \n",
    "#                         if col in X_train_final.columns and \n",
    "#                         X_train_final[col].nunique() > 10]\n",
    "\n",
    "# if high_cardinality_cols:\n",
    "#     print(f\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾è±¡: {len(high_cardinality_cols)}åˆ—\")\n",
    "#     X_train_te = advanced_target_encoding(\n",
    "#         X_train_final[best_features], y_train, \n",
    "#         [col for col in high_cardinality_cols if col in best_features]\n",
    "#     )\n",
    "    \n",
    "#     # ç°¡æ˜“åŠ¹æœç¢ºèª\n",
    "#     if len(X_train_te.columns) > len(best_features):\n",
    "#         print(f\"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç‰¹å¾´é‡: +{len(X_train_te.columns) - len(best_features)}å€‹\")\n",
    "\n",
    "# # === ç·åˆè©•ä¾¡ã¨F1ã‚¹ã‚³ã‚¢0.66é”æˆåˆ¤å®š ===\n",
    "# print(f\"\\nğŸ“Š Phase 3ç·åˆçµæœ:\")\n",
    "# print(f\"ç‰¹å¾´é‡é¸æŠ: {best_n_features}å€‹é¸æŠ, F1={selection_score:.6f}\")\n",
    "# print(f\"é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: F1={best_f1_ensemble:.6f}\")\n",
    "# print(f\"ç²¾å¯†é–¾å€¤æœ€é©åŒ–: F1={ultra_fine_f1:.6f}\")\n",
    "\n",
    "# final_f1_estimate = ultra_fine_f1\n",
    "# improvement_from_phase2 = final_f1_estimate - PHASE2_RESULTS.get('expected_ensemble_f1', 0.637)\n",
    "\n",
    "# print(f\"\\nPhase 3æ”¹å–„: {improvement_from_phase2:+.6f}\")\n",
    "# print(f\"æœ€çµ‚äºˆæƒ³F1: {final_f1_estimate:.6f}\")\n",
    "# print(f\"0.66ã¾ã§: {0.66 - final_f1_estimate:.6f}\")\n",
    "\n",
    "# if final_f1_estimate >= 0.66:\n",
    "#     print(\"ğŸ‰ F1ã‚¹ã‚³ã‚¢0.66é”æˆï¼\")\n",
    "#     achievement_status = \"ACHIEVED\"\n",
    "# elif final_f1_estimate >= 0.655:\n",
    "#     print(\"ğŸ”¥ 0.66ã«æ¥µã‚ã¦è¿‘ã„ï¼æœ€çµ‚èª¿æ•´ã§é”æˆå¯èƒ½\")\n",
    "#     achievement_status = \"VERY_CLOSE\"\n",
    "# elif final_f1_estimate >= 0.650:\n",
    "#     print(\"ğŸ“ˆ 0.65çªç ´ï¼0.66ã¾ã§ã‚ã¨ä¸€æ­©\")\n",
    "#     achievement_status = \"CLOSE\"\n",
    "# else:\n",
    "#     print(\"âš ï¸ æ›´ãªã‚‹æ”¹å–„ãŒå¿…è¦\")\n",
    "#     achievement_status = \"NEED_MORE\"\n",
    "\n",
    "# # === Phase 4ã¸ã®ææ¡ˆ ===\n",
    "# if achievement_status != \"ACHIEVED\":\n",
    "#     print(f\"\\nğŸš€ Phase 4ææ¡ˆ:\")\n",
    "#     if achievement_status == \"VERY_CLOSE\":\n",
    "#         print(\"1. è¶…ç²¾å¯†ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\")\n",
    "#         print(\"2. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿å¾®èª¿æ•´\")\n",
    "#         print(\"3. é–¾å€¤ã®0.001å˜ä½æœ€é©åŒ–\")\n",
    "#     else:\n",
    "#         print(\"1. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆçµŒæ¸ˆæŒ‡æ¨™ç­‰ï¼‰\")\n",
    "#         print(\"2. æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¿½åŠ \")\n",
    "#         print(\"3. æ™‚ç³»åˆ—ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\")\n",
    "\n",
    "# print(\"\\nâœ… Phase 3å®Œäº†ï¼å¤šè§’çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# # çµæœä¿å­˜\n",
    "# PHASE3_RESULTS = {\n",
    "#     \"best_features\": best_features,\n",
    "#     \"best_n_features\": best_n_features,\n",
    "#     \"selection_f1\": selection_score,\n",
    "#     \"ensemble_f1\": best_f1_ensemble,\n",
    "#     \"final_f1_estimate\": final_f1_estimate,\n",
    "#     \"improvement\": improvement_from_phase2,\n",
    "#     \"achievement_status\": achievement_status,\n",
    "#     \"optimal_threshold\": ultra_fine_th\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9537e7",
   "metadata": {},
   "source": [
    "# 9.4 Phase 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e7267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Phase 4æ”¹è‰¯ç‰ˆ: æœ€é«˜LB 0.6198ã‚’è¶…ãˆã‚‹æœ€çµ‚èª¿æ•´\n",
      "é©æ–°çš„æ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒç¢ºèªã•ã‚ŒãŸãŸã‚ã€å¾®èª¿æ•´ã§æ›´ãªã‚‹å‘ä¸Šã‚’ç›®æŒ‡ã™\n",
      "\n",
      "=== åˆ†æçµæœã®æ´»ç”¨ ===\n",
      "âœ… Phase 4é©æ–°çš„æ‰‹æ³•: LB 0.6198 (æœ€é«˜)\n",
      "âŒ ä¿å®ˆçš„æ‰‹æ³•: LB 0.6156 (Phase 4ã‚ˆã‚Šä½ã„)\n",
      "â†’ çµè«–: é©æ–°çš„æ‰‹æ³•ã‚’åŸºç›¤ã«å¾®èª¿æ•´ãŒæœ€é©\n",
      "\n",
      "=== Phase 4æ”¹è‰¯ç‰ˆã®è¨­è¨ˆæ–¹é‡ ===\n",
      "1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: ç¶­æŒï¼ˆåŠ¹æœã‚ã‚Šï¼‰\n",
      "2. ä¸å‡è¡¡å­¦ç¿’: å¾®èª¿æ•´ï¼ˆæ­£ä¾‹ç‡ã‚’23-25%ã«èª¿æ•´ï¼‰\n",
      "3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: é‡ã¿æœ€é©åŒ–\n",
      "4. é–¾å€¤: ã‚ˆã‚Šç²¾å¯†ãªæœ€é©åŒ–\n",
      "\n",
      "=== æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ===\n",
      "æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆ...\n",
      "æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«: 5077å€‹ (æ­£ä¾‹:98, è² ä¾‹:4979)\n",
      "\n",
      "=== æ”¹è‰¯ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ ===\n",
      "  lgb_best å­¦ç¿’ä¸­...\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "  lgb_conservative å­¦ç¿’ä¸­...\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "  lgb_aggressive å­¦ç¿’ä¸­...\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12174ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 12176ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "\n",
      "=== ç²¾å¯†é–¾å€¤æœ€é©åŒ– ===\n",
      "âœ… æ”¹è‰¯ç‰ˆæœ€é©åŒ–çµæœ:\n",
      "F1ã‚¹ã‚³ã‚¢: 0.614786\n",
      "æœ€é©é–¾å€¤: 0.57600\n",
      "\n",
      "=== æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆäºˆæ¸¬ ===\n",
      "æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 15219ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.240)\n",
      "æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆäºˆæ¸¬æ­£ä¾‹ç‡: 0.141\n",
      "\n",
      "=== æ”¹è‰¯ç‰ˆæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
      "âœ… æ”¹è‰¯ç‰ˆæå‡º: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\submission_A_v8_refined.csv\n",
      "âœ… ãƒ­ã‚°: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\run_A2_v8_refined.txt\n",
      "\n",
      "ğŸš€ Phase 4æ”¹è‰¯ç‰ˆå®Œæˆï¼\n",
      "ğŸ“Š æ”¹è‰¯ç‰ˆF1: 0.614786\n",
      "ğŸ¯ ç²¾å¯†é–¾å€¤: 0.57600\n",
      "ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v8_refined.csv\n",
      "ğŸ† ç›®æ¨™: LB 0.622+ (Phase 4ã®0.6198ã‚’è¶…ãˆã‚‹)\n"
     ]
    }
   ],
   "source": [
    "# # Phase 4æ”¹è‰¯ç‰ˆ: LB 0.62+ã‚’ç›®æŒ‡ã™æœ€çµ‚èª¿æ•´\n",
    "\n",
    "# print(\"ğŸ¯ Phase 4æ”¹è‰¯ç‰ˆ: æœ€é«˜LB 0.6198ã‚’è¶…ãˆã‚‹æœ€çµ‚èª¿æ•´\")\n",
    "# print(\"é©æ–°çš„æ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒç¢ºèªã•ã‚ŒãŸãŸã‚ã€å¾®èª¿æ•´ã§æ›´ãªã‚‹å‘ä¸Šã‚’ç›®æŒ‡ã™\")\n",
    "\n",
    "# # === åˆ†æçµæœã®æ´»ç”¨ ===\n",
    "# print(\"\\n=== åˆ†æçµæœã®æ´»ç”¨ ===\")\n",
    "# print(\"âœ… Phase 4é©æ–°çš„æ‰‹æ³•: LB 0.6198 (æœ€é«˜)\")\n",
    "# print(\"âŒ ä¿å®ˆçš„æ‰‹æ³•: LB 0.6156 (Phase 4ã‚ˆã‚Šä½ã„)\")\n",
    "# print(\"â†’ çµè«–: é©æ–°çš„æ‰‹æ³•ã‚’åŸºç›¤ã«å¾®èª¿æ•´ãŒæœ€é©\")\n",
    "\n",
    "# # === Phase 4æ”¹è‰¯ç‰ˆã®è¨­è¨ˆ ===\n",
    "# print(\"\\n=== Phase 4æ”¹è‰¯ç‰ˆã®è¨­è¨ˆæ–¹é‡ ===\")\n",
    "# print(\"1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: ç¶­æŒï¼ˆåŠ¹æœã‚ã‚Šï¼‰\")\n",
    "# print(\"2. ä¸å‡è¡¡å­¦ç¿’: å¾®èª¿æ•´ï¼ˆæ­£ä¾‹ç‡ã‚’23-25%ã«èª¿æ•´ï¼‰\")\n",
    "# print(\"3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: é‡ã¿æœ€é©åŒ–\")\n",
    "# print(\"4. é–¾å€¤: ã‚ˆã‚Šç²¾å¯†ãªæœ€é©åŒ–\")\n",
    "\n",
    "# # === å¾®èª¿æ•´ã•ã‚ŒãŸä¸å‡è¡¡å­¦ç¿’ ===\n",
    "# def refined_imbalance_handling(X, y, target_ratio=0.24):\n",
    "#     \"\"\"Phase 4ã®å¾®èª¿æ•´ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\"\"\"\n",
    "#     minority_indices = np.where(y == 1)[0]\n",
    "#     majority_indices = np.where(y == 0)[0]\n",
    "    \n",
    "#     # Phase 4ã‚ˆã‚Šæ§ãˆã‚ã ãŒåŠ¹æœçš„ãªæ¯”ç‡\n",
    "#     target_minority_size = int(len(majority_indices) * target_ratio / (1 - target_ratio))\n",
    "#     additional_samples = target_minority_size - len(minority_indices)\n",
    "    \n",
    "#     if additional_samples > 0:\n",
    "#         resampled_indices = resample(\n",
    "#             minority_indices, \n",
    "#             n_samples=additional_samples, \n",
    "#             random_state=SEED\n",
    "#         )\n",
    "        \n",
    "#         all_indices = np.concatenate([\n",
    "#             majority_indices, \n",
    "#             minority_indices, \n",
    "#             resampled_indices\n",
    "#         ])\n",
    "        \n",
    "#         X_resampled = X.iloc[all_indices] if hasattr(X, 'iloc') else X[all_indices]\n",
    "#         y_resampled = y[all_indices]\n",
    "#     else:\n",
    "#         X_resampled, y_resampled = X, y\n",
    "    \n",
    "#     print(f\"æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: {len(X_resampled)}ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: {y_resampled.mean():.3f})\")\n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # === æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ===\n",
    "# print(\"\\n=== æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ===\")\n",
    "\n",
    "# def refined_pseudo_label_learning(X_train, y_train, X_test, confidence_threshold=0.9):\n",
    "#     \"\"\"æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’\"\"\"\n",
    "#     print(\"æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆ...\")\n",
    "    \n",
    "#     # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†\n",
    "#     X_train_processed = X_train.copy()\n",
    "#     X_test_processed = X_test.copy()\n",
    "    \n",
    "#     for col in X_train_processed.columns:\n",
    "#         if X_train_processed[col].dtype == 'object':\n",
    "#             X_train_processed[col] = X_train_processed[col].astype(\"category\")\n",
    "#             X_test_processed[col] = X_test_processed[col].astype(\"category\")\n",
    "    \n",
    "#     # ã‚ˆã‚Šä¿å®ˆçš„ãªãƒ¢ãƒ‡ãƒ«ã§æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆ\n",
    "#     params_conservative = PHASE1_RESULTS[\"best_lgb_params\"].copy()\n",
    "#     params_conservative.update({\n",
    "#         'reg_alpha': 15,  # ã‚ˆã‚Šä¿å®ˆçš„\n",
    "#         'reg_lambda': 15\n",
    "#     })\n",
    "    \n",
    "#     model_pseudo = LGBMClassifier(**params_conservative)\n",
    "#     model_pseudo.fit(X_train_processed, y_train)\n",
    "#     test_probs = model_pseudo.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "#     # ã‚ˆã‚Šå³æ ¼ãªåŸºæº–ã§é«˜ä¿¡é ¼åº¦ã‚µãƒ³ãƒ—ãƒ«é¸æŠ\n",
    "#     high_conf_positive = test_probs >= confidence_threshold\n",
    "#     high_conf_negative = test_probs <= (1 - confidence_threshold)\n",
    "    \n",
    "#     high_conf_indices = high_conf_positive | high_conf_negative\n",
    "#     pseudo_labels = (test_probs >= 0.5).astype(int)\n",
    "    \n",
    "#     if high_conf_indices.sum() > 0:\n",
    "#         X_pseudo = X_test[high_conf_indices]\n",
    "#         y_pseudo = pseudo_labels[high_conf_indices]\n",
    "        \n",
    "#         print(f\"æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«: {len(X_pseudo)}å€‹ (æ­£ä¾‹:{y_pseudo.sum()}, è² ä¾‹:{(1-y_pseudo).sum()})\")\n",
    "        \n",
    "#         X_augmented = pd.concat([X_train, X_pseudo], ignore_index=True)\n",
    "#         y_augmented = np.concatenate([y_train, y_pseudo])\n",
    "        \n",
    "#         return X_augmented, y_augmented\n",
    "#     else:\n",
    "#         return X_train, y_train\n",
    "\n",
    "# # æ”¹è‰¯ç‰ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’å®Ÿè¡Œ\n",
    "# X_refined_aug, y_refined_aug = refined_pseudo_label_learning(\n",
    "#     X_train_final[PHASE3_RESULTS[\"best_features\"]], \n",
    "#     y_train, \n",
    "#     X_test_final[PHASE3_RESULTS[\"best_features\"]], \n",
    "#     confidence_threshold=0.92  # ã‚ˆã‚Šå³æ ¼\n",
    "# )\n",
    "\n",
    "# # === æ”¹è‰¯ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ===\n",
    "# print(\"\\n=== æ”¹è‰¯ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ ===\")\n",
    "\n",
    "# refined_models = {\n",
    "#     'lgb_best': {\n",
    "#         'model': LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"]),\n",
    "#         'weight': 0.45  # å¾®èª¿æ•´\n",
    "#     },\n",
    "#     'lgb_conservative': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             n_estimators=2500,\n",
    "#             learning_rate=0.015,\n",
    "#             num_leaves=40,\n",
    "#             reg_alpha=12,\n",
    "#             reg_lambda=12,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.30\n",
    "#     },\n",
    "#     'lgb_aggressive': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             n_estimators=800,\n",
    "#             learning_rate=0.07,\n",
    "#             num_leaves=80,\n",
    "#             min_child_samples=8,\n",
    "#             subsample=0.85,\n",
    "#             colsample_bytree=0.85,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED+1,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.25\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "# X_refined = X_refined_aug[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# X_refined_numeric = X_refined.copy()\n",
    "# for col in X_refined_numeric.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_refined_numeric[col] = pd.Categorical(X_refined_numeric[col]).codes\n",
    "\n",
    "# # æ”¹è‰¯ç‰ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’\n",
    "# oof_refined = np.zeros(len(X_refined))\n",
    "\n",
    "# for model_name, config in refined_models.items():\n",
    "#     print(f\"  {model_name} å­¦ç¿’ä¸­...\")\n",
    "#     model_oof = np.zeros(len(X_refined))\n",
    "    \n",
    "#     for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_refined, y_refined_aug)):\n",
    "#         X_tr, X_va = X_refined_numeric.iloc[tr_idx], X_refined_numeric.iloc[va_idx]\n",
    "#         y_tr, y_va = y_refined_aug[tr_idx], y_refined_aug[va_idx]\n",
    "        \n",
    "#         # æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "#         X_tr_res, y_tr_res = refined_imbalance_handling(X_tr, y_tr, target_ratio=0.24)\n",
    "        \n",
    "#         model = config['model']\n",
    "#         model.fit(\n",
    "#             X_tr_res, y_tr_res,\n",
    "#             eval_set=[(X_va, y_va)],\n",
    "#             callbacks=[early_stopping(100, verbose=False)]\n",
    "#         )\n",
    "        \n",
    "#         model_oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "    \n",
    "#     oof_refined += model_oof * config['weight']\n",
    "\n",
    "# # === ç²¾å¯†é–¾å€¤æœ€é©åŒ– ===\n",
    "# print(\"\\n=== ç²¾å¯†é–¾å€¤æœ€é©åŒ– ===\")\n",
    "\n",
    "# # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ã®ã¿ã§F1è©•ä¾¡\n",
    "# oof_original_part = oof_refined[:len(y_train)]\n",
    "\n",
    "# ultra_fine_thresholds = np.linspace(0.45, 0.65, 101)  # 0.002åˆ»ã¿\n",
    "# f1s_refined = [f1_score(y_train, (oof_original_part >= t).astype(int)) for t in ultra_fine_thresholds]\n",
    "# refined_f1 = max(f1s_refined)\n",
    "# refined_threshold = ultra_fine_thresholds[np.argmax(f1s_refined)]\n",
    "\n",
    "# print(f\"âœ… æ”¹è‰¯ç‰ˆæœ€é©åŒ–çµæœ:\")\n",
    "# print(f\"F1ã‚¹ã‚³ã‚¢: {refined_f1:.6f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {refined_threshold:.5f}\")\n",
    "\n",
    "# # === æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆäºˆæ¸¬ ===\n",
    "# print(\"\\n=== æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆäºˆæ¸¬ ===\")\n",
    "\n",
    "# X_test_refined = X_test_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# for col in X_test_refined.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_test_refined[col] = pd.Categorical(X_test_refined[col]).codes\n",
    "\n",
    "# # æ”¹è‰¯ç‰ˆãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å…¨è¨“ç·´\n",
    "# X_full_refined, y_full_refined = refined_imbalance_handling(\n",
    "#     X_refined_numeric, y_refined_aug, target_ratio=0.24\n",
    "# )\n",
    "\n",
    "# test_prob_refined = np.zeros(len(X_test_refined))\n",
    "\n",
    "# for model_name, config in refined_models.items():\n",
    "#     model = config['model']\n",
    "#     model.fit(X_full_refined, y_full_refined)\n",
    "#     test_prob_refined += model.predict_proba(X_test_refined)[:, 1] * config['weight']\n",
    "\n",
    "# test_pred_refined = (test_prob_refined >= refined_threshold).astype(int)\n",
    "# test_refined_rate = test_pred_refined.mean()\n",
    "\n",
    "# print(f\"æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆäºˆæ¸¬æ­£ä¾‹ç‡: {test_refined_rate:.3f}\")\n",
    "\n",
    "# # === æ”¹è‰¯ç‰ˆæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
    "# print(\"\\n=== æ”¹è‰¯ç‰ˆæå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\")\n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# OUT_DIR = r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\"\n",
    "\n",
    "# def get_next_version(out_dir):\n",
    "#     existing_files = list(Path(out_dir).glob(\"submission_A_v*.csv\"))\n",
    "#     if not existing_files:\n",
    "#         return 1\n",
    "#     versions = []\n",
    "#     for f in existing_files:\n",
    "#         try:\n",
    "#             v = int(f.stem.split('_v')[1].split('_')[0])\n",
    "#             versions.append(v)\n",
    "#         except:\n",
    "#             pass\n",
    "#     return max(versions, default=0) + 1\n",
    "\n",
    "# version = get_next_version(OUT_DIR)\n",
    "# sub_name = f\"submission_A_v{version}_refined.csv\"\n",
    "# log_name = f\"run_A2_v{version}_refined.txt\"\n",
    "\n",
    "# # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "# submit_df = pd.DataFrame({\n",
    "#     ID_COL: test[ID_COL].values, \n",
    "#     \"pred\": test_pred_refined\n",
    "# })\n",
    "\n",
    "# submit_df.to_csv(os.path.join(OUT_DIR, sub_name), header=False, index=False)\n",
    "\n",
    "# # ãƒ­ã‚°ä½œæˆ\n",
    "# log_content = f\"\"\"# Phase 4 Refined - Version {version}\n",
    "\n",
    "# ## ğŸ¯ Phase 4æ”¹è‰¯ç‰ˆ: LB 0.62+ã‚’ç›®æŒ‡ã™æœ€çµ‚èª¿æ•´\n",
    "\n",
    "# ### æ”¹è‰¯ç‚¹\n",
    "# 1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: ã‚ˆã‚Šå³æ ¼ãªåŸºæº– (ä¿¡é ¼åº¦92%)\n",
    "# 2. ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: é©åº¦ãªèª¿æ•´ (æ­£ä¾‹ç‡24%)\n",
    "# 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: é‡ã¿å¾®èª¿æ•´\n",
    "# 4. é–¾å€¤: ç²¾å¯†æœ€é©åŒ– ({refined_threshold:.5f})\n",
    "\n",
    "# ### æ€§èƒ½\n",
    "# - Refined F1: {refined_f1:.6f}\n",
    "# - Threshold: {refined_threshold:.5f}\n",
    "# - Test Positive Rate: {test_refined_rate:.3f}\n",
    "# - Target LB: 0.62+ (Phase 4ã®0.6198ã‚’è¶…ãˆã‚‹)\n",
    "\n",
    "# ### æ¯”è¼ƒ\n",
    "# - Phase 4é©æ–°çš„: LB 0.6198\n",
    "# - ä¿å®ˆçš„: LB 0.6156\n",
    "# - æ”¹è‰¯ç‰ˆ: æœŸå¾…LB 0.622+\n",
    "\n",
    "# ### æˆ¦ç•¥\n",
    "# Phase 4ã®é©æ–°çš„æ‰‹æ³•ã®æœ‰åŠ¹æ€§ãŒç¢ºèªã•ã‚ŒãŸãŸã‚ã€\n",
    "# éåº¦ã«ä¿å®ˆçš„ã«ãªã‚‰ãšã€å¾®èª¿æ•´ã«ã‚ˆã‚‹æ”¹å–„ã‚’è¿½æ±‚ã€‚\n",
    "\n",
    "# version: {version}\n",
    "# approach: phase4_refined\n",
    "# threshold: {refined_threshold:.5f}\n",
    "# pseudo_labels: enhanced\n",
    "# resampling: moderate_24percent\n",
    "# ensemble: weight_optimized\n",
    "# target: LB_0.622_plus\n",
    "# \"\"\"\n",
    "\n",
    "# with open(os.path.join(OUT_DIR, log_name), \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(log_content)\n",
    "\n",
    "# print(f\"âœ… æ”¹è‰¯ç‰ˆæå‡º: {os.path.join(OUT_DIR, sub_name)}\")\n",
    "# print(f\"âœ… ãƒ­ã‚°: {os.path.join(OUT_DIR, log_name)}\")\n",
    "\n",
    "# print(f\"\\nğŸš€ Phase 4æ”¹è‰¯ç‰ˆå®Œæˆï¼\")\n",
    "# print(f\"ğŸ“Š æ”¹è‰¯ç‰ˆF1: {refined_f1:.6f}\")\n",
    "# print(f\"ğŸ¯ ç²¾å¯†é–¾å€¤: {refined_threshold:.5f}\")\n",
    "# print(f\"ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v{version}_refined.csv\")\n",
    "# print(f\"ğŸ† ç›®æ¨™: LB 0.622+ (Phase 4ã®0.6198ã‚’è¶…ãˆã‚‹)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ce34f",
   "metadata": {},
   "source": [
    "# 9.5 æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9be79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ F1ã‚¹ã‚³ã‚¢0.66é”æˆï¼æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæº–å‚™\n",
      "é”æˆF1ã‚¹ã‚³ã‚¢: 0.699704\n",
      "ç›®æ¨™0.66ã‚’ 39.7ãƒã‚¤ãƒ³ãƒˆä¸Šå›ã‚‹å¤§æˆåŠŸï¼\n",
      "\n",
      "=== æå‡ºç”¨å¤‰æ•°ã®è¨­å®š ===\n",
      "OOFäºˆæ¸¬è¨­å®šå®Œäº†: 13578ä»¶\n",
      "ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆä¸­...\n",
      "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 7552\n",
      "ç‰¹å¾´é‡æ•°: 45\n",
      "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 7552\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 9223ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "  lgb_bestã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬...\n",
      "  lgb_conservativeã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬...\n",
      "  lgb_aggressiveã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬...\n",
      "  lgb_dartã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬...\n",
      "ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†: 7552ä»¶\n",
      "æå‡ºé–¾å€¤è¨­å®š: 0.54600\n",
      "\n",
      "âœ… æå‡ºæº–å‚™å®Œäº†ï¼\n",
      "ğŸ“Š æœ€çµ‚æ€§èƒ½:\n",
      "   F1ã‚¹ã‚³ã‚¢: 0.699704\n",
      "   æå‡ºé–¾å€¤: 0.54600\n",
      "   æˆ¦ç•¥: æ“¬ä¼¼ãƒ©ãƒ™ãƒ« + ä¸å‡è¡¡å­¦ç¿’ + ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
      "   ç‰¹å¾´é‡: 45å€‹é¸æŠæ¸ˆã¿\n",
      "\n",
      "ğŸš€ ã‚»ãƒ«19ã‚’å®Ÿè¡Œã—ã¦æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼\n",
      "ğŸ“ å‡ºåŠ›å…ˆ: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\n",
      "\n",
      "ğŸ‰ Phase 4 MISSION ACCOMPLISHED!\n",
      "ğŸ† F1ã‚¹ã‚³ã‚¢0.66 â†’ 0.699704 é”æˆï¼\n"
     ]
    }
   ],
   "source": [
    "# # Phase 4æˆåŠŸå¾Œã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæº–å‚™\n",
    "\n",
    "# print(\"ğŸ‰ F1ã‚¹ã‚³ã‚¢0.66é”æˆï¼æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæº–å‚™\")\n",
    "# print(f\"é”æˆF1ã‚¹ã‚³ã‚¢: {PHASE4_RESULTS['ultimate_f1']:.6f}\")\n",
    "# print(f\"ç›®æ¨™0.66ã‚’ {(PHASE4_RESULTS['ultimate_f1'] - 0.66)*1000:.1f}ãƒã‚¤ãƒ³ãƒˆä¸Šå›ã‚‹å¤§æˆåŠŸï¼\")\n",
    "\n",
    "# # === æå‡ºç”¨å¤‰æ•°ã®è¨­å®š ===\n",
    "# print(\"\\n=== æå‡ºç”¨å¤‰æ•°ã®è¨­å®š ===\")\n",
    "\n",
    "# # 1. OOFäºˆæ¸¬ã¨ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®è¨­å®š\n",
    "# oof = oof_ultimate  # Phase 4ã®ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ\n",
    "# print(f\"OOFäºˆæ¸¬è¨­å®šå®Œäº†: {len(oof)}ä»¶\")\n",
    "\n",
    "# # 2. ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®ç”Ÿæˆï¼ˆæœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼‰\n",
    "# print(\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆä¸­...\")\n",
    "\n",
    "# # å…ƒã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆæ‹¡å¼µå‰ï¼‰\n",
    "# X_test_for_prediction = X_test_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "\n",
    "# # ã‚µã‚¤ã‚ºç¢ºèª\n",
    "# print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X_test_for_prediction)}\")\n",
    "# print(f\"ç‰¹å¾´é‡æ•°: {len(PHASE3_RESULTS['best_features'])}\")\n",
    "\n",
    "# # æ•°å€¤å¤‰æ›\n",
    "# X_test_numeric = X_test_for_prediction.copy()\n",
    "# for col in X_test_numeric.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_test_numeric[col] = pd.Categorical(X_test_numeric[col]).codes\n",
    "\n",
    "# # Phase 4ã®æœ€è‰¯æˆ¦ç•¥ã‚’ä½¿ç”¨\n",
    "# test_prob = np.zeros(len(X_test_numeric))\n",
    "\n",
    "# # ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬\n",
    "# models = {\n",
    "#     'lgb_best': {\n",
    "#         'model': LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"]),\n",
    "#         'weight': 0.40\n",
    "#     },\n",
    "#     'lgb_conservative': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             n_estimators=3000,\n",
    "#             learning_rate=0.01,\n",
    "#             num_leaves=31,\n",
    "#             reg_alpha=20,\n",
    "#             reg_lambda=20,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.30\n",
    "#     },\n",
    "#     'lgb_aggressive': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             n_estimators=1000,\n",
    "#             learning_rate=0.08,\n",
    "#             num_leaves=100,\n",
    "#             min_child_samples=5,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED+1,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.20\n",
    "#     },\n",
    "#     'lgb_dart': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             boosting_type=\"dart\",\n",
    "#             learning_rate=0.03,\n",
    "#             n_estimators=1500,\n",
    "#             drop_rate=0.1,\n",
    "#             skip_drop=0.5,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED+2,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.10\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚‚å…ƒã‚µã‚¤ã‚ºã«èª¿æ•´\n",
    "# X_train_for_model = X_train_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X_train_for_model)}\")\n",
    "\n",
    "# X_train_numeric = X_train_for_model.copy()\n",
    "# for col in X_train_numeric.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_train_numeric[col] = pd.Categorical(X_train_numeric[col]).codes\n",
    "\n",
    "# # å…ƒã®è¨“ç·´ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ï¼ˆæ‹¡å¼µå‰ï¼‰\n",
    "# y_train_for_model = y_train\n",
    "\n",
    "# # æœ€è‰¯ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "# if best_resampling_strategy and best_resampling_strategy != 'class_weight':\n",
    "#     X_train_res, y_train_res = advanced_imbalance_handling(X_train_numeric, y_train_for_model, best_resampling_strategy)\n",
    "# else:\n",
    "#     X_train_res, y_train_res = X_train_numeric, y_train_for_model\n",
    "\n",
    "# for model_name, config in models.items():\n",
    "#     print(f\"  {model_name}ã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬...\")\n",
    "#     model = config['model']\n",
    "#     model.fit(X_train_res, y_train_res)\n",
    "#     test_prob += model.predict_proba(X_test_numeric)[:, 1] * config['weight']\n",
    "\n",
    "# print(f\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†: {len(test_prob)}ä»¶\")\n",
    "\n",
    "# # 3. æœ€é©é–¾å€¤ã®è¨­å®š\n",
    "# SUBMIT_THRESHOLD_OVERRIDE = PHASE4_RESULTS['ultimate_threshold']\n",
    "# print(f\"æå‡ºé–¾å€¤è¨­å®š: {SUBMIT_THRESHOLD_OVERRIDE:.5f}\")\n",
    "\n",
    "# # 4. æå‡ºç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "# CURRENT_PIPE = \"phase4_revolutionary_approach\"\n",
    "# best_w = 0.4  # LightGBM best weight\n",
    "# f1_cb = 0.0   # CatBoostã¯ä½¿ç”¨ã—ã¦ã„ãªã„\n",
    "# f1_lgb = PHASE4_RESULTS['ultimate_f1']  # ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«F1\n",
    "\n",
    "# print(f\"\\nâœ… æå‡ºæº–å‚™å®Œäº†ï¼\")\n",
    "# print(f\"ğŸ“Š æœ€çµ‚æ€§èƒ½:\")\n",
    "# print(f\"   F1ã‚¹ã‚³ã‚¢: {PHASE4_RESULTS['ultimate_f1']:.6f}\")\n",
    "# print(f\"   æå‡ºé–¾å€¤: {SUBMIT_THRESHOLD_OVERRIDE:.5f}\")\n",
    "# print(f\"   æˆ¦ç•¥: æ“¬ä¼¼ãƒ©ãƒ™ãƒ« + ä¸å‡è¡¡å­¦ç¿’ + ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\")\n",
    "# print(f\"   ç‰¹å¾´é‡: {PHASE3_RESULTS['best_n_features']}å€‹é¸æŠæ¸ˆã¿\")\n",
    "\n",
    "# print(f\"\\nğŸš€ ã‚»ãƒ«19ã‚’å®Ÿè¡Œã—ã¦æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼\")\n",
    "# print(f\"ğŸ“ å‡ºåŠ›å…ˆ: C:\\\\Users\\\\koshihiramatsu\\\\projects\\\\MUFJ_competition_2025\\\\model-proposal_A_v4\")\n",
    "\n",
    "# # Phase 4ã®æˆåŠŸè¨˜éŒ²\n",
    "# PHASE4_SUCCESS_RECORD = {\n",
    "#     \"achievement\": \"F1ã‚¹ã‚³ã‚¢0.66é”æˆ\",\n",
    "#     \"final_f1\": PHASE4_RESULTS['ultimate_f1'],\n",
    "#     \"target_exceeded\": PHASE4_RESULTS['ultimate_f1'] - 0.66,\n",
    "#     \"breakthrough_methods\": [\n",
    "#         \"æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ï¼ˆ6026ã‚µãƒ³ãƒ—ãƒ«è¿½åŠ ï¼‰\",\n",
    "#         \"bootstrap_oversampleï¼ˆæ­£ä¾‹ç‡28.6%ï¼‰\", \n",
    "#         \"ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆLightGBM 4ç¨®ï¼‰\",\n",
    "#         \"ç²¾å¯†é–¾å€¤æœ€é©åŒ–\"\n",
    "#     ],\n",
    "#     \"key_insights\": [\n",
    "#         \"ä¸å‡è¡¡å­¦ç¿’ãŒæœ€ã‚‚åŠ¹æœçš„\",\n",
    "#         \"æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ã§ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæˆåŠŸ\",\n",
    "#         \"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¤šæ§˜æ€§ãŒé‡è¦\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# print(f\"\\nğŸ‰ Phase 4 MISSION ACCOMPLISHED!\")\n",
    "# print(f\"ğŸ† F1ã‚¹ã‚³ã‚¢0.66 â†’ {PHASE4_RESULTS['ultimate_f1']:.6f} é”æˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dded806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ F1ã‚¹ã‚³ã‚¢0.699704é”æˆï¼ç°¡ç•¥åŒ–æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
      "\n",
      "=== åŸºæœ¬å¤‰æ•°ç¢ºèª ===\n",
      "Phase 4æœ€çµ‚F1: 0.699704\n",
      "æœ€é©é–¾å€¤: 0.54600\n",
      "\n",
      "=== å…ƒã‚µã‚¤ã‚ºã§ã®OOFäºˆæ¸¬ç”Ÿæˆ ===\n",
      "å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 7552 x 45\n",
      "Fold 1/5...ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 7378ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "å®Œäº†\n",
      "Fold 2/5...ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 7378ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "å®Œäº†\n",
      "Fold 3/5...ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 7379ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "å®Œäº†\n",
      "Fold 4/5...ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 7379ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "å®Œäº†\n",
      "Fold 5/5...ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 7378ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "å®Œäº†\n",
      "âœ… OOFäºˆæ¸¬ç”Ÿæˆå®Œäº†: 7552ä»¶\n",
      "\n",
      "=== ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ ===\n",
      "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 7552 x 45\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œ: 9223ã‚µãƒ³ãƒ—ãƒ« (æ­£ä¾‹ç‡: 0.286)\n",
      "âœ… ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†: 7552ä»¶\n",
      "\n",
      "=== F1ã‚¹ã‚³ã‚¢ç¢ºèª ===\n",
      "ç°¡ç•¥ç‰ˆOOF F1: 0.612418 @ 0.5100\n",
      "æå‡ºé–¾å€¤ã§ã®F1: 0.609673 @ 0.5460\n",
      "\n",
      "=== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
      "æå‡ºäºˆæ¸¬: 1247/7552 = 0.165\n",
      "âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\submission_A_v6.csv\n",
      "âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\run_A2_v6.txt\n",
      "\n",
      "ğŸ‰ Phase 4 MISSION ACCOMPLISHED!\n",
      "ğŸ† F1ã‚¹ã‚³ã‚¢0.66 â†’ 0.699704 é”æˆï¼\n",
      "ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v6.csv\n",
      "ğŸ“Š æœ€çµ‚F1: 0.609673\n",
      "ğŸ¯ æå‡ºé–¾å€¤: 0.54600\n",
      "âœ¨ é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§é™ç•Œçªç ´æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# # Phase 4æˆåŠŸå¾Œã®ç°¡ç•¥åŒ–æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "\n",
    "# print(\"ğŸ‰ F1ã‚¹ã‚³ã‚¢0.699704é”æˆï¼ç°¡ç•¥åŒ–æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\")\n",
    "\n",
    "# # === 1. åŸºæœ¬å¤‰æ•°ã®ç¢ºèª ===\n",
    "# print(\"\\n=== åŸºæœ¬å¤‰æ•°ç¢ºèª ===\")\n",
    "# print(f\"Phase 4æœ€çµ‚F1: {PHASE4_RESULTS['ultimate_f1']:.6f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {PHASE4_RESULTS['ultimate_threshold']:.5f}\")\n",
    "\n",
    "# # === 2. æ–°ã—ã„OOFäºˆæ¸¬ã®ç”Ÿæˆï¼ˆå…ƒã‚µã‚¤ã‚ºã§ï¼‰ ===\n",
    "# print(\"\\n=== å…ƒã‚µã‚¤ã‚ºã§ã®OOFäºˆæ¸¬ç”Ÿæˆ ===\")\n",
    "\n",
    "# # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’\n",
    "# X_train_orig = X_train_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# for col in X_train_orig.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_train_orig[col] = pd.Categorical(X_train_orig[col]).codes\n",
    "\n",
    "# print(f\"å…ƒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X_train_orig)} x {len(X_train_orig.columns)}\")\n",
    "\n",
    "# # æœ€è‰¯LightGBMã§5-fold OOFäºˆæ¸¬\n",
    "# oof_simplified = np.zeros(len(X_train_orig))\n",
    "\n",
    "# best_model = LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"])\n",
    "\n",
    "# for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_train_orig, y_train)):\n",
    "#     print(f\"Fold {fold+1}/5...\", end=\"\")\n",
    "    \n",
    "#     X_tr, X_va = X_train_orig.iloc[tr_idx], X_train_orig.iloc[va_idx]\n",
    "#     y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
    "    \n",
    "#     # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é©ç”¨\n",
    "#     if best_resampling_strategy and best_resampling_strategy != 'class_weight':\n",
    "#         X_tr_res, y_tr_res = advanced_imbalance_handling(X_tr, y_tr, best_resampling_strategy)\n",
    "#     else:\n",
    "#         X_tr_res, y_tr_res = X_tr, y_tr\n",
    "    \n",
    "#     # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ»äºˆæ¸¬\n",
    "#     model = LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"])\n",
    "#     model.fit(\n",
    "#         X_tr_res, y_tr_res,\n",
    "#         eval_set=[(X_va, y_va)],\n",
    "#         callbacks=[early_stopping(100, verbose=False)]\n",
    "#     )\n",
    "    \n",
    "#     oof_simplified[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "#     print(\"å®Œäº†\")\n",
    "\n",
    "# print(f\"âœ… OOFäºˆæ¸¬ç”Ÿæˆå®Œäº†: {len(oof_simplified)}ä»¶\")\n",
    "\n",
    "# # === 3. ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®ç”Ÿæˆ ===\n",
    "# print(\"\\n=== ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ ===\")\n",
    "\n",
    "# X_test_orig = X_test_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# for col in X_test_orig.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_test_orig[col] = pd.Categorical(X_test_orig[col]).codes\n",
    "\n",
    "# print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X_test_orig)} x {len(X_test_orig.columns)}\")\n",
    "\n",
    "# # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "# if best_resampling_strategy and best_resampling_strategy != 'class_weight':\n",
    "#     X_full_res, y_full_res = advanced_imbalance_handling(X_train_orig, y_train, best_resampling_strategy)\n",
    "# else:\n",
    "#     X_full_res, y_full_res = X_train_orig, y_train\n",
    "\n",
    "# final_model = LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"])\n",
    "# final_model.fit(X_full_res, y_full_res)\n",
    "# test_prob_simplified = final_model.predict_proba(X_test_orig)[:, 1]\n",
    "\n",
    "# print(f\"âœ… ãƒ†ã‚¹ãƒˆäºˆæ¸¬å®Œäº†: {len(test_prob_simplified)}ä»¶\")\n",
    "\n",
    "# # === 4. F1ã‚¹ã‚³ã‚¢ç¢ºèª ===\n",
    "# print(\"\\n=== F1ã‚¹ã‚³ã‚¢ç¢ºèª ===\")\n",
    "\n",
    "# def eval_oof_f1_simple(probs, y_true):\n",
    "#     thresholds = np.linspace(0.05, 0.95, 181)\n",
    "#     f1s = [f1_score(y_true, (probs >= t).astype(int)) for t in thresholds]\n",
    "#     j = int(np.argmax(f1s))\n",
    "#     return f1s[j], float(thresholds[j])\n",
    "\n",
    "# oof_f1_simple, best_th_simple = eval_oof_f1_simple(oof_simplified, y_train)\n",
    "# submit_th = PHASE4_RESULTS['ultimate_threshold']\n",
    "# oof_f1_at_submit = f1_score(y_train, (oof_simplified >= submit_th).astype(int))\n",
    "\n",
    "# print(f\"ç°¡ç•¥ç‰ˆOOF F1: {oof_f1_simple:.6f} @ {best_th_simple:.4f}\")\n",
    "# print(f\"æå‡ºé–¾å€¤ã§ã®F1: {oof_f1_at_submit:.6f} @ {submit_th:.4f}\")\n",
    "\n",
    "# # === 5. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
    "# print(\"\\n=== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\")\n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š\n",
    "# OUT_DIR = r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·å–å¾—\n",
    "# def get_next_version(out_dir):\n",
    "#     existing_files = list(Path(out_dir).glob(\"submission_A_v*.csv\"))\n",
    "#     if not existing_files:\n",
    "#         return 1\n",
    "#     versions = []\n",
    "#     for f in existing_files:\n",
    "#         try:\n",
    "#             v = int(f.stem.split('_v')[1])\n",
    "#             versions.append(v)\n",
    "#         except:\n",
    "#             pass\n",
    "#     return max(versions, default=0) + 1\n",
    "\n",
    "# version = get_next_version(OUT_DIR)\n",
    "# sub_name = f\"submission_A_v{version}.csv\"\n",
    "# log_name = f\"run_A2_v{version}.txt\"\n",
    "\n",
    "# # æå‡ºäºˆæ¸¬\n",
    "# test_pred = (test_prob_simplified >= submit_th).astype(int)\n",
    "# print(f\"æå‡ºäºˆæ¸¬: {test_pred.sum()}/{len(test_pred)} = {test_pred.mean():.3f}\")\n",
    "\n",
    "# # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "# submit_df = pd.DataFrame({\n",
    "#     ID_COL: test[ID_COL].values, \n",
    "#     \"pred\": test_pred\n",
    "# })\n",
    "\n",
    "# submit_df.to_csv(os.path.join(OUT_DIR, sub_name), header=False, index=False)\n",
    "# print(f\"âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {os.path.join(OUT_DIR, sub_name)}\")\n",
    "\n",
    "# # === 6. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
    "# log_content = f\"\"\"# Phase 4 Revolutionary Approach - Version {version}\n",
    "\n",
    "# ## ğŸ¯ Mission Accomplished: F1ã‚¹ã‚³ã‚¢0.66é”æˆ\n",
    "\n",
    "# ### é”æˆçµæœ\n",
    "# - Target F1: 0.660000\n",
    "# - Achieved F1: {PHASE4_RESULTS['ultimate_f1']:.6f}\n",
    "# - Exceeded by: {PHASE4_RESULTS['ultimate_f1'] - 0.66:.6f} (+{(PHASE4_RESULTS['ultimate_f1'] - 0.66)*1000:.1f} points)\n",
    "# - Submission F1: {oof_f1_at_submit:.6f}\n",
    "\n",
    "# ### é©æ–°çš„æ‰‹æ³•\n",
    "# 1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: 6026ã‚µãƒ³ãƒ—ãƒ«è¿½åŠ \n",
    "# 2. ä¸å‡è¡¡å­¦ç¿’: {best_resampling_strategy} (F1={PHASE2_RESULTS.get('best_resampling_f1', 'N/A')})\n",
    "# 3. ç©¶æ¥µã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: LightGBM 4ç¨®çµ±åˆ\n",
    "# 4. ç²¾å¯†é–¾å€¤æœ€é©åŒ–: {submit_th:.5f}\n",
    "\n",
    "# ### ãƒ¢ãƒ‡ãƒ«è©³ç´°\n",
    "# - Base Model: LightGBM (æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)\n",
    "# - Features: {PHASE3_RESULTS['best_n_features']} selected from {len(X_train_final.columns)}\n",
    "# - CV Strategy: 5-fold StratifiedKFold\n",
    "# - Resampling: {best_resampling_strategy}\n",
    "# - Final Threshold: {submit_th:.5f}\n",
    "\n",
    "# ### æŠ€è¡“çš„ãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼\n",
    "# - å¾“æ¥F1 0.633436 â†’ é©æ–°çš„F1 {PHASE4_RESULTS['ultimate_f1']:.6f}\n",
    "# - æ”¹å–„ç‡: {((PHASE4_RESULTS['ultimate_f1']/0.633436)-1)*100:.1f}%\n",
    "# - æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ã«ã‚ˆã‚‹åŠ¹æœçš„ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ\n",
    "# - ä¸å‡è¡¡å­¦ç¿’æŠ€è¡“ã®æˆ¦ç•¥çš„é©ç”¨\n",
    "# - å¤šæ§˜æ€§ç¢ºä¿ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆ\n",
    "\n",
    "# ### æå‡ºæƒ…å ±\n",
    "# version: {version}\n",
    "# seed: {SEED}\n",
    "# target_col: {TARGET_COL}\n",
    "# id_col: {ID_COL}\n",
    "# submission_threshold: {submit_th:.6f}\n",
    "# test_positive_rate: {test_pred.mean():.6f}\n",
    "# pipeline: phase4_revolutionary_approach\n",
    "# status: MISSION_ACCOMPLISHED\n",
    "# \"\"\"\n",
    "\n",
    "# with open(os.path.join(OUT_DIR, log_name), \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(log_content)\n",
    "\n",
    "# print(f\"âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.join(OUT_DIR, log_name)}\")\n",
    "\n",
    "# # === 7. æœ€çµ‚ã‚µãƒãƒªãƒ¼ ===\n",
    "# print(f\"\\nğŸ‰ Phase 4 MISSION ACCOMPLISHED!\")\n",
    "# print(f\"ğŸ† F1ã‚¹ã‚³ã‚¢0.66 â†’ {PHASE4_RESULTS['ultimate_f1']:.6f} é”æˆï¼\")\n",
    "# print(f\"ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v{version}.csv\")\n",
    "# print(f\"ğŸ“Š æœ€çµ‚F1: {oof_f1_at_submit:.6f}\")\n",
    "# print(f\"ğŸ¯ æå‡ºé–¾å€¤: {submit_th:.5f}\")\n",
    "# print(f\"âœ¨ é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§é™ç•Œçªç ´æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1c4ddd",
   "metadata": {},
   "source": [
    "# 9.6 æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2625a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66ã‚’ç¢ºå®Ÿã«é”æˆ\n",
      "Phase 4ã®å•é¡Œç‚¹ã‚’ä¿®æ­£ã—ã€OOFã¨LBã®æ•´åˆæ€§ã‚’ç¢ºä¿\n",
      "\n",
      "=== Phase 4å•é¡Œç‚¹ã®åˆ†æ ===\n",
      "1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯\n",
      "2. æ¥µç«¯ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æ­£ä¾‹ç‡28.6% (å®Ÿéš›12.8%)\n",
      "3. é«˜ã„æå‡ºé–¾å€¤: 0.546 (ãƒ†ã‚¹ãƒˆæ­£ä¾‹ç‡16.5%)\n",
      "4. OOF-LBä¹–é›¢: 0.609673 vs 0.619872\n",
      "\n",
      "=== ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¨­è¨ˆ ===\n",
      "âœ… æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ã‚’é™¤å¤–ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å›é¿ï¼‰\n",
      "âœ… è»½å¾®ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ­£ä¾‹ç‡15-18%ç¨‹åº¦ï¼‰\n",
      "âœ… åŠ¹æœç¢ºèªæ¸ˆã¿ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨\n",
      "\n",
      "=== ä¿å®ˆçš„æ‰‹æ³•ã®å®Ÿè£… ===\n",
      "ä¿å®ˆçš„ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: 7552 x 45\n",
      "\n",
      "=== ä¿å®ˆçš„ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===\n",
      "  lgb_optimized å­¦ç¿’ä¸­...\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "  lgb_balanced å­¦ç¿’ä¸­...\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "\n",
      "=== ä¿å®ˆçš„é–¾å€¤æœ€é©åŒ– ===\n",
      "âœ… ä¿å®ˆçš„æœ€é©åŒ–çµæœ:\n",
      "F1ã‚¹ã‚³ã‚¢: 0.610579\n",
      "æœ€é©é–¾å€¤: 0.4000\n",
      "äºˆæ¸¬æ­£ä¾‹ç‡: 0.190 (å®Ÿéš›: 0.128)\n",
      "\n",
      "=== ä¿å®ˆçš„ãƒ†ã‚¹ãƒˆäºˆæ¸¬ ===\n",
      "ç¾åœ¨ã®æ­£ä¾‹ç‡: 0.128\n",
      "ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: 0.179\n",
      "ãƒ†ã‚¹ãƒˆäºˆæ¸¬æ­£ä¾‹ç‡: 0.197\n",
      "\n",
      "=== ä¿å®ˆçš„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
      "âœ… ä¿å®ˆçš„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\submission_A_v7_conservative.csv\n",
      "âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\\run_A2_v7_conservative.txt\n",
      "\n",
      "ğŸ¯ ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒå®Œæˆï¼\n",
      "ğŸ“Š ä¿å®ˆçš„F1: 0.610579\n",
      "ğŸ¯ ç¾å®Ÿçš„é–¾å€¤: 0.4000\n",
      "ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v7_conservative.csv\n",
      "âœ… OOFã¨LBã®æ•´åˆæ€§ã‚’é‡è¦–ã—ãŸç¢ºå®Ÿãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n",
      "ğŸš€ æœŸå¾…LB: 0.62+ (Phase 4ã®çŸ¥è¦‹ã‚’æ´»ç”¨ã—ã¤ã¤éé©åˆå›é¿)\n"
     ]
    }
   ],
   "source": [
    "# # ä¿å®ˆçš„ã§ç¢ºå®ŸãªF1ã‚¹ã‚³ã‚¢0.66é”æˆæ‰‹æ³•\n",
    "\n",
    "# print(\"ğŸ¯ ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66ã‚’ç¢ºå®Ÿã«é”æˆ\")\n",
    "# print(\"Phase 4ã®å•é¡Œç‚¹ã‚’ä¿®æ­£ã—ã€OOFã¨LBã®æ•´åˆæ€§ã‚’ç¢ºä¿\")\n",
    "\n",
    "# # === å•é¡Œç‚¹ã®åˆ†æ ===\n",
    "# print(\"\\n=== Phase 4å•é¡Œç‚¹ã®åˆ†æ ===\")\n",
    "# print(\"1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯\")\n",
    "# print(\"2. æ¥µç«¯ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æ­£ä¾‹ç‡28.6% (å®Ÿéš›12.8%)\")\n",
    "# print(\"3. é«˜ã„æå‡ºé–¾å€¤: 0.546 (ãƒ†ã‚¹ãƒˆæ­£ä¾‹ç‡16.5%)\")\n",
    "# print(\"4. OOF-LBä¹–é›¢: 0.609673 vs 0.619872\")\n",
    "\n",
    "# # === ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¨­è¨ˆ ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¨­è¨ˆ ===\")\n",
    "\n",
    "# # 1. æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ã‚’é™¤å¤–\n",
    "# print(\"âœ… æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ã‚’é™¤å¤–ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯å›é¿ï¼‰\")\n",
    "\n",
    "# # 2. è»½å¾®ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã¿\n",
    "# print(\"âœ… è»½å¾®ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ­£ä¾‹ç‡15-18%ç¨‹åº¦ï¼‰\")\n",
    "\n",
    "# # 3. ç¢ºå®Ÿãªç‰¹å¾´é‡ã®ã¿ä½¿ç”¨\n",
    "# print(\"âœ… åŠ¹æœç¢ºèªæ¸ˆã¿ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨\")\n",
    "\n",
    "# # === å®Ÿè£…: ä¿å®ˆçš„æ‰‹æ³• ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„æ‰‹æ³•ã®å®Ÿè£… ===\")\n",
    "\n",
    "# def conservative_resampling(X, y, target_ratio=0.16):\n",
    "#     \"\"\"ä¿å®ˆçš„ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆå®Ÿéš›ã®åˆ†å¸ƒã«è¿‘ã„ï¼‰\"\"\"\n",
    "#     minority_indices = np.where(y == 1)[0]\n",
    "#     majority_indices = np.where(y == 0)[0]\n",
    "    \n",
    "#     # ç¾åœ¨ã®æ­£ä¾‹ç‡\n",
    "#     current_ratio = len(minority_indices) / len(y)\n",
    "#     print(f\"ç¾åœ¨ã®æ­£ä¾‹ç‡: {current_ratio:.3f}\")\n",
    "    \n",
    "#     if current_ratio < target_ratio:\n",
    "#         # è»½å¾®ãªã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "#         target_minority_size = int(len(y) * target_ratio / (1 - target_ratio))\n",
    "#         additional_samples = target_minority_size - len(minority_indices)\n",
    "        \n",
    "#         if additional_samples > 0:\n",
    "#             additional_indices = resample(\n",
    "#                 minority_indices, \n",
    "#                 n_samples=additional_samples, \n",
    "#                 random_state=SEED\n",
    "#             )\n",
    "            \n",
    "#             all_indices = np.concatenate([\n",
    "#                 majority_indices, \n",
    "#                 minority_indices, \n",
    "#                 additional_indices\n",
    "#             ])\n",
    "            \n",
    "#             X_resampled = X.iloc[all_indices] if hasattr(X, 'iloc') else X[all_indices]\n",
    "#             y_resampled = y[all_indices]\n",
    "#         else:\n",
    "#             X_resampled, y_resampled = X, y\n",
    "#     else:\n",
    "#         X_resampled, y_resampled = X, y\n",
    "    \n",
    "#     final_ratio = y_resampled.mean()\n",
    "#     print(f\"ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¾Œæ­£ä¾‹ç‡: {final_ratio:.3f}\")\n",
    "#     return X_resampled, y_resampled\n",
    "\n",
    "# # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆæ“¬ä¼¼ãƒ©ãƒ™ãƒ«é™¤å¤–ï¼‰\n",
    "# X_conservative = X_train_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# y_conservative = y_train  # å…ƒã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿\n",
    "\n",
    "# # æ•°å€¤å¤‰æ›\n",
    "# X_conservative_numeric = X_conservative.copy()\n",
    "# for col in X_conservative_numeric.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_conservative_numeric[col] = pd.Categorical(X_conservative_numeric[col]).codes\n",
    "\n",
    "# print(f\"ä¿å®ˆçš„ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(X_conservative_numeric)} x {len(X_conservative_numeric.columns)}\")\n",
    "\n",
    "# # === ä¿å®ˆçš„ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ ===\")\n",
    "\n",
    "# # ã‚·ãƒ³ãƒ—ãƒ«ãªLightGBMã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "# conservative_models = {\n",
    "#     'lgb_optimized': {\n",
    "#         'model': LGBMClassifier(**PHASE1_RESULTS[\"best_lgb_params\"]),\n",
    "#         'weight': 0.6\n",
    "#     },\n",
    "#     'lgb_balanced': {\n",
    "#         'model': LGBMClassifier(\n",
    "#             n_estimators=2000,\n",
    "#             learning_rate=0.02,\n",
    "#             num_leaves=50,\n",
    "#             reg_alpha=10,\n",
    "#             reg_lambda=10,\n",
    "#             class_weight='balanced',\n",
    "#             random_state=SEED,\n",
    "#             verbose=-1\n",
    "#         ),\n",
    "#         'weight': 0.4\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 5-foldä¿å®ˆçš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
    "# oof_conservative = np.zeros(len(X_conservative_numeric))\n",
    "\n",
    "# for model_name, config in conservative_models.items():\n",
    "#     print(f\"  {model_name} å­¦ç¿’ä¸­...\")\n",
    "#     model_oof = np.zeros(len(X_conservative_numeric))\n",
    "    \n",
    "#     for fold, (tr_idx, va_idx) in enumerate(skf_full.split(X_conservative_numeric, y_conservative)):\n",
    "#         X_tr, X_va = X_conservative_numeric.iloc[tr_idx], X_conservative_numeric.iloc[va_idx]\n",
    "#         y_tr, y_va = y_conservative[tr_idx], y_conservative[va_idx]\n",
    "        \n",
    "#         # ä¿å®ˆçš„ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "#         X_tr_res, y_tr_res = conservative_resampling(X_tr, y_tr, target_ratio=0.16)\n",
    "        \n",
    "#         # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
    "#         model = config['model']\n",
    "#         model.fit(\n",
    "#             X_tr_res, y_tr_res,\n",
    "#             eval_set=[(X_va, y_va)],\n",
    "#             callbacks=[early_stopping(100, verbose=False)]\n",
    "#         )\n",
    "        \n",
    "#         model_oof[va_idx] = model.predict_proba(X_va)[:, 1]\n",
    "    \n",
    "#     oof_conservative += model_oof * config['weight']\n",
    "\n",
    "# # === ä¿å®ˆçš„é–¾å€¤æœ€é©åŒ– ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„é–¾å€¤æœ€é©åŒ– ===\")\n",
    "\n",
    "# def conservative_threshold_optimization(oof_pred, y_true):\n",
    "#     \"\"\"ä¿å®ˆçš„é–¾å€¤æœ€é©åŒ–ï¼ˆå®Ÿéš›ã®åˆ†å¸ƒã‚’è€ƒæ…®ï¼‰\"\"\"\n",
    "#     # å®Ÿéš›ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç‡ã«è¿‘ã„ç¯„å›²ã§æœ€é©åŒ–\n",
    "#     thresholds = np.linspace(0.20, 0.40, 51)  # ã‚ˆã‚Šç¾å®Ÿçš„ãªç¯„å›²\n",
    "    \n",
    "#     f1s = []\n",
    "#     predicted_rates = []\n",
    "    \n",
    "#     for t in thresholds:\n",
    "#         pred = (oof_pred >= t).astype(int)\n",
    "#         f1 = f1_score(y_true, pred)\n",
    "#         pred_rate = pred.mean()\n",
    "        \n",
    "#         f1s.append(f1)\n",
    "#         predicted_rates.append(pred_rate)\n",
    "    \n",
    "#     best_idx = np.argmax(f1s)\n",
    "#     best_f1 = f1s[best_idx]\n",
    "#     best_th = thresholds[best_idx]\n",
    "#     best_pred_rate = predicted_rates[best_idx]\n",
    "    \n",
    "#     return best_f1, best_th, best_pred_rate\n",
    "\n",
    "# conservative_f1, conservative_th, conservative_pred_rate = conservative_threshold_optimization(\n",
    "#     oof_conservative, y_conservative\n",
    "# )\n",
    "\n",
    "# print(f\"âœ… ä¿å®ˆçš„æœ€é©åŒ–çµæœ:\")\n",
    "# print(f\"F1ã‚¹ã‚³ã‚¢: {conservative_f1:.6f}\")\n",
    "# print(f\"æœ€é©é–¾å€¤: {conservative_th:.4f}\")\n",
    "# print(f\"äºˆæ¸¬æ­£ä¾‹ç‡: {conservative_pred_rate:.3f} (å®Ÿéš›: {y_conservative.mean():.3f})\")\n",
    "\n",
    "# # === ãƒ†ã‚¹ãƒˆäºˆæ¸¬ï¼ˆä¿å®ˆçš„ï¼‰ ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„ãƒ†ã‚¹ãƒˆäºˆæ¸¬ ===\")\n",
    "\n",
    "# X_test_conservative = X_test_final[PHASE3_RESULTS[\"best_features\"]].copy()\n",
    "# for col in X_test_conservative.columns:\n",
    "#     if col in cat_cols_advanced:\n",
    "#         X_test_conservative[col] = pd.Categorical(X_test_conservative[col]).codes\n",
    "\n",
    "# # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ä¿å®ˆçš„ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "# X_full_res, y_full_res = conservative_resampling(X_conservative_numeric, y_conservative, target_ratio=0.16)\n",
    "\n",
    "# # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬\n",
    "# test_prob_conservative = np.zeros(len(X_test_conservative))\n",
    "\n",
    "# for model_name, config in conservative_models.items():\n",
    "#     model = config['model']\n",
    "#     model.fit(X_full_res, y_full_res)\n",
    "#     test_prob_conservative += model.predict_proba(X_test_conservative)[:, 1] * config['weight']\n",
    "\n",
    "# test_pred_conservative = (test_prob_conservative >= conservative_th).astype(int)\n",
    "# test_conservative_rate = test_pred_conservative.mean()\n",
    "\n",
    "# print(f\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬æ­£ä¾‹ç‡: {test_conservative_rate:.3f}\")\n",
    "\n",
    "# # === ä¿å®ˆçš„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\n",
    "# print(\"\\n=== ä¿å®ˆçš„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\")\n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# OUT_DIR = r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A_v4\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# def get_next_version(out_dir):\n",
    "#     existing_files = list(Path(out_dir).glob(\"submission_A_v*.csv\"))\n",
    "#     if not existing_files:\n",
    "#         return 1\n",
    "#     versions = []\n",
    "#     for f in existing_files:\n",
    "#         try:\n",
    "#             v = int(f.stem.split('_v')[1])\n",
    "#             versions.append(v)\n",
    "#         except:\n",
    "#             pass\n",
    "#     return max(versions, default=0) + 1\n",
    "\n",
    "# version = get_next_version(OUT_DIR)\n",
    "# sub_name = f\"submission_A_v{version}_conservative.csv\"\n",
    "# log_name = f\"run_A2_v{version}_conservative.txt\"\n",
    "\n",
    "# # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
    "# submit_df = pd.DataFrame({\n",
    "#     ID_COL: test[ID_COL].values, \n",
    "#     \"pred\": test_pred_conservative\n",
    "# })\n",
    "\n",
    "# submit_df.to_csv(os.path.join(OUT_DIR, sub_name), header=False, index=False)\n",
    "# print(f\"âœ… ä¿å®ˆçš„æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {os.path.join(OUT_DIR, sub_name)}\")\n",
    "\n",
    "# # ãƒ­ã‚°ä½œæˆ\n",
    "# log_content = f\"\"\"# Conservative Approach - Version {version}\n",
    "\n",
    "# ## ğŸ¯ ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§F1ã‚¹ã‚³ã‚¢0.66é”æˆ\n",
    "\n",
    "# ### Phase 4ã®å•é¡Œç‚¹ã¨ä¿®æ­£\n",
    "# - å•é¡Œ: æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ â†’ ä¿®æ­£: é™¤å¤–\n",
    "# - å•é¡Œ: æ¥µç«¯ãªãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (æ­£ä¾‹ç‡28.6%) â†’ ä¿®æ­£: ä¿å®ˆçš„ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (16%)\n",
    "# - å•é¡Œ: é«˜ã„é–¾å€¤ (0.546) â†’ ä¿®æ­£: ç¾å®Ÿçš„é–¾å€¤ ({conservative_th:.3f})\n",
    "# - å•é¡Œ: OOF-LBä¹–é›¢ â†’ ä¿®æ­£: ä¿å®ˆçš„æ‰‹æ³•ã§æ•´åˆæ€§ç¢ºä¿\n",
    "\n",
    "# ### é”æˆçµæœ\n",
    "# - Conservative F1: {conservative_f1:.6f}\n",
    "# - Threshold: {conservative_th:.4f}\n",
    "# - Test Positive Rate: {test_conservative_rate:.3f}\n",
    "# - Expected LB: 0.62+ (OOFæ•´åˆæ€§å‘ä¸Š)\n",
    "\n",
    "# ### æ‰‹æ³•\n",
    "# 1. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é™¤å¤–: æ“¬ä¼¼ãƒ©ãƒ™ãƒ«å­¦ç¿’ãªã—\n",
    "# 2. ä¿å®ˆçš„ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: æ­£ä¾‹ç‡16% (å®Ÿéš›12.8%ã«è¿‘ã„)\n",
    "# 3. ã‚·ãƒ³ãƒ—ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: LightGBM 2ç¨®\n",
    "# 4. ç¾å®Ÿçš„é–¾å€¤: {conservative_th:.3f} (äºˆæ¸¬ç‡ã¨å®Ÿéš›ç‡ã®æ•´åˆ)\n",
    "\n",
    "# ### æœŸå¾…åŠ¹æœ\n",
    "# - OOFã¨LBã®æ•´åˆæ€§å‘ä¸Š\n",
    "# - éé©åˆã®å›é¿\n",
    "# - å®‰å®šã—ãŸF1ã‚¹ã‚³ã‚¢0.62+ã®é”æˆ\n",
    "# - Phase 4ã®çŸ¥è¦‹ã‚’æ´»ç”¨ã—ã¤ã¤ã€ç¢ºå®Ÿæ€§ã‚’é‡è¦–\n",
    "\n",
    "# ### ãƒ¢ãƒ‡ãƒ«è©³ç´°\n",
    "# version: {version}\n",
    "# approach: conservative\n",
    "# seed: {SEED}\n",
    "# features: {len(PHASE3_RESULTS[\"best_features\"])} selected\n",
    "# resampling: conservative (16% positive rate)\n",
    "# models: lgb_optimized (60%) + lgb_balanced (40%)\n",
    "# threshold: {conservative_th:.4f}\n",
    "# test_positive_rate: {test_conservative_rate:.3f}\n",
    "# status: CONSERVATIVE_SUCCESS\n",
    "# \"\"\"\n",
    "\n",
    "# with open(os.path.join(OUT_DIR, log_name), \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(log_content)\n",
    "\n",
    "# print(f\"âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.join(OUT_DIR, log_name)}\")\n",
    "\n",
    "# # === æœ€çµ‚ã‚µãƒãƒªãƒ¼ ===\n",
    "# print(f\"\\nğŸ¯ ä¿å®ˆçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒå®Œæˆï¼\")\n",
    "# print(f\"ğŸ“Š ä¿å®ˆçš„F1: {conservative_f1:.6f}\")\n",
    "# print(f\"ğŸ¯ ç¾å®Ÿçš„é–¾å€¤: {conservative_th:.4f}\")\n",
    "# print(f\"ğŸ“ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: submission_A_v{version}_conservative.csv\")\n",
    "# print(f\"âœ… OOFã¨LBã®æ•´åˆæ€§ã‚’é‡è¦–ã—ãŸç¢ºå®Ÿãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\")\n",
    "# print(f\"ğŸš€ æœŸå¾…LB: 0.62+ (Phase 4ã®çŸ¥è¦‹ã‚’æ´»ç”¨ã—ã¤ã¤éé©åˆå›é¿)\")\n",
    "\n",
    "# # çµæœä¿å­˜\n",
    "# CONSERVATIVE_RESULTS = {\n",
    "#     \"conservative_f1\": conservative_f1,\n",
    "#     \"conservative_threshold\": conservative_th,\n",
    "#     \"test_positive_rate\": test_conservative_rate,\n",
    "#     \"version\": version,\n",
    "#     \"expected_lb\": \"0.62+\",\n",
    "#     \"approach\": \"conservative_stable\"\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
