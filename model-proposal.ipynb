{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04a8ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 前処理(リークなし)完了: (7552, 23) (7552, 22)\n"
     ]
    }
   ],
   "source": [
    "# ■ 完全前処理セル（リークなし版：ノート最初に一度だけ）\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. データ読み込み\n",
    "DATA_DIR = r\"G:\\マイドライブ\\MUFJ_competition_2025\\data\"\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "test_df  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "# 2. 対数変換 (log1p)\n",
    "for col in ['GrossApproval','SBAGuaranteedApproval','JobsSupported']:\n",
    "    train_df[f'{col}_log1p'] = np.log1p(train_df[col])\n",
    "    test_df [f'{col}_log1p'] = np.log1p(test_df [col])\n",
    "\n",
    "# 3. 比率・交互作用特徴\n",
    "train_df['ratio1']    = train_df['SBAGuaranteedApproval'] / train_df['GrossApproval']\n",
    "train_df['interact1'] = train_df['TermInMonths'] * train_df['InitialInterestRate']\n",
    "train_df['ratio2']    = train_df['GrossApproval_log1p'] / (train_df['TermInMonths'] + 1)\n",
    "train_df['interact2'] = train_df['JobsSupported_log1p'] * train_df['InitialInterestRate']\n",
    "\n",
    "test_df ['ratio1']    = test_df ['SBAGuaranteedApproval'] / test_df ['GrossApproval']\n",
    "test_df ['interact1'] = test_df ['TermInMonths'] * test_df ['InitialInterestRate']\n",
    "test_df ['ratio2']    = test_df ['GrossApproval_log1p'] / (test_df ['TermInMonths'] + 1)\n",
    "test_df ['interact2'] = test_df ['JobsSupported_log1p'] * test_df ['InitialInterestRate']\n",
    "\n",
    "# 4. カテゴリ型キャスト（ここまでで止める）\n",
    "cat_cols = [\n",
    "    'Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "    'BusinessType','BusinessAge','CollateralInd'\n",
    "]\n",
    "for c in cat_cols:\n",
    "    train_df[c] = train_df[c].astype('category')\n",
    "    test_df [c] = test_df [c].astype('category')\n",
    "\n",
    "# 5〜6（TE/FE作成）はやらない。OOFセルで作る\n",
    "# 8（カテゴリ一括コード化）もやらない。OOFセルが _code 列を別名で作る\n",
    "\n",
    "# 参考：基本数値特徴（OOFセル内で使う）\n",
    "base_feats = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate',\n",
    "    'TermInMonths','CongressionalDistrict','RevolverStatus'\n",
    "]\n",
    "te_cols = ['Subprogram','NaicsSector','BusinessAge','BusinessType']  # TE対象\n",
    "\n",
    "print(\"✅ 前処理(リークなし)完了:\", train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007f6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ カテゴリ変数を整数コード化する共通関数\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categories(df, cat_cols):\n",
    "    \"\"\"\n",
    "    DataFrame df の cat_cols で指定した列を\n",
    "    pd.Categorical(...).codes で整数コード化して上書きします。\n",
    "    \"\"\"\n",
    "    for c in cat_cols:\n",
    "        df[c] = pd.Categorical(df[c]).codes\n",
    "    return df\n",
    "\n",
    "# カテゴリ列一覧\n",
    "cat_cols = [\n",
    "    'Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "    'BusinessType','BusinessAge','CollateralInd'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1bdb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ category → codes 完了（dtype に依存せず実行）\n"
     ]
    }
   ],
   "source": [
    "# ── カテゴリ変数を整数コード化（修正版）──\n",
    "import pandas as pd\n",
    "\n",
    "cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "            'BusinessType','BusinessAge','CollateralInd']\n",
    "for c in cat_cols:\n",
    "    # pandas.Categorical でカテゴリ型に変換してから .codes 取得\n",
    "    train_df[c] = pd.Categorical(train_df[c]).codes\n",
    "    test_df [c] = pd.Categorical(test_df[c]).codes\n",
    "\n",
    "print(\"✅ category → codes 完了（dtype に依存せず実行）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc575f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed: (7552, 61)\n",
      "X_test_processed : (7552, 61)\n"
     ]
    }
   ],
   "source": [
    "# ■ 前処理パイプライン（修正版：X_train 定義 → 列指定を cat_cols ベースに）\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 0) 前提: train_df, test_df, features_fe, cat_cols が既に定義済み\n",
    "X_train = train_df[features_fe].copy()\n",
    "y_train = train_df['LoanStatus'].astype(int)\n",
    "X_test  = test_df[features_fe].copy()\n",
    "\n",
    "# 1) 列リスト（カテゴリは cat_cols を明示、残りを数値扱い）\n",
    "categorical_features = [c for c in cat_cols if c in X_train.columns]\n",
    "numeric_features     = [c for c in X_train.columns if c not in categorical_features]\n",
    "\n",
    "# 2) 変換器\n",
    "numeric_transformer     = StandardScaler(with_mean=True, with_std=True)\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# 3) ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer,     numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 4) 適用\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"X_train_processed:\", X_train_processed.shape)\n",
    "print(\"X_test_processed :\", X_test_processed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24760c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed: (7552, 61)\n",
      "X_test_processed : (7552, 61)\n",
      "✅ X_train/X_test のカテゴリコード化完了\n"
     ]
    }
   ],
   "source": [
    "# 特徴量と目的変数の切り出し\n",
    "X_train = train_df.drop(columns=['LoanStatus'])\n",
    "y_train = train_df['LoanStatus']\n",
    "X_test  = test_df.copy()\n",
    "\n",
    "# 学習データに対して fit_transform，テストデータに対して transform\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"X_train_processed:\", X_train_processed.shape)\n",
    "print(\"X_test_processed :\", X_test_processed.shape)\n",
    "\n",
    "# ■ カテゴリ列コード化：X_train／X_test\n",
    "X_train = encode_categories(X_train, cat_cols)\n",
    "X_test  = encode_categories(X_test,  cat_cols)\n",
    "print(\"✅ X_train/X_test のカテゴリコード化完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb7473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 Scores: [0.39365918 0.39295393 0.38522427 0.41463415 0.3907455 ]\n",
      "Mean CV F1: 0.3954434065099962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# モデル定義（クラス重みをバランスさせ、収束最大反復回数を増加）\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=0)\n",
    "\n",
    "# 5分割クロスバリデーションで F1 スコアを計算\n",
    "cv_scores = cross_val_score(\n",
    "    lr,\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "print(\"CV F1 Scores:\", cv_scores)\n",
    "print(\"Mean CV F1:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed63ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "70 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "70 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1203, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.39555185 0.3954464  0.13095172 0.12961879        nan 0.13095172\n",
      " 0.39554439 0.13088419 0.13095172 0.13102135        nan 0.39534266\n",
      "        nan        nan 0.39554923 0.13095172 0.13095172 0.39555185\n",
      " 0.13095172 0.39511914        nan        nan 0.13102135 0.39696149\n",
      " 0.39565336 0.12122487 0.39553538 0.39575503 0.13102135 0.39554923\n",
      " 0.13095172        nan        nan 0.13102135        nan        nan\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params : {'C': np.float64(0.39425426472734726), 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "Best F1    : 0.3969614944104854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# ■ 探索するハイパーパラメータ空間\n",
    "param_dist = {\n",
    "    'C': uniform(loc=0.01, scale=10),             # 正則化強度の逆数\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],# 正則化項\n",
    "    'solver': ['saga'],                           # elasticnet対応\n",
    "    'class_weight': [None, 'balanced']            # クラス不均衡対応\n",
    "}\n",
    "\n",
    "# ■ RandomizedSearchCV の設定\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=10000, random_state=0),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,            # 試行回数\n",
    "    cv=5,                 # 5分割 CV\n",
    "    scoring='f1',         # F1 スコアで評価\n",
    "    verbose=1,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ■ 探索の実行\n",
    "rs.fit(X_train_processed, y_train)\n",
    "\n",
    "# ■ 最適値の確認\n",
    "print(\"Best params :\", rs.best_params_)\n",
    "print(\"Best F1    :\", rs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e1c9dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  LoanStatus\n",
      "0  7553           0\n",
      "1  7554           0\n",
      "2  7555           0\n",
      "3  7556           0\n",
      "4  7557           1\n",
      ">>> submission_best.csv を出力しました。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. RandomizedSearchCV で見つかった最良モデルを取り出す\n",
    "best_lr = rs.best_estimator_\n",
    "\n",
    "# 2. 全学習データで再学習\n",
    "best_lr.fit(X_train_processed, y_train)\n",
    "\n",
    "# 3. 評価用データに対する予測\n",
    "y_pred_best = best_lr.predict(X_test_processed)\n",
    "\n",
    "# 4. 元データの id を再取得\n",
    "test_ids = pd.read_csv(f\"{DATA_DIR}/test.csv\")['id']\n",
    "\n",
    "# 5. 提出用 DataFrame の構築\n",
    "submission_best = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'LoanStatus': y_pred_best\n",
    "})\n",
    "\n",
    "# 6. CSV として出力\n",
    "submission_best.to_csv('submission_best.csv', index=False)\n",
    "print(submission_best.head())\n",
    "print(\">>> submission_best.csv を出力しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a068fcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 最適閾値: 0.664（対応 F1: 0.457）\n",
      ">>> submission_fixed.csv をヘッダーなしで出力しました。\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LoanStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  LoanStatus\n",
       "0  7553           0\n",
       "1  7554           0\n",
       "2  7555           0\n",
       "3  7556           0\n",
       "4  7557           1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from numpy import where\n",
    "\n",
    "# 1. 学習データ上での陽性クラス確率を取得\n",
    "y_scores_train = best_lr.predict_proba(X_train_processed)[:, 1]\n",
    "\n",
    "# 2. precision–recall 曲線から閾値と適合率・再現率を取得\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, y_scores_train)\n",
    "\n",
    "# 3. 各閾値での F1 を計算し、最大となる閾値を選択\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "ix = np.nanargmax(f1_scores[:-1])   # 最後の要素は無効\n",
    "best_threshold = thresholds[ix]\n",
    "print(f\"→ 最適閾値: {best_threshold:.3f}（対応 F1: {f1_scores[ix]:.3f}）\")\n",
    "\n",
    "# 4. テストデータ上での陽性クラス確率を取得\n",
    "test_scores = best_lr.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# 5. 最適閾値で二値化\n",
    "y_pred_thresh = where(test_scores >= best_threshold, 1, 0)\n",
    "\n",
    "# 6. 提出用 ID を再取得\n",
    "test_ids = pd.read_csv(f\"{DATA_DIR}/test.csv\")['id']\n",
    "\n",
    "# 7. 提出用 DataFrame を構築\n",
    "submission_fixed = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'LoanStatus': y_pred_thresh\n",
    "})\n",
    "\n",
    "# 8. CSV として出力（ヘッダー行を含めない）\n",
    "submission_fixed.to_csv('submission_fixed.csv', index=False, header=False)\n",
    "print(\">>> submission_fixed.csv をヘッダーなしで出力しました。\")\n",
    "submission_fixed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d598a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Train):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.958     0.694     0.805      6588\n",
      "           1      0.274     0.791     0.407       964\n",
      "\n",
      "    accuracy                          0.706      7552\n",
      "   macro avg      0.616     0.743     0.606      7552\n",
      "weighted avg      0.871     0.706     0.754      7552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ■ 8.1：混同行列と分類レポート (Train データ)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 学習データへの予測値取得\n",
    "y_train_pred = best_lr.predict(X_train_processed)\n",
    "\n",
    "\n",
    "# 混同行列の算出\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# 可視化\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Train)')\n",
    "plt.show()\n",
    "\n",
    "# 分類レポートの表示\n",
    "print(\"Classification Report (Train):\")\n",
    "print(classification_report(y_train, y_train_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "212efa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ■ 8.2：ROC 曲線および Precision–Recall 曲線 (Train データ)\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# 学習データ上での確率予測\n",
    "y_scores = best_lr.predict_proba(X_train_processed)[:, 1]\n",
    "\n",
    "# ROC 曲線\n",
    "fpr, tpr, _ = roc_curve(y_train, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (Train)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Precision–Recall 曲線\n",
    "precision, recall, _ = precision_recall_curve(y_train, y_scores)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall Curve (Train)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bf0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step 3.1：追加特徴量の設計\n",
    "import numpy as np\n",
    "\n",
    "# 対数変換済み変数の再計算\n",
    "for col in ['GrossApproval','SBAGuaranteedApproval','JobsSupported']:\n",
    "    train_df[col+'_log1p'] = np.log1p(train_df[col])\n",
    "    test_df [col+'_log1p'] = np.log1p(test_df [col])\n",
    "\n",
    "# 比率特徴量と交互作用項の追加\n",
    "train_df['ratio1']    = train_df['SBAGuaranteedApproval'] / train_df['GrossApproval']\n",
    "train_df['interact1'] = train_df['TermInMonths'] * train_df['InitialInterestRate']\n",
    "test_df ['ratio1']    = test_df ['SBAGuaranteedApproval'] / test_df ['GrossApproval']\n",
    "test_df ['interact1'] = test_df ['TermInMonths'] * test_df ['InitialInterestRate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868ba87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step 3.2：カテゴリ変数を category 型に変換\n",
    "cat_cols = [\n",
    "    'Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "    'BusinessType','BusinessAge','CollateralInd'\n",
    "]\n",
    "for c in cat_cols:\n",
    "    train_df[c] = train_df[c].astype('category')\n",
    "    test_df [c] = test_df [c].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e4dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step 3.3：特徴量リストと目的変数の準備\n",
    "features = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ApprovalFiscalYear','InitialInterestRate',\n",
    "    'TermInMonths','CongressionalDistrict','RevolverStatus'\n",
    "] + cat_cols\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['LoanStatus']\n",
    "X_test  = test_df [features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81bef680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (from lightgbm) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ■ Step 3.4a：lightgbm をインストール\n",
    "%pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd1c35a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM CV F1 scores: [0.60759494 0.60194175 0.58031088 0.61047836 0.62559242]\n",
      "Mean CV F1: 0.6051836684162373\n",
      "CV F1 mean: 0.6169  (std: 0.0159)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step 3.4b：モデル定義と 5分割 CV による F1 評価\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    class_weight='balanced',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    model, X_train, y_train,\n",
    "    cv=5, scoring='f1', n_jobs=-1\n",
    ")\n",
    "print(\"LightGBM CV F1 scores:\", cv_scores)\n",
    "print(\"Mean CV F1:\", cv_scores.mean())\n",
    "\n",
    "# ── ローカル CV による F1 推定 ──\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "f1_scores = cross_val_score(\n",
    "    model,\n",
    "    train_df[features_fe],\n",
    "    train_df['LoanStatus'],\n",
    "    cv=cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"CV F1 mean: {f1_scores.mean():.4f}  (std: {f1_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6372c6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[LightGBM] [Info] Number of positive: 964, number of negative: 6588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1720\n",
      "[LightGBM] [Info] Number of data points in the train set: 7552, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Best params : {'learning_rate': np.float64(0.01402150923749871), 'max_depth': 11, 'min_child_samples': 14, 'n_estimators': 386, 'num_leaves': 55}\n",
      "Best CV F1  : 0.6125178434787305\n"
     ]
    }
   ],
   "source": [
    "# ■ Step 4（統合版）：LightGBM ハイパーパラメータ探索＋最適モデル取得\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 1. 探索空間の定義\n",
    "param_dist = {\n",
    "    'num_leaves': randint(31, 256),\n",
    "    'max_depth': randint(3, 12),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'min_child_samples': randint(1, 20)\n",
    "}\n",
    "\n",
    "# 2. RandomizedSearchCV の定義\n",
    "rs_lgb = RandomizedSearchCV(\n",
    "    estimator=LGBMClassifier(class_weight='balanced', random_state=0),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# 3. 探索の実行\n",
    "rs_lgb.fit(X_train, y_train)\n",
    "\n",
    "# 4. 結果表示と最適モデル抽出\n",
    "print(\"Best params :\", rs_lgb.best_params_)\n",
    "print(\"Best CV F1  :\", rs_lgb.best_score_)\n",
    "best_lgb = rs_lgb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f0a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 964, number of negative: 6588\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1720\n",
      "[LightGBM] [Info] Number of data points in the train set: 7552, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(class_weight='balanced',\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ■ Step 5.1：最適モデルによる全学習データ再学習\n",
    "best_lgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d54ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No improvement CV F1=0.6169 ≤ 0.6800\n"
     ]
    }
   ],
   "source": [
    "# ── 条件付き提出セル（修正版） ──\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# (A) 提出用 ID を sample_submit.csv から取得\n",
    "DATA_DIR   = Path(r\"G:\\マイドライブ\\MUFJ_competition_2025\\data\")\n",
    "sample_df  = pd.read_csv(\n",
    "    DATA_DIR/\"sample_submit.csv\",\n",
    "    header=None,\n",
    "    names=[\"id\",\"LoanStatus\"],\n",
    "    sep=r\"\\s+\"\n",
    ")\n",
    "test_ids   = sample_df[\"id\"]\n",
    "\n",
    "# (B) 条件判定\n",
    "best_f1_est = 0.670   # これまでのベスト推定F1\n",
    "margin      = 0.01    # 最低更新幅\n",
    "mean_f1     = f1_scores.mean()\n",
    "\n",
    "if mean_f1 > best_f1_est + margin:\n",
    "    # (C) 全データで再学習＆予測\n",
    "    best_lgb.fit(train_df[features_fe], train_df['LoanStatus'])\n",
    "    y_pred = best_lgb.predict(test_df[features_fe])\n",
    "    # (D) 提出ファイル作成（ヘッダーなし）\n",
    "    pd.DataFrame({\"id\": test_ids, \"LoanStatus\": y_pred})\\\n",
    "      .to_csv(\"submission.csv\", index=False, header=False)\n",
    "    print(f\"✅ Improved CV F1={mean_f1:.4f}, submission.csv を出力しました\")\n",
    "else:\n",
    "    print(f\"❌ No improvement CV F1={mean_f1:.4f} ≤ {best_f1_est+margin:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df840cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習用: (6041, 16) (6041,)\n",
      "検証用: (1511, 16) (1511,)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step H1：訓練データを再分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"学習用:\", X_tr.shape, y_tr.shape)\n",
    "print(\"検証用:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b623ccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1700\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(class_weight='balanced',\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ■ Step H2：best_lgb で再学習\n",
    "# ※ Step 4.2 を実行済みで best_lgb が定義されている想定\n",
    "best_lgb.fit(X_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "916e016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold-out F1 Score: 0.681\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.969     0.920     0.944      1318\n",
      "           1      0.595     0.798     0.681       193\n",
      "\n",
      "    accuracy                          0.905      1511\n",
      "   macro avg      0.782     0.859     0.813      1511\n",
      "weighted avg      0.921     0.905     0.910      1511\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 30495 (\\N{CJK UNIFIED IDEOGRAPH-771F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12521 (\\N{KATAKANA LETTER RA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12505 (\\N{KATAKANA LETTER BE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12523 (\\N{KATAKANA LETTER RU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 20104 (\\N{CJK UNIFIED IDEOGRAPH-4E88}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 28204 (\\N{CJK UNIFIED IDEOGRAPH-6E2C}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ■ Step H3：検証用データへの予測と評価指標算出\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 予測\n",
    "y_val_pred = best_lgb.predict(X_val)\n",
    "\n",
    "# F1 スコア\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "print(f\"Hold-out F1 Score: {f1:.3f}\")\n",
    "\n",
    "# 分類レポート\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n",
    "\n",
    "# 混同行列の可視化\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"予測ラベル\")\n",
    "plt.ylabel(\"真のラベル\")\n",
    "plt.title(\"Confusion Matrix (Validation)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73589e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 最適閾値 t*: 0.518\n",
      "→ 閾値適用後 F1 (Validation): 0.682\n"
     ]
    }
   ],
   "source": [
    "# ■ Step H4：Hold-out 検証データでの閾値最適化\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 1. 検証データ上の確率予測\n",
    "y_val_scores = best_lgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 2. Precision–Recall 曲線から閾値一覧を取得\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_scores)\n",
    "\n",
    "# 3. 各閾値での F1-score を計算\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# 4. 最適閾値と対応 F1 を抽出\n",
    "ix = np.nanargmax(f1_scores[:-1])\n",
    "best_thresh = thresholds[ix]\n",
    "best_f1_val = f1_scores[ix]\n",
    "\n",
    "print(f\"→ 最適閾値 t*: {best_thresh:.3f}\")\n",
    "print(f\"→ 閾値適用後 F1 (Validation): {best_f1_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "840d3490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ratio1  interact1    ratio2  interact2\n",
      "0  0.308391    1001.28  0.118156   8.262314\n",
      "1  0.800355     856.80  0.112185   5.822436\n",
      "2  0.324869     514.08  0.100058  18.930355\n",
      "3  0.608446     377.60  0.195918   7.031119\n",
      "4  0.403468    1563.00  0.048409   9.335067\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.1：ratio1, interact1 と ratio2, interact2 をまとめて再計算\n",
    "import numpy as np\n",
    "\n",
    "# 既存の対数変換済み変数がある前提\n",
    "# ratio1 = SBAGuaranteedApproval / GrossApproval\n",
    "# interact1 = TermInMonths * InitialInterestRate\n",
    "train_df['ratio1']    = train_df['SBAGuaranteedApproval'] / train_df['GrossApproval']\n",
    "train_df['interact1'] = train_df['TermInMonths'] * train_df['InitialInterestRate']\n",
    "\n",
    "test_df ['ratio1']    = test_df ['SBAGuaranteedApproval'] / test_df ['GrossApproval']\n",
    "test_df ['interact1'] = test_df ['TermInMonths'] * test_df ['InitialInterestRate']\n",
    "\n",
    "# 新規特徴量\n",
    "# ratio2 = GrossApproval_log1p / (TermInMonths + 1)\n",
    "# interact2 = JobsSupported_log1p * InitialInterestRate\n",
    "train_df['ratio2']    = train_df['GrossApproval_log1p'] / (train_df['TermInMonths'] + 1)\n",
    "train_df['interact2'] = train_df['JobsSupported_log1p'] * train_df['InitialInterestRate']\n",
    "\n",
    "test_df ['ratio2']    = test_df ['GrossApproval_log1p'] / (test_df ['TermInMonths'] + 1)\n",
    "test_df ['interact2'] = test_df ['JobsSupported_log1p'] * test_df ['InitialInterestRate']\n",
    "\n",
    "# 確認\n",
    "print(train_df[['ratio1','interact1','ratio2','interact2']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03b55733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X の形状: (7552, 18)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.2：features リストに ratio2, interact2 を追加\n",
    "cat_cols = [\n",
    "    'Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "    'BusinessType','BusinessAge','CollateralInd'\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate',\n",
    "    'TermInMonths','CongressionalDistrict','RevolverStatus'\n",
    "] + cat_cols\n",
    "\n",
    "# ■ 学習用／検証用特徴量行列と目的変数ベクトル\n",
    "X = train_df[features]\n",
    "y = train_df['LoanStatus']\n",
    "X_test_feat = test_df[features]\n",
    "\n",
    "print(\"X の形状:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8908da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習用: (6041, 18) (6041,)\n",
      "検証用: (1511, 18) (1511,)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.3：学習データを Hold-out 分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"学習用:\", X_tr.shape, y_tr.shape)\n",
    "print(\"検証用:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfef25b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2210\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "→ Hold-out F1 (Milestone 1): 0.667\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.4：best_lgb で再学習し、Hold-out F1 を算出\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 再学習\n",
    "best_lgb.fit(X_tr, y_tr)\n",
    "\n",
    "# 検証データ予測\n",
    "y_val_pred = best_lgb.predict(X_val)\n",
    "\n",
    "# Hold-out F1 の計算\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "print(f\"→ Hold-out F1 (Milestone 1): {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc876705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step M1.5a：RandomizedSearchCV による M1 モデルの最適化\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 探索パラメータ空間（M1 用）\n",
    "param_dist_m1 = {\n",
    "    'num_leaves': randint(20, 200),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(100, 800),\n",
    "    'min_child_samples': randint(5, 300)\n",
    "}\n",
    "\n",
    "rs_m1 = RandomizedSearchCV(\n",
    "    estimator=LGBMClassifier(class_weight='balanced', random_state=0),\n",
    "    param_distributions=param_dist_m1,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d2eeb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2210\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best params (M1): {'learning_rate': np.float64(0.12995829151457663), 'max_depth': 14, 'min_child_samples': 192, 'n_estimators': 289, 'num_leaves': 194}\n",
      "Best CV F1 (M1): 0.6181271202761515\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.5b：ハイパーパラメータ探索の実行\n",
    "rs_m1.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"Best params (M1):\", rs_m1.best_params_)\n",
    "print(\"Best CV F1 (M1):\", rs_m1.best_score_)\n",
    "\n",
    "# 最適モデルを再定義\n",
    "best_lgb_m1 = rs_m1.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af5c65e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2210\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "→ Hold-out F1 (M1.5): 0.620\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M1.5c：Hold-out データで再評価\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 再学習\n",
    "best_lgb_m1.fit(X_tr, y_tr)\n",
    "\n",
    "# 検証予測\n",
    "y_val_pred_m1 = best_lgb_m1.predict(X_val)\n",
    "f1_m1 = f1_score(y_val, y_val_pred_m1)\n",
    "\n",
    "print(f\"→ Hold-out F1 (M1.5): {f1_m1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68c553d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練用: (6041, 18) (6041,)\n",
      "検証用: (1511, 18) (1511,)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.0：特徴量再計算と Hold-out 分割\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. ratio1, interact1, ratio2, interact2 を再計算 ---\n",
    "train_df['ratio1']    = train_df['SBAGuaranteedApproval'] / train_df['GrossApproval']\n",
    "train_df['interact1'] = train_df['TermInMonths'] * train_df['InitialInterestRate']\n",
    "train_df['ratio2']    = train_df['GrossApproval_log1p'] / (train_df['TermInMonths'] + 1)\n",
    "train_df['interact2'] = train_df['JobsSupported_log1p'] * train_df['InitialInterestRate']\n",
    "\n",
    "test_df ['ratio1']    = test_df ['SBAGuaranteedApproval'] / test_df ['GrossApproval']\n",
    "test_df ['interact1'] = test_df ['TermInMonths'] * test_df ['InitialInterestRate']\n",
    "test_df ['ratio2']    = test_df ['GrossApproval_log1p'] / (test_df ['TermInMonths'] + 1)\n",
    "test_df ['interact2'] = test_df ['JobsSupported_log1p'] * test_df ['InitialInterestRate']\n",
    "\n",
    "# --- 2. 特徴量リストの定義 ---\n",
    "cat_cols = [\n",
    "    'Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "    'BusinessType','BusinessAge','CollateralInd'\n",
    "]\n",
    "features = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate',\n",
    "    'TermInMonths','CongressionalDistrict','RevolverStatus'\n",
    "] + cat_cols\n",
    "\n",
    "# --- 3. 学習データの特徴量行列と目的変数ベクトル ---\n",
    "X = train_df[features]\n",
    "y = train_df['LoanStatus']\n",
    "\n",
    "# --- 4. Hold-out 分割 ---\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"訓練用:\", X_tr.shape, y_tr.shape)\n",
    "print(\"検証用:\", X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "362dd606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (from xgboost) (2.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.1：xgboost をインストール\n",
    "%pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95e585ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "符号化後の dtypes:\n",
      " float64    8\n",
      "int8       6\n",
      "int64      4\n",
      "Name: count, dtype: int64\n",
      "✅ X_tr_xgb/X_val_xgb のカテゴリコード化完了\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.2a：XGBoost 用にカテゴリ変数をコード化\n",
    "X_tr_xgb = X_tr.copy()\n",
    "X_val_xgb = X_val.copy()\n",
    "for c in cat_cols:\n",
    "    X_tr_xgb[c]  = X_tr_xgb[c].cat.codes\n",
    "    X_val_xgb[c] = X_val_xgb[c].cat.codes\n",
    "print(\"符号化後の dtypes:\\n\", X_tr_xgb.dtypes.value_counts())\n",
    "\n",
    "# ■ XGBoost 用のカテゴリ列コード化（共通関数版）\n",
    "X_tr_xgb  = encode_categories(X_tr_xgb,  cat_cols)\n",
    "X_val_xgb = encode_categories(X_val_xgb, cat_cols)\n",
    "print(\"✅ X_tr_xgb/X_val_xgb のカテゴリコード化完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fa365c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:30:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Hold-out F1 : 0.5945945945945946\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.2b：XGBoost モデルの学習\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb.fit(X_tr_xgb, y_tr)\n",
    "p_xgb_val = xgb.predict_proba(X_val_xgb)[:, 1]\n",
    "y_xgb_pred = (p_xgb_val >= 0.5).astype(int)\n",
    "print(\"XGB Hold-out F1 :\", f1_score(y_val, y_xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9259d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step M2.2c：best_lgb を再定義（RandomizedSearchCV の結果から）\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "\n",
    "# ■ Step M2.2d：Hold-out 用データ X_val が定義されていなければ再定義\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# （features と train_df, 既に計算済みの新特徴量を前提）\n",
    "X = train_df[features]\n",
    "y = train_df['LoanStatus']\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9a919f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2210\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LGBMClassifier</div></div><div><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(class_weight='balanced',\n",
       "               learning_rate=np.float64(0.01402150923749871), max_depth=11,\n",
       "               min_child_samples=14, n_estimators=386, num_leaves=55,\n",
       "               random_state=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ■ Step M2.2c：best_lgb を新特徴量で再学習\n",
    "# これで best_lgb が 18次元入力に対応します\n",
    "best_lgb.fit(X_tr, y_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5080f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Optimal weight w*: 0.70\n",
      "→ Soft-ensemble Hold-out F1: 0.697\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.3：LGB + XGB のソフトアンサンブル重み探索\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. LightGBM の確率予測（Hold-out データ）\n",
    "p_lgb_val = best_lgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 2. XGBoost の確率予測（既に p_xgb_val を計算済みであればここは不要）\n",
    "#    もし定義が消えていたら再度計算：\n",
    "# from xgboost import XGBClassifier\n",
    "# xgb = XGBClassifier(n_estimators=200, learning_rate=0.05, eval_metric='logloss', random_state=0)\n",
    "# xgb.fit(X_tr_xgb, y_tr)\n",
    "# p_xgb_val = xgb.predict_proba(X_val_xgb)[:, 1]\n",
    "\n",
    "best_w, best_f1 = 0.0, 0.0\n",
    "# w は LGB の重み、(1−w) が XGB の重み\n",
    "for w in np.linspace(0, 1, 21):\n",
    "    p_ens = w * p_lgb_val + (1 - w) * p_xgb_val\n",
    "    y_ens = (p_ens >= 0.5).astype(int)\n",
    "    f1 = f1_score(y_val, y_ens)\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_w = f1, w\n",
    "\n",
    "print(f\"→ Optimal weight w*: {best_w:.2f}\")\n",
    "print(f\"→ Soft-ensemble Hold-out F1: {best_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6abf3e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Optimal threshold t*: 0.437\n",
      "→ Threshold-optimized Hold-out F1: 0.692\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.4：ソフトアンサンブルの閾値最適化\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# 1. LightGBM と XGBoost の確率予測（Hold-out データ）\n",
    "p_lgb_val = best_lgb.predict_proba(X_val)[:, 1]\n",
    "p_xgb_val = xgb.predict_proba(X_val_xgb)[:, 1]\n",
    "\n",
    "# 2. 重み w* = 0.50 でアンサンブル確率を計算\n",
    "p_ens_val = 0.50 * p_lgb_val + 0.50 * p_xgb_val\n",
    "\n",
    "# 3. Precision–Recall 曲線から全閾値を取得\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, p_ens_val)\n",
    "\n",
    "# 4. 各閾値での F1-score を計算\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# thresholds の長さは f1_scores の長さマイナス1\n",
    "ix = np.nanargmax(f1_scores[:-1])\n",
    "\n",
    "# 5. 最適閾値と対応 F1-score を表示\n",
    "best_t_ens = thresholds[ix]\n",
    "best_f1_ens = f1_scores[ix]\n",
    "\n",
    "print(f\"→ Optimal threshold t*: {best_t_ens:.3f}\")\n",
    "print(f\"→ Threshold-optimized Hold-out F1: {best_f1_ens:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3245f66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Optimal (w, t): (0.70, 0.494)\n",
      "→ Joint-optimized Hold-out F1: 0.700\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.5：重み w と閾値 t の同時グリッドサーチ\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# 1. Hold-out データでの確率予測（再定義が必要なら再計算）\n",
    "p_lgb_val = best_lgb.predict_proba(X_val)[:, 1]\n",
    "p_xgb_val = xgb.predict_proba(X_val_xgb)[:, 1]\n",
    "\n",
    "# 2. グリッドの設定\n",
    "w_list = np.linspace(0, 1, 11)    # LGB の重みを 0.0～1.0 で 11 点\n",
    "best_w, best_t, best_f1 = 0, 0, 0\n",
    "\n",
    "# 3. 二重ループで w と t を探索\n",
    "for w in w_list:\n",
    "    p_ens = w * p_lgb_val + (1 - w) * p_xgb_val\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, p_ens)\n",
    "    # thresholds[i] に対応する F1 は f1_scores[i]\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "    for i, t in enumerate(thresholds):\n",
    "        if f1_scores[i] > best_f1:\n",
    "            best_f1 = f1_scores[i]\n",
    "            best_w  = w\n",
    "            best_t  = t\n",
    "\n",
    "print(f\"→ Optimal (w, t): ({best_w:.2f}, {best_t:.3f})\")\n",
    "print(f\"→ Joint-optimized Hold-out F1: {best_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60ad6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (for LGB)     : (7552, 18)\n",
      "X_test_xgb shape (for XGB) : (7552, 18)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step M2.5a：テストデータの特徴量行列準備（18次元版）\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 既存の test_df に対し、ratio1, interact1, ratio2, interact2 を再計算\n",
    "test_df['ratio1']    = test_df['SBAGuaranteedApproval'] / test_df['GrossApproval']\n",
    "test_df['interact1'] = test_df['TermInMonths'] * test_df['InitialInterestRate']\n",
    "test_df['ratio2']    = test_df['GrossApproval_log1p'] / (test_df['TermInMonths'] + 1)\n",
    "test_df['interact2'] = test_df['JobsSupported_log1p'] * test_df['InitialInterestRate']\n",
    "\n",
    "# 2. features リストで指定した順序に合わせる\n",
    "X_test = test_df[features]\n",
    "\n",
    "# 3. XGBoost 用にカテゴリを符号化\n",
    "X_test_xgb = X_test.copy()\n",
    "for c in cat_cols:\n",
    "    X_test_xgb[c] = X_test_xgb[c].cat.codes\n",
    "\n",
    "# 4. 確認\n",
    "print(\"X_test shape (for LGB)     :\", X_test.shape)\n",
    "print(\"X_test_xgb shape (for XGB) :\", X_test_xgb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e2974d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ OOF 最適閾値 t_oof = 0.593\n",
      "→ OOF F1 Score   = 0.624\n"
     ]
    }
   ],
   "source": [
    "# ■ Step C1：Out-of-Fold 予測による F1 推定\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. アンサンブル Predict_proba 関数の定義\n",
    "def ens_predict_proba(X):\n",
    "    p1 = best_lgb.predict_proba(X)[:,1]\n",
    "    # XGBoost 用に符号化\n",
    "    X_xgb = X.copy()\n",
    "    for c in cat_cols:\n",
    "        X_xgb[c] = X_xgb[c].cat.codes\n",
    "    p2 = xgb.predict_proba(X_xgb)[:,1]\n",
    "    # 重み w*=0.10 を適用\n",
    "    return 0.10*p1 + 0.90*p2\n",
    "\n",
    "# 2. StratifiedKFold で OOF 予測を取得\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# cross_val_predict による Probability 出力\n",
    "oof_probs = cross_val_predict(\n",
    "    estimator=best_lgb,  # ダミー、predict_proba は ens_predict_proba のみ使用\n",
    "    X=X, y=y,\n",
    "    cv=kf,\n",
    "    method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")[:,1]\n",
    "\n",
    "# 3. 最適閾値 t* の再探索（OOF 上）\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y, oof_probs)\n",
    "f1_scores = 2*(precision*recall)/(precision+recall)\n",
    "ix = np.nanargmax(f1_scores[:-1])\n",
    "t_oof = thresholds[ix]\n",
    "f1_oof = f1_scores[ix]\n",
    "\n",
    "print(f\"→ OOF 最適閾値 t_oof = {t_oof:.3f}\")\n",
    "print(f\"→ OOF F1 Score   = {f1_oof:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee209e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested CV F1 scores: [0.60542797 0.59130435 0.60510806 0.62114537 0.63716814]\n",
      "Nested CV Mean F1: 0.6120307787651956\n",
      "Nested CV Std   F1: 0.01572303358983776\n"
     ]
    }
   ],
   "source": [
    "# ■ Step C2：Nested CV による F1 推定\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# パイプラインにして CV 内でチューニングを実施\n",
    "pipe = Pipeline([\n",
    "    ('model', LGBMClassifier(class_weight='balanced', random_state=0))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'model__num_leaves': randint(20, 150),\n",
    "    'model__max_depth': randint(3, 12),\n",
    "    'model__learning_rate': uniform(0.01, 0.2),\n",
    "    'model__n_estimators': randint(100, 500)\n",
    "}\n",
    "\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "nested_cv = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=inner_cv,\n",
    "    scoring='f1',\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 外側 CV でのスコア取得\n",
    "nested_scores = cross_val_score(\n",
    "    nested_cv, X, y,\n",
    "    cv=outer_cv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Nested CV F1 scores:\", nested_scores)\n",
    "print(\"Nested CV Mean F1:\", nested_scores.mean())\n",
    "print(\"Nested CV Std   F1:\", nested_scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ce1d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% CI for F1: [0.603, 0.648]\n"
     ]
    }
   ],
   "source": [
    "# ■ Step C3：ブートストラップによる F1 信頼区間\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "n_boot = 100\n",
    "boot_scores = []\n",
    "\n",
    "for i in range(n_boot):\n",
    "    idx = np.random.choice(len(y), len(y), replace=True)\n",
    "    score = f1_score(y.iloc[idx], oof_probs[idx] >= t_oof)\n",
    "    boot_scores.append(score)\n",
    "\n",
    "lower = np.percentile(boot_scores, 2.5)\n",
    "upper = np.percentile(boot_scores, 97.5)\n",
    "print(f\"Bootstrap 95% CI for F1: [{lower:.3f}, {upper:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19f57d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE 特徴量サンプル：\n",
      "  Subprogram_te NaicsSector_te BusinessAge_te BusinessType_te\n",
      "0      0.127054       0.124487       0.072387        0.136788\n",
      "1      0.127054       0.185378       0.196364        0.136788\n",
      "2      0.032645       0.147825       0.109507        0.136788\n",
      "3      0.137690       0.050735       0.109507        0.136788\n",
      "4      0.137690       0.142535       0.120265        0.136788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koshihiramatsu\\AppData\\Local\\Temp\\ipykernel_8948\\2100055917.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = train_df.groupby(col)['LoanStatus'].agg(['count','mean'])\n",
      "C:\\Users\\koshihiramatsu\\AppData\\Local\\Temp\\ipykernel_8948\\2100055917.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = train_df.groupby(col)['LoanStatus'].agg(['count','mean'])\n",
      "C:\\Users\\koshihiramatsu\\AppData\\Local\\Temp\\ipykernel_8948\\2100055917.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = train_df.groupby(col)['LoanStatus'].agg(['count','mean'])\n",
      "C:\\Users\\koshihiramatsu\\AppData\\Local\\Temp\\ipykernel_8948\\2100055917.py:13: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  agg = train_df.groupby(col)['LoanStatus'].agg(['count','mean'])\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F1.1（修正版）：Target Encoding の計算と適用\n",
    "import numpy as np\n",
    "\n",
    "# 平滑化パラメータと全体平均\n",
    "k_smooth = 10\n",
    "global_mean = train_df['LoanStatus'].mean()\n",
    "\n",
    "# 対象カテゴリ変数\n",
    "te_cols = ['Subprogram','NaicsSector','BusinessAge','BusinessType']\n",
    "\n",
    "for col in te_cols:\n",
    "    # 1. グループごとの件数と平均を取得\n",
    "    agg = train_df.groupby(col)['LoanStatus'].agg(['count','mean'])\n",
    "    counts = agg['count']\n",
    "    means  = agg['mean']\n",
    "    # 2. 平滑化 TE を計算\n",
    "    smooth = (means * counts + global_mean * k_smooth) / (counts + k_smooth)\n",
    "    # 3. train にマッピング\n",
    "    train_df[f'{col}_te'] = train_df[col].map(smooth)\n",
    "    # 4. test にマッピング（astype(float) → fillna）\n",
    "    mapped = test_df[col].map(smooth).astype(float)\n",
    "    test_df[f'{col}_te'] = mapped.fillna(global_mean)\n",
    "\n",
    "print(\"TE 特徴量サンプル：\")\n",
    "print(train_df[[f'{col}_te' for col in te_cols]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bc106eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量数: 22\n",
      "訓練用: (6041, 22) 検証用: (1511, 22)\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F1.2：features リストに TE を追加し、Hold-out 分割を実行\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 既存の num／cat／interaction 特徴量リスト\n",
    "features = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate',\n",
    "    'TermInMonths','CongressionalDistrict','RevolverStatus'\n",
    "] + cat_cols  # 元のカテゴリ列\n",
    "\n",
    "# 追加する TE 列\n",
    "features += [f'{col}_te' for col in te_cols]\n",
    "\n",
    "# 特徴量行列と目的変数\n",
    "X = train_df[features]\n",
    "y = train_df['LoanStatus']\n",
    "\n",
    "# Hold-out 分割\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(\"特徴量数:\", len(features))\n",
    "print(\"訓練用:\", X_tr.shape, \"検証用:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "637066e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step F1.2a：best_lgb を再定義（RandomizedSearchCV の結果から）\n",
    "# 前提：Step 4 統合セルで rs_lgb, best_lgb を定義済み\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "\n",
    "# または、自前でモデルハイパーパラメータを指定して再定義する場合\n",
    "# from lightgbm import LGBMClassifier\n",
    "# best_lgb = LGBMClassifier(\n",
    "#     n_estimators=191,\n",
    "#     max_depth=5,\n",
    "#     num_leaves=105,\n",
    "#     learning_rate=0.1579,\n",
    "#     min_child_samples=116,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d78bc5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2248\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "→ Hold-out F1 (with TE): 0.667\n",
      "\n",
      "■ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.966     0.917     0.941      1318\n",
      "           1      0.581     0.782     0.667       193\n",
      "\n",
      "    accuracy                          0.900      1511\n",
      "   macro avg      0.774     0.850     0.804      1511\n",
      "weighted avg      0.917     0.900     0.906      1511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F1.3：再定義した best_lgb で再学習し、Hold-out F1 を算出\n",
    "\n",
    "# 1. 最適モデルを再取得\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "\n",
    "# 2. 再学習\n",
    "best_lgb.fit(X_tr, y_tr)\n",
    "\n",
    "# 3. 検証データで予測\n",
    "y_val_pred = best_lgb.predict(X_val)\n",
    "\n",
    "# 4. 評価指標の算出\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "print(f\"→ Hold-out F1 (with TE): {f1:.3f}\\n\")\n",
    "print(\"■ Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "580827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ■ Step F1.3a：最適 LightGBM モデルを再取得\n",
    "best_lgb = rs_lgb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e348795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ TE モデル最適閾値 t*: 0.582\n",
      "→ Threshold-optimized Hold-out F1 (TE): 0.682\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F1.4：Hold-out データでの閾値最適化（TE モデル）\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# 1. 検証データでの確率予測\n",
    "y_val_probs = best_lgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# 2. Precision–Recall 曲線を計算\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_probs)\n",
    "\n",
    "# 3. 各閾値での F1-score を計算\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# 4. 最適閾値と対応する F1 を抽出\n",
    "ix = np.nanargmax(f1_scores[:-1])\n",
    "best_thresh_te = thresholds[ix]\n",
    "best_f1_te     = f1_scores[ix]\n",
    "\n",
    "print(f\"→ TE モデル最適閾値 t*: {best_thresh_te:.3f}\")\n",
    "print(f\"→ Threshold-optimized Hold-out F1 (TE): {best_f1_te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "375bf4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Encoding 特徴量サンプル：\n",
      "   Subprogram_fe  NaicsSector_fe  BusinessAge_fe  BusinessType_fe\n",
      "0       0.459613        0.056409        0.046743         0.893406\n",
      "1       0.459613        0.086732        0.163400         0.893406\n",
      "2       0.016022        0.158369        0.370233         0.893406\n",
      "3       0.434587        0.012447        0.370233         0.893406\n",
      "4       0.434587        0.061176        0.336997         0.893406\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F2.1：Frequency Encoding の計算と適用（修正版）\n",
    "freq_cols = ['Subprogram', 'NaicsSector', 'BusinessAge', 'BusinessType']\n",
    "\n",
    "for col in freq_cols:\n",
    "    freq = train_df[col].value_counts(normalize=True)\n",
    "    # train 側：一度 float にしてから fillna\n",
    "    train_df[f'{col}_fe'] = train_df[col].map(freq).astype(float).fillna(0.0)\n",
    "    # test 側：同様に cast → fillna\n",
    "    test_df [f'{col}_fe'] = test_df [col].map(freq).astype(float).fillna(0.0)\n",
    "\n",
    "# 結果確認\n",
    "print(\"Frequency Encoding 特徴量サンプル：\")\n",
    "print(train_df[[f'{col}_fe' for col in freq_cols]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e3916d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000607 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2287\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "→ Hold‐out F1 (with FE): 0.664\n",
      "\n",
      "■ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.919     0.941      1318\n",
      "           1      0.582     0.772     0.664       193\n",
      "\n",
      "    accuracy                          0.900      1511\n",
      "   macro avg      0.773     0.845     0.803      1511\n",
      "weighted avg      0.916     0.900     0.906      1511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F2.2：Hold‐out 分割 → モデル再学習 → Hold‐out F1 算出 (FE 特徴量含む)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1. 特徴量リストに FE 列を追加\n",
    "#    事前に定義済み：features, freq_cols\n",
    "features_fe = features + [f'{col}_fe' for col in freq_cols]\n",
    "\n",
    "# 2. Hold‐out 分割\n",
    "X = train_df[features_fe]\n",
    "y = train_df['LoanStatus']\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# 3. モデル再定義・再学習\n",
    "#    rs_lgb = RandomizedSearchCV(..., estimator=LGBMClassifier(...)) を既に実行済み\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "best_lgb.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Hold‐out 上での評価\n",
    "y_val_pred = best_lgb.predict(X_val)\n",
    "f1_fe = f1_score(y_val, y_val_pred)\n",
    "print(f\"→ Hold‐out F1 (with FE): {f1_fe:.3f}\\n\")\n",
    "print(\"■ Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5491058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 771, number of negative: 5270\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2287\n",
      "[LightGBM] [Info] Number of data points in the train set: 6041, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "→ Hold‐out F1 (with FE): 0.664\n",
      "\n",
      "■ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.965     0.919     0.941      1318\n",
      "           1      0.582     0.772     0.664       193\n",
      "\n",
      "    accuracy                          0.900      1511\n",
      "   macro avg      0.773     0.845     0.803      1511\n",
      "weighted avg      0.916     0.900     0.906      1511\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ■ Step F2.2：Hold‐out 分割 → モデル再学習 → Hold‐out F1 算出 (FE 特徴量含む)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# 1. FE 列を features_fe に追加済みとする\n",
    "#    （features と freq_cols は事前定義済み）\n",
    "features_fe = features + [f'{col}_fe' for col in freq_cols]\n",
    "\n",
    "# 2. Hold‐out 分割\n",
    "X = train_df[features_fe]\n",
    "y = train_df['LoanStatus']\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "# 3. モデル再定義・再学習\n",
    "best_lgb = rs_lgb.best_estimator_\n",
    "best_lgb.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Hold‐out 上での評価\n",
    "y_val_pred = best_lgb.predict(X_val)\n",
    "f1_fe = f1_score(y_val, y_val_pred)\n",
    "print(f\"→ Hold‐out F1 (with FE): {f1_fe:.3f}\\n\")\n",
    "print(\"■ Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "276aedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>LoanStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7555</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  LoanStatus\n",
       "0  7553           0\n",
       "1  7554           0\n",
       "2  7555           0\n",
       "3  7556           0\n",
       "4  7557           1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ■ Step F2.3：テストデータ予測 → 提出ファイル（FE拡張版）生成\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ① テスト用 ID の読み込み\n",
    "test_ids = pd.read_csv(f\"{DATA_DIR}/test.csv\")['id']\n",
    "\n",
    "# ② Booster API で NumPy 配列を渡し確率予測\n",
    "proba_fe = best_lgb.booster_.predict(\n",
    "    test_df[features_fe].values,\n",
    "    num_iteration=best_lgb.best_iteration_\n",
    ")\n",
    "\n",
    "# ③ 0.5 しきい値で二値化\n",
    "y_pred_fe = (proba_fe >= 0.5).astype(int)\n",
    "\n",
    "# ④ 提出用 DataFrame 作成・CSV 出力（ヘッダー行を省略）\n",
    "submission_fe = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'LoanStatus': y_pred_fe\n",
    "})\n",
    "submission_fe.to_csv('submission_fixed.csv', index=False, header=False)\n",
    "\n",
    "# ⑤ 確認\n",
    "submission_fe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ee68a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⛔ v2セルはスキップ中（Optunaセルを使用）\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# === Aライン強化 v2：年度Grouped-OOF × 多シード × 交差TE(Year×Sector) 追加（無効化） ===\n",
    "# このセルは Optuna セルを使うためデフォルトでスキップします。\n",
    "RUN_V2 = False\n",
    "if not RUN_V2:\n",
    "    print(\"⛔ v2セルはスキップ中（Optunaセルを使用）\")\n",
    "    raise SystemExit  # 以降の処理を実行せず終了\n",
    "\n",
    "from pathlib import Path\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# ----- 出力先 & バージョン名 -----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_path = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ----- 依存（前処理セルの train_df/test_df / base_feats / cat_cols / te_cols を使う） -----\n",
    "train = train_df.copy()\n",
    "test  = test_df.copy()\n",
    "\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "if 'te_cols' not in globals():\n",
    "    te_cols  = ['Subprogram','NaicsSector','BusinessAge','BusinessType']\n",
    "\n",
    "y_all   = train['LoanStatus'].astype(int).reset_index(drop=True)\n",
    "groups  = train['ApprovalFiscalYear'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# ----- カテゴリ整数化（未知=-1） -----\n",
    "def fit_category_codes(train, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(train[c]).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_codes(df, maps):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c + \"_code\"] = df[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "code_maps  = fit_category_codes(train, cat_cols)\n",
    "train_code = apply_category_codes(train, code_maps).reset_index(drop=True)\n",
    "test_code  = apply_category_codes(test,  code_maps).reset_index(drop=True)\n",
    "\n",
    "# ----- 年×業種の交差カテゴリを追加（TE/FE用。元カテゴリのまま扱う） -----\n",
    "def make_year_sector(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['NaicsSector'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "train['YearSector'] = make_year_sector(train)\n",
    "test ['YearSector'] = make_year_sector(test)\n",
    "\n",
    "te_cols_local = te_cols + ['YearSector']  # 交差列を追加\n",
    "\n",
    "# ----- 年度グループ付きの分割 -----\n",
    "def make_splits(seed):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    return list(sgkf.split(train, y_all, groups=groups))\n",
    "\n",
    "# ----- OOFで TE/FE（与えた splits でリーク防止） -----\n",
    "def add_te_fe_oof_with_splits(train_df, y, test_df, cols, splits, k_smooth=10):\n",
    "    df = train_df.reset_index(drop=True).copy()\n",
    "    y  = pd.Series(y).reset_index(drop=True).rename('y')\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype('object')\n",
    "    test_df = test_df.reset_index(drop=True).copy()\n",
    "    for c in cols:\n",
    "        test_df[c] = test_df[c].astype('object')\n",
    "\n",
    "    te_train = pd.DataFrame(index=df.index)\n",
    "    fe_train = pd.DataFrame(index=df.index)\n",
    "    te_test  = pd.DataFrame(index=test_df.index)\n",
    "    fe_test  = pd.DataFrame(index=test_df.index)\n",
    "\n",
    "    gmean = float(y.mean())\n",
    "\n",
    "    for col in cols:\n",
    "        te_train[col + \"_teCV\"] = np.nan\n",
    "        fe_train[col + \"_feCV\"] = np.nan\n",
    "\n",
    "        for tr_idx, va_idx in splits:\n",
    "            tr_col = df[col].iloc[tr_idx].reset_index(drop=True)\n",
    "            yy_tr  = y.iloc[tr_idx].reset_index(drop=True)\n",
    "\n",
    "            agg = pd.DataFrame({'col': tr_col, 'y': yy_tr}).groupby('col')['y'].agg(['count','mean'])\n",
    "            smooth = (agg['mean']*agg['count'] + gmean*k_smooth) / (agg['count'] + k_smooth)\n",
    "\n",
    "            mapped = df[col].iloc[va_idx].map(smooth)\n",
    "            mapped = pd.Series(mapped.to_numpy(dtype='float64'), index=va_idx)\n",
    "            te_train.loc[va_idx, col + \"_teCV\"] = mapped.fillna(gmean).to_numpy()\n",
    "\n",
    "            freq = tr_col.value_counts(normalize=True)\n",
    "            mapped_f = df[col].iloc[va_idx].map(freq)\n",
    "            mapped_f = pd.Series(mapped_f.to_numpy(dtype='float64'), index=va_idx)\n",
    "            fe_train.loc[va_idx, col + \"_feCV\"] = mapped_f.fillna(0.0).to_numpy()\n",
    "\n",
    "        agg_full = pd.DataFrame({'col': df[col], 'y': y}).groupby('col')['y'].agg(['count','mean'])\n",
    "        smooth_f = (agg_full['mean']*agg_full['count'] + gmean*k_smooth) / (agg_full['count'] + k_smooth)\n",
    "        te_test[col + \"_teCV\"] = pd.Series(test_df[col].map(smooth_f).to_numpy(dtype='float64')).fillna(gmean).to_numpy()\n",
    "\n",
    "        freq_full = df[col].value_counts(normalize=True)\n",
    "        fe_test[col + \"_feCV\"] = pd.Series(test_df[col].map(freq_full).to_numpy(dtype='float64')).fillna(0.0).to_numpy()\n",
    "\n",
    "    return (pd.concat([df.reset_index(drop=True), te_train, fe_train], axis=1),\n",
    "            pd.concat([test_df.reset_index(drop=True), te_test, fe_test], axis=1))\n",
    "\n",
    "# ----- モデル設定（rs_lgbがあれば流用、なければ既定） -----\n",
    "if 'rs_lgb' in globals():\n",
    "    base_params = dict(rs_lgb.best_params_)\n",
    "    base_params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "else:\n",
    "    base_params = dict(\n",
    "        n_estimators=1200, learning_rate=0.03,\n",
    "        num_leaves=128, max_depth=10, min_child_samples=120,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        reg_lambda=1.0, reg_alpha=0.1,\n",
    "        class_weight='balanced', verbosity=-1\n",
    "    )\n",
    "\n",
    "# ----- 多シード反復 -----\n",
    "seeds = [0, 1, 2]\n",
    "oof_probas = []\n",
    "test_probas = []\n",
    "best_rounds_each_seed = []\n",
    "\n",
    "for seed in seeds:\n",
    "    splits = make_splits(seed)\n",
    "\n",
    "    # TE/FE をこの splits で作り直す（交差列入り）\n",
    "    train_tefe, test_tefe = add_te_fe_oof_with_splits(\n",
    "        train[te_cols_local], y_all, test[te_cols_local],\n",
    "        te_cols_local, splits, k_smooth=10\n",
    "    )\n",
    "\n",
    "    # 特徴行列（数値 + コード + TE/FE）\n",
    "    X_all = pd.concat([\n",
    "        train_code[base_feats],\n",
    "        train_code[[c+\"_code\" for c in cat_cols]],\n",
    "        train_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "        train_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "    ], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "    T_all = pd.concat([\n",
    "        test_code[base_feats],\n",
    "        test_code[[c+\"_code\" for c in cat_cols]],\n",
    "        test_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "        test_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "    ], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # OOF 予測（callbackで早期終了）\n",
    "    oof = np.zeros(len(X_all))\n",
    "    best_rounds = []\n",
    "\n",
    "    for tr_idx, va_idx in splits:\n",
    "        Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "        params = dict(base_params)\n",
    "        params['random_state'] = seed\n",
    "\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=0)]\n",
    "        )\n",
    "        oof[va_idx] = model.predict_proba(Xva, num_iteration=model.best_iteration_)[:, 1]\n",
    "        bi = getattr(model, \"best_iteration_\", None)\n",
    "        if bi is None:\n",
    "            bi = params.get('n_estimators', 1000)\n",
    "        best_rounds.append(int(bi))\n",
    "\n",
    "    oof_probas.append(oof)\n",
    "    best_rounds_each_seed.append(int(np.mean(best_rounds)))\n",
    "\n",
    "    # テスト予測（full fit、平均best_roundsを使う）\n",
    "    params_full = dict(base_params)\n",
    "    params_full['random_state'] = seed\n",
    "    params_full['n_estimators'] = best_rounds_each_seed[-1]\n",
    "    final = LGBMClassifier(**params_full)\n",
    "    final.fit(X_all, y_all)\n",
    "    test_probas.append(final.predict_proba(T_all)[:, 1])\n",
    "\n",
    "# ----- OOF平均でしきい値最適化 -----\n",
    "oof_mean = np.mean(oof_probas, axis=0)\n",
    "ths = np.linspace(0.10, 0.90, 161)\n",
    "f1s = [f1_score(y_all, (oof_mean >= t).astype(int)) for t in ths]\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_th  = float(ths[best_idx])\n",
    "f1_oof   = float(f1s[best_idx])\n",
    "\n",
    "# ----- テスト確率も平均して二値化 → 単一CSV出力（ヘッダー無し2列） -----\n",
    "test_mean = np.mean(test_probas, axis=0)\n",
    "test_pred = (test_mean >= best_th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "submission = pd.DataFrame({'id': ids, 'LoanStatus': test_pred})\n",
    "submission.to_csv(out_path, index=False, header=False)\n",
    "\n",
    "print(f\"🔎 OOF F1(grouped×seeds×crossTE)={f1_oof:.3f} | best_th={best_th:.3f} | best_rounds={best_rounds_each_seed}\")\n",
    "print(f\"✅ submission を保存: {out_path}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (grouped×seeds×crossTE): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Best threshold: {best_th:.6f}\\n\")\n",
    "    f.write(f\"Best rounds per seed: {best_rounds_each_seed}\\n\")\n",
    "    f.write(f\"Params: {base_params}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "952e1954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\koshihiramatsu\\anaconda3\\envs\\ds-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-08-09 17:49:56,875] A new study created in memory with name: lgbm_grouped_oof_f1\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[332]\tvalid_0's binary_logloss: 0.270402\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1157]\tvalid_0's binary_logloss: 0.227946\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[599]\tvalid_0's binary_logloss: 0.279958\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[511]\tvalid_0's binary_logloss: 0.297803\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1454]\tvalid_0's binary_logloss: 0.194602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.603047:   3%|▎         | 1/30 [00:04<02:15,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:01,536] Trial 0 finished with value: 0.6030465949820788 and parameters: {'k_smooth': 30, 'learning_rate': 0.04424664598342847, 'num_leaves': 166, 'max_depth': 9, 'min_child_samples': 90, 'subsample': 0.8583576452266624, 'colsample_bytree': 0.775034884505077, 'reg_alpha': 1.7835460015641595, 'reg_lambda': 4.818313802505147}. Best is trial 0 with value: 0.6030465949820788.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid_0's binary_logloss: 0.278039\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[995]\tvalid_0's binary_logloss: 0.234713\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[726]\tvalid_0's binary_logloss: 0.295721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[627]\tvalid_0's binary_logloss: 0.308454\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1060]\tvalid_0's binary_logloss: 0.21911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.603047:   7%|▋         | 2/30 [00:07<01:45,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:04,698] Trial 1 finished with value: 0.5975286849073257 and parameters: {'k_smooth': 22, 'learning_rate': 0.05187987898085556, 'num_leaves': 150, 'max_depth': 9, 'min_child_samples': 186, 'subsample': 0.6284144232791548, 'colsample_bytree': 0.6348517198806163, 'reg_alpha': 0.04043679488065144, 'reg_lambda': 4.16309922773969}. Best is trial 0 with value: 0.6030465949820788.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.270375\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[502]\tvalid_0's binary_logloss: 0.223418\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[391]\tvalid_0's binary_logloss: 0.283942\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[322]\tvalid_0's binary_logloss: 0.28748\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[978]\tvalid_0's binary_logloss: 0.196915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.607502:  10%|█         | 3/30 [00:10<01:30,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:07,529] Trial 2 finished with value: 0.6075022872827082 and parameters: {'k_smooth': 40, 'learning_rate': 0.061051910606000605, 'num_leaves': 251, 'max_depth': 11, 'min_child_samples': 98, 'subsample': 0.9122116705145822, 'colsample_bytree': 0.6473097703475733, 'reg_alpha': 1.2798420426550476, 'reg_lambda': 0.716766437045232}. Best is trial 2 with value: 0.6075022872827082.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[400]\tvalid_0's binary_logloss: 0.276595\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1119]\tvalid_0's binary_logloss: 0.240677\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[773]\tvalid_0's binary_logloss: 0.294366\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[840]\tvalid_0's binary_logloss: 0.301107\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1555]\tvalid_0's binary_logloss: 0.239941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.607502:  13%|█▎        | 4/30 [00:14<01:31,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:11,333] Trial 3 finished with value: 0.5998271391529818 and parameters: {'k_smooth': 48, 'learning_rate': 0.029598928806247413, 'num_leaves': 124, 'max_depth': 7, 'min_child_samples': 157, 'subsample': 0.7824601328866194, 'colsample_bytree': 0.8273735795474594, 'reg_alpha': 0.037579600872710284, 'reg_lambda': 3.0881774853793855}. Best is trial 2 with value: 0.6075022872827082.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid_0's binary_logloss: 0.274396\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[839]\tvalid_0's binary_logloss: 0.223514\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[568]\tvalid_0's binary_logloss: 0.285253\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[463]\tvalid_0's binary_logloss: 0.292562\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1112]\tvalid_0's binary_logloss: 0.20015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.609353:  17%|█▋        | 5/30 [00:18<01:34,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:15,553] Trial 4 finished with value: 0.6093528578176665 and parameters: {'k_smooth': 33, 'learning_rate': 0.03607006419219659, 'num_leaves': 243, 'max_depth': 10, 'min_child_samples': 78, 'subsample': 0.7748127815197365, 'colsample_bytree': 0.8790524783709059, 'reg_alpha': 0.12045094325853967, 'reg_lambda': 3.3338335772283383}. Best is trial 4 with value: 0.6093528578176665.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[697]\tvalid_0's binary_logloss: 0.26999\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's binary_logloss: 0.226936\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1868]\tvalid_0's binary_logloss: 0.282409\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1371]\tvalid_0's binary_logloss: 0.295214\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.21306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.609353:  20%|██        | 6/30 [00:25<01:52,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:22,036] Trial 5 finished with value: 0.6030013642564802 and parameters: {'k_smooth': 35, 'learning_rate': 0.015487965920269622, 'num_leaves': 60, 'max_depth': 7, 'min_child_samples': 79, 'subsample': 0.8280787081671519, 'colsample_bytree': 0.7754406053849281, 'reg_alpha': 1.9767476761184524, 'reg_lambda': 0.5102240537401403}. Best is trial 4 with value: 0.6093528578176665.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[900]\tvalid_0's binary_logloss: 0.27493\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1990]\tvalid_0's binary_logloss: 0.227486\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1969]\tvalid_0's binary_logloss: 0.291597\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1568]\tvalid_0's binary_logloss: 0.303191\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.229073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.609353:  23%|██▎       | 7/30 [00:31<01:59,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:28,317] Trial 6 finished with value: 0.6028622540250447 and parameters: {'k_smooth': 14, 'learning_rate': 0.013985468210233495, 'num_leaves': 177, 'max_depth': 7, 'min_child_samples': 99, 'subsample': 0.6977702368006411, 'colsample_bytree': 0.6635878334582078, 'reg_alpha': 0.22075028232861027, 'reg_lambda': 3.281647947326367}. Best is trial 4 with value: 0.6093528578176665.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[599]\tvalid_0's binary_logloss: 0.261341\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's binary_logloss: 0.219731\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1091]\tvalid_0's binary_logloss: 0.276799\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1103]\tvalid_0's binary_logloss: 0.282919\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1920]\tvalid_0's binary_logloss: 0.183914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.609533:  27%|██▋       | 8/30 [00:40<02:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:36,919] Trial 7 finished with value: 0.6095329802599904 and parameters: {'k_smooth': 11, 'learning_rate': 0.015049828898097459, 'num_leaves': 113, 'max_depth': 11, 'min_child_samples': 28, 'subsample': 0.9351779629995216, 'colsample_bytree': 0.6384393631575852, 'reg_alpha': 1.9529189300267915, 'reg_lambda': 2.343256008238508}. Best is trial 7 with value: 0.6095329802599904.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's binary_logloss: 0.278228\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1060]\tvalid_0's binary_logloss: 0.212643\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1240]\tvalid_0's binary_logloss: 0.307139\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[830]\tvalid_0's binary_logloss: 0.306419\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1846]\tvalid_0's binary_logloss: 0.211253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.609533:  30%|███       | 9/30 [00:43<01:55,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:40,621] Trial 8 finished with value: 0.6086956521739131 and parameters: {'k_smooth': 49, 'learning_rate': 0.035174661971646086, 'num_leaves': 197, 'max_depth': 5, 'min_child_samples': 64, 'subsample': 0.6480786244852675, 'colsample_bytree': 0.718456079008858, 'reg_alpha': 0.2374554379084881, 'reg_lambda': 1.5899158969698801}. Best is trial 7 with value: 0.6095329802599904.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1006]\tvalid_0's binary_logloss: 0.266728\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1992]\tvalid_0's binary_logloss: 0.224886\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1906]\tvalid_0's binary_logloss: 0.280564\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1609]\tvalid_0's binary_logloss: 0.286868\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1996]\tvalid_0's binary_logloss: 0.20903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.609533:  33%|███▎      | 10/30 [00:52<02:11,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:49,610] Trial 9 finished with value: 0.6029609690444145 and parameters: {'k_smooth': 24, 'learning_rate': 0.01142696669642141, 'num_leaves': 186, 'max_depth': 9, 'min_child_samples': 60, 'subsample': 0.8092992213866799, 'colsample_bytree': 0.6375762043033767, 'reg_alpha': 1.1518929911123585, 'reg_lambda': 4.646480987881071}. Best is trial 7 with value: 0.6095329802599904.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[306]\tvalid_0's binary_logloss: 0.274412\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[637]\tvalid_0's binary_logloss: 0.218276\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[621]\tvalid_0's binary_logloss: 0.283357\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[558]\tvalid_0's binary_logloss: 0.285897\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[942]\tvalid_0's binary_logloss: 0.186998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.609533:  37%|███▋      | 11/30 [00:59<02:07,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:50:56,598] Trial 10 finished with value: 0.5976627712854758 and parameters: {'k_smooth': 5, 'learning_rate': 0.019654925674019167, 'num_leaves': 97, 'max_depth': 12, 'min_child_samples': 15, 'subsample': 0.995356955865403, 'colsample_bytree': 0.9413155209638853, 'reg_alpha': 0.7338671210455445, 'reg_lambda': 2.064414321841941}. Best is trial 7 with value: 0.6095329802599904.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[323]\tvalid_0's binary_logloss: 0.269429\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[773]\tvalid_0's binary_logloss: 0.22315\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[650]\tvalid_0's binary_logloss: 0.281141\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[536]\tvalid_0's binary_logloss: 0.286319\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1061]\tvalid_0's binary_logloss: 0.178884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 7. Best value: 0.609533:  40%|████      | 12/30 [01:07<02:05,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:04,192] Trial 11 finished with value: 0.6006137658921525 and parameters: {'k_smooth': 13, 'learning_rate': 0.020880106900689144, 'num_leaves': 251, 'max_depth': 11, 'min_child_samples': 19, 'subsample': 0.7298499012944348, 'colsample_bytree': 0.884955410597166, 'reg_alpha': 0.7147982153704264, 'reg_lambda': 2.95910607048785}. Best is trial 7 with value: 0.6095329802599904.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[447]\tvalid_0's binary_logloss: 0.26827\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1198]\tvalid_0's binary_logloss: 0.219365\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[766]\tvalid_0's binary_logloss: 0.280116\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[888]\tvalid_0's binary_logloss: 0.287552\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1860]\tvalid_0's binary_logloss: 0.191015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.613442:  43%|████▎     | 13/30 [01:13<01:56,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:10,762] Trial 12 finished with value: 0.6134415877311682 and parameters: {'k_smooth': 16, 'learning_rate': 0.024948222194067716, 'num_leaves': 35, 'max_depth': 11, 'min_child_samples': 38, 'subsample': 0.9134648060971456, 'colsample_bytree': 0.9762505737361376, 'reg_alpha': 1.4992813785867436, 'reg_lambda': 3.8112907927035704}. Best is trial 12 with value: 0.6134415877311682.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[488]\tvalid_0's binary_logloss: 0.269086\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.219384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[954]\tvalid_0's binary_logloss: 0.287329\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[601]\tvalid_0's binary_logloss: 0.292664\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1984]\tvalid_0's binary_logloss: 0.194547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 12. Best value: 0.613442:  47%|████▋     | 14/30 [01:20<01:49,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:17,633] Trial 13 finished with value: 0.6097215667767815 and parameters: {'k_smooth': 16, 'learning_rate': 0.02193752284949523, 'num_leaves': 36, 'max_depth': 12, 'min_child_samples': 31, 'subsample': 0.9528935735632432, 'colsample_bytree': 0.9830950525223859, 'reg_alpha': 1.53711470123462, 'reg_lambda': 3.947887497320112}. Best is trial 12 with value: 0.6134415877311682.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[417]\tvalid_0's binary_logloss: 0.271224\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1105]\tvalid_0's binary_logloss: 0.222694\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[759]\tvalid_0's binary_logloss: 0.278991\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[798]\tvalid_0's binary_logloss: 0.285842\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1695]\tvalid_0's binary_logloss: 0.189841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.613722:  50%|█████     | 15/30 [01:27<01:42,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:24,441] Trial 14 finished with value: 0.6137218045112782 and parameters: {'k_smooth': 19, 'learning_rate': 0.0241417173275755, 'num_leaves': 40, 'max_depth': 12, 'min_child_samples': 43, 'subsample': 0.9514870713878738, 'colsample_bytree': 0.9939712558561908, 'reg_alpha': 1.5122859307595813, 'reg_lambda': 4.044750309214884}. Best is trial 14 with value: 0.6137218045112782.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's binary_logloss: 0.26573\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\tvalid_0's binary_logloss: 0.227513\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[236]\tvalid_0's binary_logloss: 0.28267\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[306]\tvalid_0's binary_logloss: 0.293221\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[723]\tvalid_0's binary_logloss: 0.176353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.613722:  53%|█████▎    | 16/30 [01:31<01:21,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:27,946] Trial 15 finished with value: 0.6091278807049254 and parameters: {'k_smooth': 20, 'learning_rate': 0.07480364384620089, 'num_leaves': 70, 'max_depth': 12, 'min_child_samples': 44, 'subsample': 0.8828991412485403, 'colsample_bytree': 0.9956275520037633, 'reg_alpha': 1.5104892637408136, 'reg_lambda': 3.9710330091925465}. Best is trial 14 with value: 0.6137218045112782.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[490]\tvalid_0's binary_logloss: 0.274488\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1679]\tvalid_0's binary_logloss: 0.231586\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1050]\tvalid_0's binary_logloss: 0.287535\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[776]\tvalid_0's binary_logloss: 0.299062\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.200947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.613722:  57%|█████▋    | 17/30 [01:37<01:16,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:33,895] Trial 16 finished with value: 0.5971907566832805 and parameters: {'k_smooth': 5, 'learning_rate': 0.02548855576212181, 'num_leaves': 40, 'max_depth': 10, 'min_child_samples': 133, 'subsample': 0.9858981174444015, 'colsample_bytree': 0.9400236043095626, 'reg_alpha': 1.522973382832628, 'reg_lambda': 3.7917775992148437}. Best is trial 14 with value: 0.6137218045112782.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[417]\tvalid_0's binary_logloss: 0.274856\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1487]\tvalid_0's binary_logloss: 0.22401\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[720]\tvalid_0's binary_logloss: 0.28316\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[949]\tvalid_0's binary_logloss: 0.298686\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1336]\tvalid_0's binary_logloss: 0.202944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.613722:  60%|██████    | 18/30 [01:43<01:11,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:39,940] Trial 17 finished with value: 0.6009270965023178 and parameters: {'k_smooth': 25, 'learning_rate': 0.029183883872454425, 'num_leaves': 80, 'max_depth': 10, 'min_child_samples': 119, 'subsample': 0.8812908075605744, 'colsample_bytree': 0.9286341681760919, 'reg_alpha': 0.8791909014461322, 'reg_lambda': 4.523122187909414}. Best is trial 14 with value: 0.6137218045112782.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[496]\tvalid_0's binary_logloss: 0.27021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1746]\tvalid_0's binary_logloss: 0.219148\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1263]\tvalid_0's binary_logloss: 0.284157\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1218]\tvalid_0's binary_logloss: 0.289162\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1993]\tvalid_0's binary_logloss: 0.200676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 14. Best value: 0.613722:  63%|██████▎   | 19/30 [01:50<01:11,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:47,673] Trial 18 finished with value: 0.6097201767304861 and parameters: {'k_smooth': 19, 'learning_rate': 0.017719793107366003, 'num_leaves': 33, 'max_depth': 11, 'min_child_samples': 54, 'subsample': 0.9519936417627777, 'colsample_bytree': 0.8424413861758244, 'reg_alpha': 1.2957317344405104, 'reg_lambda': 2.6959821266043495}. Best is trial 14 with value: 0.6137218045112782.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  67%|██████▋   | 20/30 [01:58<01:08,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:51:55,488] Trial 19 finished with value: 0.6179009541117674 and parameters: {'k_smooth': 9, 'learning_rate': 0.024781124923563432, 'num_leaves': 93, 'max_depth': 12, 'min_child_samples': 45, 'subsample': 0.8476028533782086, 'colsample_bytree': 0.9644797168301948, 'reg_alpha': 1.7250263945237354, 'reg_lambda': 4.982743504628338}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1994]\tvalid_0's binary_logloss: 0.276706\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.270013\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1998]\tvalid_0's binary_logloss: 0.313427\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's binary_logloss: 0.318829\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1999]\tvalid_0's binary_logloss: 0.27993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  70%|███████   | 21/30 [02:05<01:01,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:02,074] Trial 20 finished with value: 0.5965684117905852 and parameters: {'k_smooth': 10, 'learning_rate': 0.010226000569811091, 'num_leaves': 92, 'max_depth': 5, 'min_child_samples': 126, 'subsample': 0.854925711575995, 'colsample_bytree': 0.9021934251407135, 'reg_alpha': 1.733920818663474, 'reg_lambda': 4.771211682779504}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[439]\tvalid_0's binary_logloss: 0.268512\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1223]\tvalid_0's binary_logloss: 0.220818\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[864]\tvalid_0's binary_logloss: 0.284924\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[628]\tvalid_0's binary_logloss: 0.287691\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1484]\tvalid_0's binary_logloss: 0.183007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  73%|███████▎  | 22/30 [02:12<00:55,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:09,258] Trial 21 finished with value: 0.6095921111609144 and parameters: {'k_smooth': 8, 'learning_rate': 0.024936522365343664, 'num_leaves': 54, 'max_depth': 12, 'min_child_samples': 42, 'subsample': 0.9055826780811815, 'colsample_bytree': 0.9795986995421819, 'reg_alpha': 1.7000463555219214, 'reg_lambda': 4.288373013060694}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's binary_logloss: 0.266113\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[623]\tvalid_0's binary_logloss: 0.222246\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[504]\tvalid_0's binary_logloss: 0.286353\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[447]\tvalid_0's binary_logloss: 0.284298\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[532]\tvalid_0's binary_logloss: 0.195131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  77%|███████▋  | 23/30 [02:17<00:43,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:13,980] Trial 22 finished with value: 0.604129263913824 and parameters: {'k_smooth': 18, 'learning_rate': 0.03545970652205858, 'num_leaves': 57, 'max_depth': 12, 'min_child_samples': 11, 'subsample': 0.9200190686821077, 'colsample_bytree': 0.9608456190247163, 'reg_alpha': 1.365318627671559, 'reg_lambda': 3.5664801027184443}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[337]\tvalid_0's binary_logloss: 0.267508\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1021]\tvalid_0's binary_logloss: 0.22641\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[700]\tvalid_0's binary_logloss: 0.284336\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[663]\tvalid_0's binary_logloss: 0.287911\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1394]\tvalid_0's binary_logloss: 0.181792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  80%|████████  | 24/30 [02:24<00:39,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:21,223] Trial 23 finished with value: 0.6092778574844572 and parameters: {'k_smooth': 16, 'learning_rate': 0.023889885738252622, 'num_leaves': 119, 'max_depth': 11, 'min_child_samples': 41, 'subsample': 0.8350567113109755, 'colsample_bytree': 0.9135963145558268, 'reg_alpha': 1.0816332547204546, 'reg_lambda': 4.279165574367091}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[321]\tvalid_0's binary_logloss: 0.27321\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.229875\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.276643\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[688]\tvalid_0's binary_logloss: 0.286151\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1463]\tvalid_0's binary_logloss: 0.198977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  83%|████████▎ | 25/30 [02:30<00:32,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:27,499] Trial 24 finished with value: 0.6054002842254855 and parameters: {'k_smooth': 28, 'learning_rate': 0.029352826027366906, 'num_leaves': 88, 'max_depth': 10, 'min_child_samples': 71, 'subsample': 0.9751847374760663, 'colsample_bytree': 0.9594039255973681, 'reg_alpha': 1.6256783304166231, 'reg_lambda': 4.959809716032826}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalid_0's binary_logloss: 0.27117\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1124]\tvalid_0's binary_logloss: 0.223459\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[847]\tvalid_0's binary_logloss: 0.287077\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[758]\tvalid_0's binary_logloss: 0.293313\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1596]\tvalid_0's binary_logloss: 0.197371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  87%|████████▋ | 26/30 [02:37<00:26,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:34,757] Trial 25 finished with value: 0.6108637577916296 and parameters: {'k_smooth': 9, 'learning_rate': 0.017874044045728742, 'num_leaves': 49, 'max_depth': 12, 'min_child_samples': 47, 'subsample': 0.8808974405186855, 'colsample_bytree': 0.9990822714140141, 'reg_alpha': 1.8674813027386217, 'reg_lambda': 0.022203198371913313}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid_0's binary_logloss: 0.267878\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[537]\tvalid_0's binary_logloss: 0.218987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[457]\tvalid_0's binary_logloss: 0.282235\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[441]\tvalid_0's binary_logloss: 0.285524\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[721]\tvalid_0's binary_logloss: 0.177156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  90%|█████████ | 27/30 [02:42<00:18,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:39,417] Trial 26 finished with value: 0.6078674948240166 and parameters: {'k_smooth': 13, 'learning_rate': 0.0409640322762571, 'num_leaves': 76, 'max_depth': 11, 'min_child_samples': 28, 'subsample': 0.7436833438141194, 'colsample_bytree': 0.8656713316158491, 'reg_alpha': 1.3924690366674928, 'reg_lambda': 3.493061898511826}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[437]\tvalid_0's binary_logloss: 0.274948\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1269]\tvalid_0's binary_logloss: 0.229625\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[941]\tvalid_0's binary_logloss: 0.282948\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[846]\tvalid_0's binary_logloss: 0.293915\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1422]\tvalid_0's binary_logloss: 0.199834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  93%|█████████▎| 28/30 [02:48<00:12,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:45,302] Trial 27 finished with value: 0.6080760095011877 and parameters: {'k_smooth': 22, 'learning_rate': 0.02644182926100157, 'num_leaves': 136, 'max_depth': 8, 'min_child_samples': 84, 'subsample': 0.9546450533795641, 'colsample_bytree': 0.9548612597310835, 'reg_alpha': 0.9275660096063614, 'reg_lambda': 4.524792990077587}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[277]\tvalid_0's binary_logloss: 0.273002\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[705]\tvalid_0's binary_logloss: 0.221846\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[598]\tvalid_0's binary_logloss: 0.281137\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[547]\tvalid_0's binary_logloss: 0.287702\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1227]\tvalid_0's binary_logloss: 0.18422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901:  97%|█████████▋| 29/30 [02:53<00:05,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:50,725] Trial 28 finished with value: 0.6080079090459714 and parameters: {'k_smooth': 16, 'learning_rate': 0.031156711452003143, 'num_leaves': 108, 'max_depth': 12, 'min_child_samples': 61, 'subsample': 0.9013347740687951, 'colsample_bytree': 0.9123366743144984, 'reg_alpha': 1.1608719638321547, 'reg_lambda': 3.746325421933225}. Best is trial 19 with value: 0.6179009541117674.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[245]\tvalid_0's binary_logloss: 0.265312\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[732]\tvalid_0's binary_logloss: 0.224699\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[426]\tvalid_0's binary_logloss: 0.278567\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[468]\tvalid_0's binary_logloss: 0.28433\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1345]\tvalid_0's binary_logloss: 0.190014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 19. Best value: 0.617901: 100%|██████████| 30/30 [02:58<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 17:52:55,527] Trial 29 finished with value: 0.6091278807049254 and parameters: {'k_smooth': 30, 'learning_rate': 0.046112866579724646, 'num_leaves': 65, 'max_depth': 8, 'min_child_samples': 32, 'subsample': 0.8519000687276822, 'colsample_bytree': 0.7314156201270453, 'reg_alpha': 1.8223948488159034, 'reg_lambda': 4.984117287233841}. Best is trial 19 with value: 0.6179009541117674.\n",
      "🔎 OPTUNA BEST OOF F1=0.618 | best_th=0.465 | rounds≈1062\n",
      "BEST PARAMS: {'k_smooth': 9, 'learning_rate': 0.024781124923563432, 'num_leaves': 93, 'max_depth': 12, 'min_child_samples': 45, 'subsample': 0.8476028533782086, 'colsample_bytree': 0.9644797168301948, 'reg_alpha': 1.7250263945237354, 'reg_lambda': 4.982743504628338}\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v6.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v6.txt\n"
     ]
    }
   ],
   "source": [
    "# === Optuna 最適化セル：Grouped-OOF(F1) 最大化＋しきい値同時最適化 → 単一CSV出力 ===\n",
    "import warnings, numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# --- Optuna 用意 ---\n",
    "try:\n",
    "    import optuna\n",
    "except ModuleNotFoundError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"optuna\"])\n",
    "    import optuna\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ===== 出力先 & バージョン名 =====\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_path = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ===== 依存 =====\n",
    "train = train_df.copy()\n",
    "test  = test_df.copy()\n",
    "\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "if 'te_cols' not in globals():\n",
    "    te_cols  = ['Subprogram','NaicsSector','BusinessAge','BusinessType']\n",
    "\n",
    "y_all  = train['LoanStatus'].astype(int).reset_index(drop=True)\n",
    "groups = train['ApprovalFiscalYear'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# ===== ヘルパ =====\n",
    "def fit_category_codes(train, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(train[c]).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_codes(df, maps):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c + \"_code\"] = df[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "def make_year_sector(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['NaicsSector'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "def make_splits(seed):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    return list(sgkf.split(train, y_all, groups=groups))\n",
    "\n",
    "def add_te_fe_oof_with_splits(train_df, y, test_df, cols, splits, k_smooth=10):\n",
    "    df = train_df.reset_index(drop=True).copy()\n",
    "    y  = pd.Series(y).reset_index(drop=True).rename('y')\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype('object')\n",
    "    test_df = test_df.reset_index(drop=True).copy()\n",
    "    for c in cols:\n",
    "        test_df[c] = test_df[c].astype('object')\n",
    "\n",
    "    te_train = pd.DataFrame(index=df.index)\n",
    "    fe_train = pd.DataFrame(index=df.index)\n",
    "    te_test  = pd.DataFrame(index=test_df.index)\n",
    "    fe_test  = pd.DataFrame(index=test_df.index)\n",
    "    gmean = float(y.mean())\n",
    "\n",
    "    for col in cols:\n",
    "        te_train[col + \"_teCV\"] = np.nan\n",
    "        fe_train[col + \"_feCV\"] = np.nan\n",
    "\n",
    "        for tr_idx, va_idx in splits:\n",
    "            tr_col = df[col].iloc[tr_idx].reset_index(drop=True)\n",
    "            yy_tr  = y.iloc[tr_idx].reset_index(drop=True)\n",
    "\n",
    "            agg = pd.DataFrame({'col': tr_col, 'y': yy_tr}).groupby('col')['y'].agg(['count','mean'])\n",
    "            smooth = (agg['mean']*agg['count'] + gmean*k_smooth) / (agg['count'] + k_smooth)\n",
    "\n",
    "            mapped  = df[col].iloc[va_idx].map(smooth)\n",
    "            mapped  = pd.Series(mapped.to_numpy(dtype='float64'), index=va_idx)\n",
    "            te_train.loc[va_idx, col + \"_teCV\"] = mapped.fillna(gmean).to_numpy()\n",
    "\n",
    "            freq    = tr_col.value_counts(normalize=True)\n",
    "            mappedf = df[col].iloc[va_idx].map(freq)\n",
    "            mappedf = pd.Series(mappedf.to_numpy(dtype='float64'), index=va_idx)\n",
    "            fe_train.loc[va_idx, col + \"_feCV\"] = mappedf.fillna(0.0).to_numpy()\n",
    "\n",
    "        agg_full = pd.DataFrame({'col': df[col], 'y': y}).groupby('col')['y'].agg(['count','mean'])\n",
    "        smooth_f = (agg_full['mean']*agg_full['count'] + gmean*k_smooth) / (agg_full['count'] + k_smooth)\n",
    "        te_test[col + \"_teCV\"] = pd.Series(test_df[col].map(smooth_f).to_numpy(dtype='float64')).fillna(gmean).to_numpy()\n",
    "\n",
    "        freq_full = df[col].value_counts(normalize=True)\n",
    "        fe_test[col + \"_feCV\"] = pd.Series(test_df[col].map(freq_full).to_numpy(dtype='float64')).fillna(0.0).to_numpy()\n",
    "\n",
    "    return (pd.concat([df.reset_index(drop=True), te_train, fe_train], axis=1),\n",
    "            pd.concat([test_df.reset_index(drop=True), te_test, fe_test], axis=1))\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.10, 0.90, 161)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# ===== 前処理（カテゴリコード＋交差列） =====\n",
    "code_maps  = fit_category_codes(train, cat_cols)\n",
    "train_code = apply_category_codes(train, code_maps).reset_index(drop=True)\n",
    "test_code  = apply_category_codes(test,  code_maps).reset_index(drop=True)\n",
    "\n",
    "train['YearSector'] = make_year_sector(train)\n",
    "test ['YearSector'] = make_year_sector(test)\n",
    "te_cols_local = te_cols + ['YearSector']\n",
    "\n",
    "# ===== Optuna 目的関数 =====\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    splits = make_splits(seed=0)\n",
    "    train_tefe, test_tefe = add_te_fe_oof_with_splits(\n",
    "        train[te_cols_local], y_all, test[te_cols_local],\n",
    "        te_cols_local, splits, k_smooth=trial.suggest_int(\"k_smooth\", 5, 50)\n",
    "    )\n",
    "    X_all = pd.concat([\n",
    "        train_code[base_feats],\n",
    "        train_code[[c+\"_code\" for c in cat_cols]],\n",
    "        train_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "        train_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "    ], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "    params = dict(\n",
    "        learning_rate      = trial.suggest_float(\"learning_rate\", 0.01, 0.08, log=True),\n",
    "        num_leaves         = trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "        max_depth          = trial.suggest_int(\"max_depth\", 5, 12),\n",
    "        min_child_samples  = trial.suggest_int(\"min_child_samples\", 10, 200),\n",
    "        subsample          = trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        colsample_bytree   = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        reg_alpha          = trial.suggest_float(\"reg_alpha\", 0.0, 2.0),\n",
    "        reg_lambda         = trial.suggest_float(\"reg_lambda\", 0.0, 5.0),\n",
    "        n_estimators       = 2000,\n",
    "        class_weight       = 'balanced',\n",
    "        verbosity          = -1,\n",
    "        random_state       = 0\n",
    "    )\n",
    "\n",
    "    oof = np.zeros(len(X_all))\n",
    "    best_rounds = []\n",
    "    for tr_idx, va_idx in make_splits(seed=0):\n",
    "        Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=0)]\n",
    "        )\n",
    "        oof[va_idx] = model.predict_proba(Xva, num_iteration=model.best_iteration_)[:, 1]\n",
    "        bi = getattr(model, \"best_iteration_\", None)\n",
    "        best_rounds.append(int(bi) if bi is not None else params['n_estimators'])\n",
    "\n",
    "    th, f1 = best_f1_threshold(y_all, oof)\n",
    "    trial.set_user_attr(\"best_th\", th)\n",
    "    trial.set_user_attr(\"avg_best_rounds\", int(np.mean(best_rounds)))\n",
    "    return f1\n",
    "\n",
    "# ===== 最適化 =====\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_grouped_oof_f1\",\n",
    "                            sampler=optuna.samplers.TPESampler(seed=0),\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "best = study.best_trial\n",
    "best_params = best.params.copy()\n",
    "best_th     = best.user_attrs.get(\"best_th\", 0.5)\n",
    "best_rounds = best.user_attrs.get(\"avg_best_rounds\", best_params.get(\"n_estimators\", 1000))\n",
    "\n",
    "print(f\"🔎 OPTUNA BEST OOF F1={best.value:.3f} | best_th={best_th:.3f} | rounds≈{best_rounds}\")\n",
    "print(\"BEST PARAMS:\", best_params)\n",
    "\n",
    "# ===== ベスト設定で再構築 → 全学習 → テスト予測 =====\n",
    "splits = make_splits(seed=0)\n",
    "train_tefe, test_tefe = add_te_fe_oof_with_splits(\n",
    "    train[te_cols_local], y_all, test[te_cols_local],\n",
    "    te_cols_local, splits, k_smooth=best_params.get(\"k_smooth\", 10)\n",
    ")\n",
    "\n",
    "X_all = pd.concat([\n",
    "    train_code[base_feats],\n",
    "    train_code[[c+\"_code\" for c in cat_cols]],\n",
    "    train_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "    train_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "T_all = pd.concat([\n",
    "    test_code[base_feats],\n",
    "    test_code[[c+\"_code\" for c in cat_cols]],\n",
    "    test_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "    test_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "final_params = dict(best_params)\n",
    "final_params.update(dict(\n",
    "    n_estimators = int(best_rounds),\n",
    "    class_weight = 'balanced',\n",
    "    verbosity    = -1,\n",
    "    random_state = 0\n",
    "))\n",
    "final_params.pop(\"k_smooth\", None)\n",
    "\n",
    "final = LGBMClassifier(**final_params)\n",
    "final.fit(X_all, y_all)\n",
    "\n",
    "proba_test = final.predict_proba(T_all)[:, 1]\n",
    "pred_test  = (proba_test >= best_th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "submission = pd.DataFrame({'id': ids, 'LoanStatus': pred_test})\n",
    "submission.to_csv(out_path, index=False, header=False)\n",
    "print(f\"✅ submission を保存: {out_path}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OPTUNA BEST OOF F1: {best.value:.6f}\\n\")\n",
    "    f.write(f\"Best threshold   : {best_th:.6f}\\n\")\n",
    "    f.write(f\"Avg best rounds  : {best_rounds}\\n\")\n",
    "    f.write(f\"Best params      : {best_params}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87a5b521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "🔧 OOF F1 (calibrated×seeds) = 0.617901 | best_th = 0.230 | rounds(full) = 1062 | rounds(OOF avg) = [1062, 1062, 1062, 1062, 1062]\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v8.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v8.txt\n"
     ]
    }
   ],
   "source": [
    "# === Aライン強化 v3：多シードOOF×Isotonicキャリブ→F1最適しきい値→単一CSV出力 ===\n",
    "import re, warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----- 出力先 -----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ----- 依存（前処理セル & Optunaセルの変数を使う） -----\n",
    "train = train_df.copy()\n",
    "test  = test_df.copy()\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "if 'te_cols' not in globals():\n",
    "    te_cols  = ['Subprogram','NaicsSector','BusinessAge','BusinessType']\n",
    "\n",
    "y_all  = train['LoanStatus'].astype(int).reset_index(drop=True)\n",
    "groups = train['ApprovalFiscalYear'].astype(int).reset_index(drop=True)\n",
    "\n",
    "# ----- ヘルパ -----\n",
    "def fit_category_codes(df, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(df[c]).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_codes(df, maps):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c + \"_code\"] = df[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "def make_year_sector(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['NaicsSector'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "def make_splits(seed):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    return list(sgkf.split(train, y_all, groups=groups))\n",
    "\n",
    "def add_te_fe_oof_with_splits(train_df, y, test_df, cols, splits, k_smooth=10):\n",
    "    df = train_df.reset_index(drop=True).copy()\n",
    "    y  = pd.Series(y).reset_index(drop=True).rename('y')\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype('object')\n",
    "    test_df = test_df.reset_index(drop=True).copy()\n",
    "    for c in cols:\n",
    "        test_df[c] = test_df[c].astype('object')\n",
    "\n",
    "    te_train = pd.DataFrame(index=df.index); fe_train = pd.DataFrame(index=df.index)\n",
    "    te_test  = pd.DataFrame(index=test_df.index); fe_test = pd.DataFrame(index=test_df.index)\n",
    "    gmean = float(y.mean())\n",
    "\n",
    "    for col in cols:\n",
    "        te_train[col+\"_teCV\"] = np.nan; fe_train[col+\"_feCV\"] = np.nan\n",
    "        for tr_idx, va_idx in splits:\n",
    "            tr_col = df[col].iloc[tr_idx].reset_index(drop=True)\n",
    "            yy_tr  = y.iloc[tr_idx].reset_index(drop=True)\n",
    "            agg = pd.DataFrame({'col': tr_col, 'y': yy_tr}).groupby('col')['y'].agg(['count','mean'])\n",
    "            smooth = (agg['mean']*agg['count'] + gmean*k_smooth) / (agg['count'] + k_smooth)\n",
    "\n",
    "            m = df[col].iloc[va_idx].map(smooth); m = pd.Series(m.to_numpy(dtype='float64'), index=va_idx)\n",
    "            te_train.loc[va_idx, col+\"_teCV\"] = m.fillna(gmean).to_numpy()\n",
    "\n",
    "            f = tr_col.value_counts(normalize=True); mf = df[col].iloc[va_idx].map(f)\n",
    "            mf = pd.Series(mf.to_numpy(dtype='float64'), index=va_idx)\n",
    "            fe_train.loc[va_idx, col+\"_feCV\"] = mf.fillna(0.0).to_numpy()\n",
    "\n",
    "        agg_full = pd.DataFrame({'col': df[col], 'y': y}).groupby('col')['y'].agg(['count','mean'])\n",
    "        smooth_f = (agg_full['mean']*agg_full['count'] + gmean*k_smooth) / (agg_full['count'] + k_smooth)\n",
    "        te_test[col+\"_teCV\"] = pd.Series(test_df[col].map(smooth_f).to_numpy(dtype='float64')).fillna(gmean).to_numpy()\n",
    "        freq_full = df[col].value_counts(normalize=True)\n",
    "        fe_test[col+\"_feCV\"] = pd.Series(test_df[col].map(freq_full).to_numpy(dtype='float64')).fillna(0.0).to_numpy()\n",
    "\n",
    "    return (pd.concat([df.reset_index(drop=True), te_train, fe_train], axis=1),\n",
    "            pd.concat([test_df.reset_index(drop=True), te_test, fe_test], axis=1))\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.10, 0.90, 161)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# ----- 特徴テーブル（コード + TE/FE + 交差列） -----\n",
    "code_maps  = fit_category_codes(train, cat_cols)\n",
    "train_code = apply_category_codes(train, code_maps).reset_index(drop=True)\n",
    "test_code  = apply_category_codes(test,  code_maps).reset_index(drop=True)\n",
    "\n",
    "train['YearSector'] = make_year_sector(train)\n",
    "test ['YearSector'] = make_year_sector(test)\n",
    "te_cols_local = te_cols + ['YearSector']\n",
    "\n",
    "# ----- モデル設定：Optunaの最良を使用 -----\n",
    "assert 'best_params' in globals(), \"先にOptunaセルを実行して best_params を作ってください。\"\n",
    "assert 'best_rounds' in globals(), \"先にOptunaセルを実行して best_rounds を作ってください。\"\n",
    "\n",
    "base_params = dict(best_params)\n",
    "k_smooth    = int(base_params.pop(\"k_smooth\", 10))\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1, random_state=0))\n",
    "n_rounds_full = int(best_rounds)\n",
    "\n",
    "# ----- 多シード学習 → OOF確率平均 -----\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "oof_list, test_list, rounds_used = [], [], []\n",
    "\n",
    "for seed in seeds:\n",
    "    splits = make_splits(seed=seed)\n",
    "    tr_tefe, te_tefe = add_te_fe_oof_with_splits(\n",
    "        train[te_cols_local], y_all, test[te_cols_local], te_cols_local, splits, k_smooth=k_smooth\n",
    "    )\n",
    "\n",
    "    X_all = pd.concat([\n",
    "        train_code[base_feats],\n",
    "        train_code[[c+\"_code\" for c in cat_cols]],\n",
    "        tr_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "        tr_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "    ], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "    T_all = pd.concat([\n",
    "        test_code[base_feats],\n",
    "        test_code[[c+\"_code\" for c in cat_cols]],\n",
    "        te_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "        te_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "    ], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "    oof = np.zeros(len(X_all)); best_rounds_each = []\n",
    "    for tr_idx, va_idx in splits:\n",
    "        Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "        model = LGBMClassifier(**base_params, n_estimators=2000)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=0)]\n",
    "        )\n",
    "        oof[va_idx] = model.predict_proba(Xva, num_iteration=model.best_iteration_)[:,1]\n",
    "        best_rounds_each.append(int(getattr(model, \"best_iteration_\", 2000)))\n",
    "    oof_list.append(oof)\n",
    "    rounds_used.append(int(np.mean(best_rounds_each)))\n",
    "\n",
    "    # full fit（Optuna推奨のroundsを使用）\n",
    "    final = LGBMClassifier(**base_params, n_estimators=n_rounds_full)\n",
    "    final.fit(X_all, y_all)\n",
    "    test_list.append(final.predict_proba(T_all)[:,1])\n",
    "\n",
    "# 新（rank平均）:\n",
    "def _ranknormalize(a: np.ndarray) -> np.ndarray:\n",
    "    # 0..1 に正規化された順位\n",
    "    r = np.argsort(np.argsort(a))\n",
    "    return r.astype(float) / (len(a) - 1)\n",
    "\n",
    "oof_mean  = np.mean([_ranknormalize(a) for a in oof_list], axis=0)\n",
    "test_mean = np.mean([_ranknormalize(a) for a in test_list], axis=0)\n",
    "\n",
    "# ----- Isotonicキャリブレーション → F1最適しきい値 -----\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(oof_mean, y_all)\n",
    "oof_cal = iso.transform(oof_mean)\n",
    "best_th, f1_cal = best_f1_threshold(y_all, oof_cal)\n",
    "\n",
    "# ----- テストもキャリブして二値化 -----\n",
    "test_cal = iso.transform(test_mean)\n",
    "test_pred = (test_cal >= best_th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🔧 OOF F1 (calibrated×seeds) = {f1_cal:.6f} | best_th = {best_th:.3f} | rounds(full) = {n_rounds_full} | rounds(OOF avg) = {rounds_used}\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (calibrated×seeds): {f1_cal:.6f}\\n\")\n",
    "    f.write(f\"Best threshold: {best_th:.6f}\\n\")\n",
    "    f.write(f\"Full rounds: {n_rounds_full}, OOF rounds(avg per seed): {rounds_used}\\n\")\n",
    "    f.write(f\"Params: {base_params} | k_smooth={k_smooth}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "938e70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE cols (tail): ['BusinessAge_teCV', 'BusinessType_teCV', 'NaicsSector_teCV', 'Subprogram_teCV', 'YearAge_teCV', 'YearProg_teCV', 'YearRate_teCV', 'YearRev_teCV', 'YearSector_teCV', 'YearTerm_teCV']\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1464]\tvalid_0's binary_logloss: 0.224212\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\tvalid_0's binary_logloss: 0.269777\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.279987\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.284593\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1981]\tvalid_0's binary_logloss: 0.186801\n",
      "DEBUG oof_mean range: [0.0000, 0.9979]\n",
      "DEBUG test_mean range: [0.0004, 0.9930]\n",
      "🧪 LOYO OOF F1 (calibrated) = 0.617901 | best_th=0.230 | folds(years)=5 | rounds(each)≈1062\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v23.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v23.txt\n"
     ]
    }
   ],
   "source": [
    "# === Aライン強化 v4（rate-regime入り）: LOYO × Isotonic（生確率, 交差TE: YearSector/YearAge/YearProg/YearRev/YearRate）===\n",
    "import re, warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----- 出力先とバージョン -----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ----- 依存（前処理＆Optunaの成果を利用） -----\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "if 'te_cols' not in globals():\n",
    "    te_cols  = ['Subprogram','NaicsSector','BusinessAge','BusinessType']\n",
    "\n",
    "assert 'best_params' in globals() and 'best_rounds' in globals(), \"Optuna セルを先に実行して best_params / best_rounds を作ってください。\"\n",
    "\n",
    "y_all = train['LoanStatus'].astype(int)\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "\n",
    "# ----- ヘルパ -----\n",
    "def fit_category_codes(df, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(df[c]).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_category_codes(df, maps):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c + \"_code\"] = df[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "def make_year_sector(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['NaicsSector'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "def make_year_age(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['BusinessAge'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "def make_year_prog(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['Subprogram'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "def make_year_rev(df):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        df['RevolverStatus'].astype('string').fillna('UNK')\n",
    "    )\n",
    "\n",
    "# ★ 金利レジーム（RateBin）を訓練の分位で固定し、Year×RateBin を作る\n",
    "_, RATE_BINS = pd.qcut(train['InitialInterestRate'], q=5, retbins=True, duplicates='drop')\n",
    "\n",
    "def make_rate_bin(df, bins):\n",
    "    return pd.cut(df['InitialInterestRate'], bins=bins, include_lowest=True).astype('string').fillna('UNK')\n",
    "\n",
    "def make_year_rate(df, bins):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        make_rate_bin(df, bins)\n",
    "    )\n",
    "\n",
    "# --- TermBin（訓練の分位で固定）と Year×TermBin 追加 ---\n",
    "_, TERM_BINS = pd.qcut(train['TermInMonths'], q=5, retbins=True, duplicates='drop')\n",
    "\n",
    "def make_term_bin(df, bins):\n",
    "    return pd.cut(df['TermInMonths'], bins=bins, include_lowest=True).astype('string').fillna('UNK')\n",
    "\n",
    "def make_year_term(df, bins):\n",
    "    return (\n",
    "        df['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" +\n",
    "        make_term_bin(df, bins)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# 交差キー列を作成\n",
    "train['YearSector'] = make_year_sector(train)\n",
    "test ['YearSector'] = make_year_sector(test)\n",
    "train['YearAge']    = make_year_age(train)\n",
    "test ['YearAge']    = make_year_age(test)\n",
    "train['YearProg']   = make_year_prog(train)\n",
    "test ['YearProg']   = make_year_prog(test)\n",
    "train['YearRev']    = make_year_rev(train)\n",
    "test ['YearRev']    = make_year_rev(test)\n",
    "train['YearRate']   = make_year_rate(train, RATE_BINS)   # ★ 追加\n",
    "test ['YearRate']   = make_year_rate(test,  RATE_BINS)   # ★ 追加\n",
    "\n",
    "train['YearTerm'] = make_year_term(train, TERM_BINS)\n",
    "test ['YearTerm'] = make_year_term(test,  TERM_BINS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_te_fe_oof_with_folds(train_df, y, test_df, cols, folds, k_smooth=10):\n",
    "    df = train_df.reset_index(drop=True).copy()\n",
    "    y  = pd.Series(y).reset_index(drop=True).rename('y')\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype('object')\n",
    "        test_df[c] = test_df[c].astype('object')\n",
    "\n",
    "    te_train = pd.DataFrame(index=df.index); fe_train = pd.DataFrame(index=df.index)\n",
    "    te_test  = pd.DataFrame(index=test_df.index); fe_test = pd.DataFrame(index=test_df.index)\n",
    "    gmean = float(y.mean())\n",
    "\n",
    "    for col in cols:\n",
    "        te_train[col+\"_teCV\"] = np.nan; fe_train[col+\"_feCV\"] = np.nan\n",
    "        for va_idx in folds:  # va_idx は「その年のインデックス」\n",
    "            tr_idx = np.setdiff1d(np.arange(len(df)), va_idx)\n",
    "            tr_col = df[col].iloc[tr_idx].reset_index(drop=True)\n",
    "            yy_tr  = y.iloc[tr_idx].reset_index(drop=True)\n",
    "            agg = pd.DataFrame({'col': tr_col, 'y': yy_tr}).groupby('col')['y'].agg(['count','mean'])\n",
    "            smooth = (agg['mean']*agg['count'] + gmean*k_smooth) / (agg['count'] + k_smooth)\n",
    "\n",
    "            m = df[col].iloc[va_idx].map(smooth); m = pd.Series(m.to_numpy(dtype='float64'), index=va_idx)\n",
    "            te_train.loc[va_idx, col+\"_teCV\"] = m.fillna(gmean).to_numpy()\n",
    "\n",
    "            f = tr_col.value_counts(normalize=True); mf = df[col].iloc[va_idx].map(f)\n",
    "            mf = pd.Series(mf.to_numpy(dtype='float64'), index=va_idx)\n",
    "            fe_train.loc[va_idx, col+\"_feCV\"] = mf.fillna(0.0).to_numpy()\n",
    "\n",
    "        agg_full = pd.DataFrame({'col': df[col], 'y': y}).groupby('col')['y'].agg(['count','mean'])\n",
    "        smooth_f = (agg_full['mean']*agg_full['count'] + gmean*k_smooth) / (agg_full['count'] + k_smooth)\n",
    "        te_test[col+\"_teCV\"] = pd.Series(test_df[col].map(smooth_f).to_numpy(dtype='float64')).fillna(gmean).to_numpy()\n",
    "        freq_full = df[col].value_counts(normalize=True)\n",
    "        fe_test[col+\"_feCV\"] = pd.Series(test_df[col].map(freq_full).to_numpy(dtype='float64')).fillna(0.0).to_numpy()\n",
    "\n",
    "    return (pd.concat([df.reset_index(drop=True), te_train, fe_train], axis=1),\n",
    "            pd.concat([test_df.reset_index(drop=True), te_test, fe_test], axis=1))\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.10, 0.90, 161)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# ----- 特徴テーブル作成（コード + TE/FE + 交差列） -----\n",
    "code_maps  = fit_category_codes(train, cat_cols)\n",
    "train_code = apply_category_codes(train, code_maps)\n",
    "test_code  = apply_category_codes(test,  code_maps)\n",
    "\n",
    "# ★ YearRate を含む\n",
    "te_cols_local = te_cols + ['YearSector', 'YearAge', 'YearProg', 'YearRev', 'YearRate', 'YearTerm']\n",
    "\n",
    "\n",
    "# LOYO folds：各年度をそのまま検証インデックスにする\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == y)[0] for y in years]\n",
    "\n",
    "# TE/FE を LOYO で OOF 生成\n",
    "base_params = dict(best_params)\n",
    "k_smooth    = int(base_params.pop(\"k_smooth\", 10))\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1, random_state=0))\n",
    "n_rounds_full = int(best_rounds)\n",
    "\n",
    "tr_tefe, te_tefe = add_te_fe_oof_with_folds(\n",
    "    train[te_cols_local], y_all, test[te_cols_local], te_cols_local, folds, k_smooth=k_smooth\n",
    ")\n",
    "\n",
    "# ---- デバッグ: 交差TEが入っているか確認 ----\n",
    "cols_te = [c for c in tr_tefe.columns if c.endswith('_teCV')]\n",
    "print(\"TE cols (tail):\", sorted(cols_te)[-10:])\n",
    "assert 'YearProg_teCV' in cols_te\n",
    "assert 'YearRev_teCV'  in cols_te\n",
    "assert 'YearRate_teCV' in cols_te\n",
    "assert 'YearTerm_teCV' in cols_te, \"YearTerm_teCV が作られていない\"\n",
    "\n",
    "X_all = pd.concat([\n",
    "    train_code[base_feats],\n",
    "    train_code[[c+\"_code\" for c in cat_cols]],\n",
    "    tr_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "    tr_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "T_all = pd.concat([\n",
    "    test_code[base_feats],\n",
    "    test_code[[c+\"_code\" for c in cat_cols]],\n",
    "    te_tefe[[c+\"_teCV\" for c in te_cols_local]],\n",
    "    te_tefe[[c+\"_feCV\" for c in te_cols_local]],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "# ----- LOYO で学習（foldごとに1モデル）→ OOF と test 予測 -----\n",
    "oof = np.zeros(len(X_all)); test_list = []; best_rounds_each = []\n",
    "for va_idx in folds:\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    model = LGBMClassifier(**base_params, n_estimators=2000)\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[early_stopping(stopping_rounds=100), log_evaluation(period=0)]\n",
    "    )\n",
    "    oof[va_idx] = model.predict_proba(Xva, num_iteration=model.best_iteration_)[:,1]\n",
    "    best_rounds_each.append(int(getattr(model, \"best_iteration_\", 2000)))\n",
    "\n",
    "    test_list.append(model.predict_proba(T_all, num_iteration=model.best_iteration_)[:,1])\n",
    "\n",
    "# ★ 生の確率でキャリブ\n",
    "oof_mean  = oof\n",
    "test_mean = np.mean(test_list, axis=0)\n",
    "print(f\"DEBUG oof_mean range: [{oof_mean.min():.4f}, {oof_mean.max():.4f}]\")\n",
    "print(f\"DEBUG test_mean range: [{test_mean.min():.4f}, {test_mean.max():.4f}]\")\n",
    "\n",
    "# ----- Isotonic キャリブ → F1 閾値最適 → 提出 -----\n",
    "iso = IsotonicRegression(out_of_bounds='clip')\n",
    "iso.fit(oof_mean, y_all)\n",
    "oof_cal = iso.transform(oof_mean)\n",
    "best_th, f1_cal = best_f1_threshold(y_all, oof_cal)\n",
    "\n",
    "test_cal  = iso.transform(test_mean)\n",
    "test_pred = (test_cal >= best_th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🧪 LOYO OOF F1 (calibrated) = {f1_cal:.6f} | best_th={best_th:.3f} | folds(years)={len(folds)} | rounds(each)≈{int(np.mean(best_rounds_each))}\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"LOYO OOF F1 (calibrated): {f1_cal:.6f}\\n\")\n",
    "    f.write(f\"Best threshold: {best_th:.6f}\\n\")\n",
    "    f.write(f\"Folds: {years}\\n\")\n",
    "    f.write(f\"Best rounds per fold: {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {base_params} | k_smooth={k_smooth}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d6897",
   "metadata": {},
   "source": [
    "# === A-line v7g: LOYO multi-seed bagging + 年×Revolver Isotonic + 縮約（提出1本） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "\n",
    "# v4 セルを実行済みであること（X_all, T_all, y_all, train, test, years, best_params, OUT_DIR, DATA_DIR が必要）\n",
    "need = ['X_all','T_all','y_all','train','test','years','best_params','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して次を用意して: {need}\"\n",
    "\n",
    "# 出力ファイル名（連番）\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ---- 1) LOYO × multi-seed bagging で oof / test_list を再構築 ----\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == y)[0] for y in years]\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "base_params = dict(best_params)\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "\n",
    "oof_bag = np.zeros(len(X_all), dtype=float)\n",
    "test_list_bag = []\n",
    "best_rounds_each = []\n",
    "\n",
    "for y in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    proba_va_seeds = []\n",
    "    proba_te_seeds = []\n",
    "\n",
    "    for sd in seeds:\n",
    "        params = dict(base_params)\n",
    "        params['random_state'] = sd\n",
    "        model = LGBMClassifier(**params, n_estimators=4000)  # 大きめ + ES\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=150), log_evaluation(period=0)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "        best_rounds_each.append(best_it)\n",
    "        proba_va = model.predict_proba(Xva, num_iteration=best_it)[:,1]\n",
    "        proba_te = model.predict_proba(T_all, num_iteration=best_it)[:,1]\n",
    "        proba_va_seeds.append(proba_va)\n",
    "        proba_te_seeds.append(proba_te)\n",
    "\n",
    "    # seed 平均\n",
    "    proba_va_mean = np.mean(proba_va_seeds, axis=0)\n",
    "    proba_te_mean = np.mean(proba_te_seeds, axis=0)\n",
    "    oof_bag[va_idx] = proba_va_mean\n",
    "    test_list_bag.append(proba_te_mean)\n",
    "\n",
    "# ---- 2) 年×Revolver Isotonic ＋ しきい値縮約（λ=200, MIN_N=120） ----\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 年レベルのバックアップ校正\n",
    "iso_year, th_year = {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof_bag[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof_bag, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y] = iso, th\n",
    "\n",
    "# セグメント校正＋縮約\n",
    "MIN_N = 120\n",
    "LAMBDA = 200\n",
    "iso_seg, th_seg_shrunk = {}, {}\n",
    "oof_cal = np.zeros_like(oof_bag, dtype=float)\n",
    "\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all.iloc[idx].values, oof_bag[idx]\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg = th_year[y]\n",
    "        n = len(idx)\n",
    "        alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha * th_seg + (1 - alpha) * th_year[y]\n",
    "        iso_seg[(y,r)] = iso\n",
    "        th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "# OOF F1（縮約しきい値で二値化）\n",
    "y_pred_oof = np.zeros_like(oof_bag, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# ---- 3) テストにも適用・提出CSV（ヘッダなし2列） ----\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list_bag[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]\n",
    "        th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "# 未知年度の保険（通常は空）\n",
    "unknown_idx = np.where(~np.isin(year_te, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    iso_all = IsotonicRegression(out_of_bounds='clip').fit(oof_bag, y_all.values)\n",
    "    cal_fb  = iso_all.transform(test_list_bag[0][unknown_idx])\n",
    "    th_all, _ = _best_f1_threshold(y_all.values, iso_all.transform(oof_bag))\n",
    "    test_pred[unknown_idx] = (cal_fb >= th_all).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (LOYO bagging + year×revolver isotonic+shrink) = {f1_oof:.6f} | seeds={seeds} | rounds(avg)≈{int(np.mean(best_rounds_each))}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (bagging+seg-calib): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Seeds: {seeds}\\n\")\n",
    "    f.write(f\"Best rounds(each): {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {base_params}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f5a49",
   "metadata": {},
   "source": [
    "# === A-line v7f: Stacked calibration (LR on [p, p^2, Year OHE, Revolver, p×Revolver]) + single threshold ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['train','test','years','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# 出力先（連番）\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# 1) メタ特徴（OOF側）\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy().reshape(-1,1)\n",
    "rev_tr  = train['RevolverStatus'].fillna(0).astype(int).to_numpy().reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "year_ohe = enc.fit_transform(year_tr)   # (n, k-1)\n",
    "\n",
    "p = np.clip(oof, 1e-6, 1-1e-6).reshape(-1,1)\n",
    "X_meta = np.hstack([\n",
    "    p, p**2,\n",
    "    year_ohe,\n",
    "    rev_tr,\n",
    "    p*rev_tr\n",
    "])\n",
    "y = y_all.values\n",
    "\n",
    "meta = LogisticRegression(max_iter=2000, C=0.5, class_weight='balanced', solver='lbfgs')\n",
    "meta.fit(X_meta, y)\n",
    "p_cal_oof = meta.predict_proba(X_meta)[:,1]\n",
    "\n",
    "best_th, f1_oof = _best_f1_threshold(y, p_cal_oof)\n",
    "\n",
    "# 2) テスト側（各年度モデルの出力を同じ設計行列へ通す）\n",
    "year_te = test['ApprovalFiscalYear'].to_numpy().reshape(-1,1)\n",
    "rev_te  = test['RevolverStatus'].fillna(0).astype(int).to_numpy().reshape(-1,1)\n",
    "year_ohe_te = enc.transform(year_te)\n",
    "\n",
    "test_cal = np.zeros(len(test), dtype=float)\n",
    "for i, yval in enumerate(years):\n",
    "    idx_t = np.where(test['ApprovalFiscalYear'].values == yval)[0]\n",
    "    if len(idx_t)==0: continue\n",
    "    p_fold = np.clip(test_list[i][idx_t], 1e-6, 1-1e-6).reshape(-1,1)\n",
    "    X_meta_te = np.hstack([\n",
    "        p_fold, p_fold**2,\n",
    "        year_ohe_te[idx_t],\n",
    "        rev_te[idx_t],\n",
    "        p_fold*rev_te[idx_t]\n",
    "    ])\n",
    "    test_cal[idx_t] = meta.predict_proba(X_meta_te)[:,1]\n",
    "\n",
    "# 未知年度の保険（通常は空）\n",
    "unknown_idx = np.where(~np.isin(test['ApprovalFiscalYear'].values, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    # 年度OneHotは ignore されるのでOK\n",
    "    p_fold = np.clip(test_list[0][unknown_idx], 1e-6, 1-1e-6).reshape(-1,1)\n",
    "    X_meta_te = np.hstack([\n",
    "        p_fold, p_fold**2,\n",
    "        year_ohe_te[unknown_idx],\n",
    "        rev_te[unknown_idx],\n",
    "        p_fold*rev_te[unknown_idx]\n",
    "    ])\n",
    "    test_cal[unknown_idx] = meta.predict_proba(X_meta_te)[:,1]\n",
    "\n",
    "y_test = (test_cal >= best_th).astype(int)\n",
    "\n",
    "# 3) CSV（ヘッダなし2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': y_test}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (stacked calibration) = {f1_oof:.6f}  | best_th={best_th:.3f}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (stacked calibration): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Best threshold: {best_th:.6f}\\n\")\n",
    "    f.write(f\"Meta LR params: C=0.5, class_weight=balanced\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1777a",
   "metadata": {},
   "source": [
    "# === A-line v7e': 年度×Revolver の Isotonic + しきい値縮約（小セグメントは年しきい値をそのまま採用） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['train','test','years','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# 出力先\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 1) 年レベル Isotonic と年しきい値（バックアップ）\n",
    "iso_year, th_year, size_year = {}, {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y], size_year[y] = iso, th, len(idx)\n",
    "\n",
    "# 2) 年×Revolver の Isotonic と縮約しきい値（小セグメントは年しきい値を採用）\n",
    "MIN_N = 150    # セグメント最小サイズ（しきい値最適化を行う閾値）\n",
    "LAMBDA = 350   # 縮約強度（大きいほど年しきい値に寄る）\n",
    "\n",
    "iso_seg, th_seg_shrunk, seg_sizes = {}, {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        seg_sizes[(y,r)] = len(idx)\n",
    "        yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "        # calibrator\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg = th_year[y]  # ← 小セグメントは年しきい値をそのまま使う（セグメント内最適化をしない）\n",
    "\n",
    "        # しきい値縮約（年レベルへ）\n",
    "        n = len(idx)\n",
    "        alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha * th_seg + (1 - alpha) * th_year[y]\n",
    "        iso_seg[(y,r)] = iso\n",
    "        th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "# 3) OOF を縮約しきい値で二値化 → 評価\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# 4) テストにも適用（fold_id は年に対応）\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]\n",
    "        th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "# 未知年度の保険\n",
    "unknown_idx = np.where(~np.isin(year_te, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    iso_all = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "    cal_fb  = iso_all.transform(test_list[0][unknown_idx])\n",
    "    th_all, _ = _best_f1_threshold(y_all.values, iso_all.transform(oof))\n",
    "    test_pred[unknown_idx] = (cal_fb >= th_all).astype(int)\n",
    "\n",
    "# CSV（ヘッダなし2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (year×revolver, isotonic + shrink v2) = {f1_oof:.6f}  | LAMBDA={LAMBDA}, MIN_N={MIN_N}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (year×revolver, isotonic + shrink v2): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Rev: {rev_values}\\n\")\n",
    "    f.write(f\"Shrink LAMBDA: {LAMBDA}, MIN_N: {MIN_N}\\n\")\n",
    "    f.write(f\"Segment sizes: {seg_sizes}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07128269",
   "metadata": {},
   "source": [
    "# === A-line v7e: 年度×Revolver の Isotonic + しきい値縮約（年レベルへ） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['train','test','years','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# 出力先\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# 学習データのキー\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 1) 年レベル Isotonic と年しきい値（バックアップ）\n",
    "iso_year, th_year, size_year = {}, {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y], size_year[y] = iso, th, len(idx)\n",
    "\n",
    "# 2) 年×Revolver の Isotonic と縮約しきい値\n",
    "MIN_N = 120   # セグメント最小サイズ\n",
    "LAMBDA = 200  # 縮約強度（大きいほど年しきい値に寄る）\n",
    "\n",
    "iso_seg, th_seg_shrunk, seg_sizes = {}, {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        seg_sizes[(y,r)] = len(idx)\n",
    "        yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "        # calibrator\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        # しきい値縮約（年レベルへ）\n",
    "        n = len(idx)\n",
    "        alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha * th_seg + (1 - alpha) * th_year[y]\n",
    "        iso_seg[(y,r)] = iso\n",
    "        th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "# 3) OOF を縮約しきい値で二値化 → 評価\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# 4) テストにも適用（fold_id は年に対応）\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]\n",
    "        th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "# 未知年度の保険（通常空）\n",
    "unknown_idx = np.where(~np.isin(year_te, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    iso_all = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "    cal_fb  = iso_all.transform(test_list[0][unknown_idx])\n",
    "    th_all, _ = _best_f1_threshold(y_all.values, iso_all.transform(oof))\n",
    "    test_pred[unknown_idx] = (cal_fb >= th_all).astype(int)\n",
    "\n",
    "# CSV（ヘッダなし2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (year×revolver, isotonic + threshold shrinkage) = {f1_oof:.6f}  | LAMBDA={LAMBDA}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (year×revolver, isotonic + shrink): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Rev: {rev_values}\\n\")\n",
    "    f.write(f\"Shrink LAMBDA: {LAMBDA}\\n\")\n",
    "    f.write(f\"Segment sizes: {seg_sizes}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e97ed",
   "metadata": {},
   "source": [
    "# === A-line v7d: 年度×Revolver 別 Platt(ロジスティック) キャリブ + しきい値（LOYOの oof/test_list を利用）===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['train','test','years','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# 出力名（頑健）\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "def _logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def _sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "# 学習データのキー\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 年レベルのバックアップ（Platt）\n",
    "platt_year = {}\n",
    "th_year    = {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    z = _logit(pp).reshape(-1,1)\n",
    "    # 片クラスしかないなど不安定な場合は全体で代替\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        z_all = _logit(oof).reshape(-1,1)\n",
    "        lr = LogisticRegression(solver='lbfgs', max_iter=1000).fit(z_all, y_all.values)\n",
    "        p_cal = lr.predict_proba(z)[:,1]\n",
    "    else:\n",
    "        lr = LogisticRegression(solver='lbfgs', max_iter=1000).fit(z, yy)\n",
    "        p_cal = lr.predict_proba(z)[:,1]\n",
    "    th, _ = _best_f1_threshold(yy, p_cal)\n",
    "    platt_year[y] = lr\n",
    "    th_year[y]    = th\n",
    "\n",
    "# 年×Revolver の Platt としきい値（小さいセグメントは年レベルへフォールバック）\n",
    "MIN_N = 120\n",
    "platt_seg = {}\n",
    "th_seg    = {}\n",
    "oof_cal   = np.zeros_like(oof, dtype=float)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx) >= MIN_N and len(np.unique(y_all.iloc[idx].values))==2:\n",
    "            yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "            z = _logit(pp).reshape(-1,1)\n",
    "            lr = LogisticRegression(solver='lbfgs', max_iter=1000).fit(z, yy)\n",
    "            p_cal = lr.predict_proba(z)[:,1]\n",
    "            th, _ = _best_f1_threshold(yy, p_cal)\n",
    "            platt_seg[(y,r)] = lr\n",
    "            th_seg[(y,r)]    = th\n",
    "            oof_cal[idx]     = p_cal\n",
    "        else:\n",
    "            # 年レベルにフォールバック\n",
    "            yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "            z = _logit(pp).reshape(-1,1)\n",
    "            lr = platt_year[y]\n",
    "            p_cal = lr.predict_proba(z)[:,1]\n",
    "            th    = th_year[y]\n",
    "            platt_seg[(y,r)] = lr\n",
    "            th_seg[(y,r)]    = th\n",
    "            oof_cal[idx]     = p_cal\n",
    "\n",
    "# OOF をセグメントしきい値で二値化して評価\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th  = th_seg[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof_seg = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# テストにも適用（fold_idは年に対応）\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)              # その年を外したモデル\n",
    "    proba_test_y = test_list[fold_id]     # そのモデルの test 確率\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        lr = platt_seg[(y,r)]\n",
    "        th = th_seg[(y,r)]\n",
    "        zt = _logit(proba_test_y[idx_t]).reshape(-1,1)\n",
    "        cal = lr.predict_proba(zt)[:,1]\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "# 未知年度の保険\n",
    "unknown_idx = np.where(~np.isin(year_te, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    lr_all = LogisticRegression(solver='lbfgs', max_iter=1000).fit(_logit(oof).reshape(-1,1), y_all.values)\n",
    "    th_all, _ = _best_f1_threshold(y_all.values, lr_all.predict_proba(_logit(oof).reshape(-1,1))[:,1])\n",
    "    cal_fb  = lr_all.predict_proba(_logit(test_list[0][unknown_idx]).reshape(-1,1))[:,1]\n",
    "    test_pred[unknown_idx] = (cal_fb >= th_all).astype(int)\n",
    "\n",
    "# CSV（ヘッダなし2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (year×revolver, Platt-calibrated) = {f1_oof_seg:.6f}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (year×revolver, Platt): {f1_oof_seg:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Segments: {sorted([(int(y), int(r)) for (y,r) in th_seg.keys()])}\\n\")\n",
    "    f.write(f\"Min segment size = {MIN_N}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c395e",
   "metadata": {},
   "source": [
    "# === A-line v7b: 年度×Revolver 別 Isotonic + しきい値（LOYOで作った oof/test_list を利用）===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['train','test','years','folds','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# ----- 出力名（正規表現修正版・頑健化） -----\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    # ファイル名の stem（拡張子なし）から 'v<数字>' を抽出\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m:\n",
    "        nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# 学習データを (Year, RevolverStatus) で分割\n",
    "groups = {}\n",
    "rev_values = sorted(pd.Series(train['RevolverStatus']).dropna().unique().tolist())\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where(\n",
    "            (train['ApprovalFiscalYear'].values == y) &\n",
    "            (train['RevolverStatus'].values     == r)\n",
    "        )[0]\n",
    "        if len(idx) > 0:\n",
    "            groups[(y, r)] = idx\n",
    "\n",
    "MIN_N = 120  # セグメント最小サイズ（小さすぎると年レベルへフォールバック）\n",
    "\n",
    "iso_g, th_g = {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "\n",
    "for (y, r), idx in groups.items():\n",
    "    y_true = y_all.iloc[idx].values\n",
    "    p      = oof[idx]\n",
    "    # 両クラスがあり、十分な件数があるときだけセグメント別に学習\n",
    "    if len(idx) >= MIN_N and len(np.unique(y_true)) == 2:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(p, y_true)\n",
    "        cal = iso.transform(p)\n",
    "        th, _ = _best_f1_threshold(y_true, cal)\n",
    "        iso_g[(y, r)] = iso\n",
    "        th_g[(y, r)]  = th\n",
    "        oof_cal[idx]  = cal\n",
    "    else:\n",
    "        # 年レベルへフォールバック\n",
    "        idx_y = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "        y_y   = y_all.iloc[idx_y].values\n",
    "        p_y   = oof[idx_y]\n",
    "        iso_y = IsotonicRegression(out_of_bounds='clip').fit(p_y, y_y)\n",
    "        cal   = iso_y.transform(p)\n",
    "        th_y, _ = _best_f1_threshold(y_y, iso_y.transform(p_y))\n",
    "        iso_g[(y, r)] = iso_y\n",
    "        th_g[(y, r)]  = th_y\n",
    "        oof_cal[idx]  = cal\n",
    "\n",
    "# OOF をセグメント別しきい値で評価\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for (y, r), idx in groups.items():\n",
    "    y_pred_oof[idx] = (oof_cal[idx] >= th_g[(y, r)]).astype(int)\n",
    "f1_oof_seg = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# テストにもセグメント別キャリブ＆しきい値を適用\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)  # その年を外して学習したモデルのインデックス\n",
    "    proba_test_y = test_list[fold_id]  # 該当foldモデルのtest予測（確率）\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where(\n",
    "            (test['ApprovalFiscalYear'].values == y) &\n",
    "            (test['RevolverStatus'].values     == r)\n",
    "        )[0]\n",
    "        if len(idx_t) == 0:\n",
    "            continue\n",
    "        iso = iso_g.get((y, r))\n",
    "        th  = th_g.get((y, r))\n",
    "        if iso is None or th is None:\n",
    "            # 念のため年フォールバック\n",
    "            idx_y = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "            y_y   = y_all.iloc[idx_y].values\n",
    "            p_y   = oof[idx_y]\n",
    "            iso_y = IsotonicRegression(out_of_bounds='clip').fit(p_y, y_y)\n",
    "            th, _ = _best_f1_threshold(y_y, iso_y.transform(p_y))\n",
    "            cal   = iso_y.transform(proba_test_y[idx_t])\n",
    "            test_pred[idx_t] = (cal >= th).astype(int)\n",
    "        else:\n",
    "            cal = iso.transform(proba_test_y[idx_t])\n",
    "            test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "# 未知年度フォールバック（通常は不要）\n",
    "unknown_idx = np.where(~np.isin(test['ApprovalFiscalYear'].values, years))[0]\n",
    "if len(unknown_idx) > 0:\n",
    "    iso_all = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all)\n",
    "    cal_fb  = iso_all.transform(test_list[0][unknown_idx])\n",
    "    th_all, _ = _best_f1_threshold(y_all, iso_all.transform(oof))\n",
    "    test_pred[unknown_idx] = (cal_fb >= th_all).astype(int)\n",
    "\n",
    "# CSV（ヘッダなし2列）を出力\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "# ログ\n",
    "print(f\"🏁 OOF F1 (year×revolver calibrated) = {f1_oof_seg:.6f}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (year×revolver): {f1_oof_seg:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Segments: {sorted(list(groups.keys()))}\\n\")\n",
    "    f.write(f\"Min segment size = {MIN_N}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9197e",
   "metadata": {},
   "source": [
    "# === A-line v7: 年度別 Isotonic + 年度別しきい値（LOYOで作ったoof/test_listを利用）===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# v4セルで作ったこれらが必要\n",
    "need = ['train','test','years','folds','oof','test_list','y_all','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4 を実行して: {need}\"\n",
    "\n",
    "# 出力名\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# 年度→fold index\n",
    "year_to_fold = {y:i for i, y in enumerate(years)}\n",
    "\n",
    "# 年度別 Isotonic & しきい値\n",
    "iso_by_year, th_by_year = {}, {}\n",
    "oof_cal_by_year = np.zeros_like(oof, dtype=float)\n",
    "\n",
    "for y in years:\n",
    "    idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    oof_y = oof[idx]              # ★v4で作った生OOF確率\n",
    "    y_y   = y_all.iloc[idx]\n",
    "\n",
    "    iso_y = IsotonicRegression(out_of_bounds='clip').fit(oof_y, y_y)\n",
    "    cal_y = iso_y.transform(oof_y)\n",
    "    th_y, _ = _best_f1_threshold(y_y, cal_y)\n",
    "\n",
    "    iso_by_year[y] = iso_y\n",
    "    th_by_year[y]  = th_y\n",
    "    oof_cal_by_year[idx] = cal_y\n",
    "\n",
    "# OOFを年度別閾値で評価\n",
    "y_pred_oof = np.zeros_like(oof_cal_by_year, dtype=int)\n",
    "for y in years:\n",
    "    idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    y_pred_oof[idx] = (oof_cal_by_year[idx] >= th_by_year[y]).astype(int)\n",
    "f1_oof_peryear = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# test も年度別にキャリブ＆閾値適用\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "if 'ApprovalFiscalYear' in test.columns:\n",
    "    for y in years:\n",
    "        fold_id = year_to_fold[y]\n",
    "        proba_test_y = test_list[fold_id]  # その年を外したモデルのtest予測（v4で作成）\n",
    "        idx_test = np.where(test['ApprovalFiscalYear'].values == y)[0]\n",
    "        if len(idx_test) == 0: \n",
    "            continue\n",
    "        cal_test_y = iso_by_year[y].transform(proba_test_y[idx_test])\n",
    "        test_pred[idx_test] = (cal_test_y >= th_by_year[y]).astype(int)\n",
    "\n",
    "# 未知年度があれば、全体Iso/閾値（v4の iso/best_th）でフォールバック\n",
    "unknown_idx = np.where(~np.isin(test['ApprovalFiscalYear'].values, years))[0]\n",
    "if len(unknown_idx) > 0 and 'iso' in globals() and 'best_th' in globals():\n",
    "    cal_fb = iso.transform(test_list[0][unknown_idx])\n",
    "    test_pred[unknown_idx] = (cal_fb >= best_th).astype(int)\n",
    "\n",
    "# CSV（ヘッダ無し2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 Year-wise OOF F1 = {f1_oof_peryear:.6f} | thresholds={th_by_year}\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Year-wise OOF F1: {f1_oof_peryear:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Thresholds by year: {th_by_year}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a652d",
   "metadata": {},
   "source": [
    "# === A-line v6：CV(OOF F1)ゲート付き・提出ワンショット（LOYOキャリブ固定＋TTA平均） ===\n",
    "import os, re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ── 前提：v4セルを実行済みで、以下が定義済み ──\n",
    "# X_all, T_all, y_all, iso, best_th, base_params, n_rounds_full, OUT_DIR, test, DATA_DIR\n",
    "assert all(k in globals() for k in [\n",
    "    'X_all','T_all','y_all','iso','best_th','base_params','n_rounds_full','OUT_DIR','test','DATA_DIR'\n",
    "]), \"先に v4（LOYO）セルを実行してから、このセルを実行して\"\n",
    "\n",
    "# ── 1) 直近のOOF F1（v4で計算）を取得 ──\n",
    "def read_last_oof_from_log(out_dir: Path):\n",
    "    logs = sorted(out_dir.glob(\"run_A_v*.txt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for p in logs:\n",
    "        txt = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        m = re.search(r\"(LOYO OOF F1 \\(calibrated\\)|OOF F1 \\(calibrated×seeds\\))\\s*:\\s*([0-9.]+)\", txt)\n",
    "        if m: return float(m.group(2))\n",
    "    return None\n",
    "\n",
    "if 'f1_cal' in globals():\n",
    "    current_oof = float(f1_cal)\n",
    "else:\n",
    "    current_oof = read_last_oof_from_log(OUT_DIR)\n",
    "    assert current_oof is not None, \"OOF F1 が見つからない。v4セル実行直後にこのセルを実行して\"\n",
    "\n",
    "print(f\"🔎 current OOF F1 = {current_oof:.6f}\")\n",
    "\n",
    "# ── 2) 目標と履歴（最良OOF）をロード ──\n",
    "TARGET_OOF = 0.640      # 目標（君の指定）\n",
    "MARGIN     = 0.002      # 最低更新幅\n",
    "best_file  = OUT_DIR / \"best_oof.json\"\n",
    "if best_file.exists():\n",
    "    best_rec = json.loads(best_file.read_text(encoding=\"utf-8\"))\n",
    "    best_oof = float(best_rec.get(\"best_oof\", 0.0))\n",
    "else:\n",
    "    best_oof = 0.0\n",
    "print(f\"📈 best OOF so far = {best_oof:.6f} (gate: >= {TARGET_OOF:.3f} and ≥ best+{MARGIN:.3f})\")\n",
    "\n",
    "# ── 3) ゲート判定 ──\n",
    "improved_enough = (current_oof >= TARGET_OOF) and (current_oof >= best_oof + MARGIN)\n",
    "if not improved_enough:\n",
    "    print(f\"⛔ 提出しない: OOF={current_oof:.6f} < max({TARGET_OOF:.3f}, best+{MARGIN:.3f}={best_oof+MARGIN:.3f})\")\n",
    "else:\n",
    "    # ── 4) テスト時TTA（subsample/colsample微揺らし × seeds）で確率平均 → LOYOのキャリブ＆閾値で二値化 ──\n",
    "    jitters = [-0.06, 0.0, 0.06]\n",
    "    seeds   = [0, 1]\n",
    "    preds   = []\n",
    "    for dj in jitters:\n",
    "        ss = min(1.0, max(0.60, float(base_params['subsample'])        + dj))\n",
    "        cs = min(1.0, max(0.60, float(base_params['colsample_bytree']) + dj))\n",
    "        for sd in seeds:\n",
    "            params = dict(base_params)\n",
    "            params.update(dict(subsample=ss, colsample_bytree=cs, random_state=sd))\n",
    "            model = LGBMClassifier(**params, n_estimators=int(n_rounds_full))\n",
    "            model.fit(X_all, y_all)\n",
    "            preds.append(model.predict_proba(T_all)[:, 1])\n",
    "\n",
    "    test_proba = np.mean(np.vstack(preds), axis=0)\n",
    "    test_cal   = iso.transform(test_proba)\n",
    "    y_pred     = (test_cal >= float(best_th)).astype(int)\n",
    "\n",
    "    # ── 5) 提出CSV（ヘッダ無し2列）を一個だけ出力 ──\n",
    "    existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "    next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "    out_csv  = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "    out_log  = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "    # ── 6) 履歴更新＆ログ ──\n",
    "    best_file.write_text(json.dumps({\"best_oof\": current_oof}, indent=2), encoding=\"utf-8\")\n",
    "    with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Gate submit: OOF={current_oof:.6f} (target {TARGET_OOF}, best prev {best_oof:.6f})\\n\")\n",
    "        f.write(f\"best_th(from LOYO)={best_th}\\n\")\n",
    "        f.write(f\"params(base)={base_params}, n_rounds_full={n_rounds_full}\\n\")\n",
    "        f.write(f\"jitters={jitters}, seeds={seeds}\\n\")\n",
    "        f.write(f\"output={out_csv}\\n\")\n",
    "\n",
    "    print(f\"✅ 提出CSVを出力: {out_csv}\")\n",
    "    print(f\"📝 履歴更新: {best_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6398dbcb",
   "metadata": {},
   "source": [
    "# === Aライン v5：LOYOのキャリブを固定したまま、全学習データで TTA バギングして提出1本 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# 前提：v4セルを実行済みで、以下が存在\n",
    "# X_all, T_all, y_all, iso, best_th, base_params, n_rounds_full, OUT_DIR, test, DATA_DIR\n",
    "\n",
    "# 1) 出力ファイル名を決定\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "next_ver = (max(int(re.search(r\"v(\\d+)\", p.name).group(1)) for p in existing) + 1) if existing else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# 2) TTA 設定（パラメータの微揺らし × seed）\n",
    "jitters = [-0.06, 0.0, +0.06]   # subsample / colsample に同量を足し引き\n",
    "seeds   = [0, 1]                # 合計 3×2 = 6 本（軽量運用）\n",
    "preds   = []\n",
    "\n",
    "for dj in jitters:\n",
    "    ss = min(1.0, max(0.60, float(base_params['subsample'])       + dj))\n",
    "    cs = min(1.0, max(0.60, float(base_params['colsample_bytree'])+ dj))\n",
    "    for sd in seeds:\n",
    "        params = dict(base_params)\n",
    "        params.update(dict(subsample=ss, colsample_bytree=cs, random_state=sd))\n",
    "        model = LGBMClassifier(**params, n_estimators=int(n_rounds_full))\n",
    "        model.fit(X_all, y_all)\n",
    "        preds.append(model.predict_proba(T_all)[:, 1])\n",
    "\n",
    "# 3) テスト確率を平均 → v4 の Isotonic と閾値で二値化\n",
    "test_proba_mean = np.mean(np.vstack(preds), axis=0)\n",
    "test_cal        = iso.transform(test_proba_mean)\n",
    "y_pred_test     = (test_cal >= float(best_th)).astype(int)\n",
    "\n",
    "# 4) CSV 出力（ヘッダなし2列）\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🚀 TTA済み提出を保存: {out_csv}\")\n",
    "print(f\"  jitters={jitters}, seeds={seeds}, n_models={len(preds)}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"A-line v5: TTA bagging on full data with LOYO calibration\\n\")\n",
    "    f.write(f\"jitters={jitters}, seeds={seeds}, n_models={len(preds)}\\n\")\n",
    "    f.write(f\"best_th(from v4)={best_th}\\n\")\n",
    "    f.write(f\"params(base)={base_params}, n_rounds_full={n_rounds_full}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ff219b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1187]\tvalid_0's binary_logloss: 0.246696\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1619]\tvalid_0's binary_logloss: 0.246183\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1117]\tvalid_0's binary_logloss: 0.24399\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1632]\tvalid_0's binary_logloss: 0.243819\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1260]\tvalid_0's binary_logloss: 0.240388\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[355]\tvalid_0's binary_logloss: 0.264313\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[370]\tvalid_0's binary_logloss: 0.265997\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[352]\tvalid_0's binary_logloss: 0.263843\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[424]\tvalid_0's binary_logloss: 0.265261\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[423]\tvalid_0's binary_logloss: 0.264\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[881]\tvalid_0's binary_logloss: 0.279662\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[841]\tvalid_0's binary_logloss: 0.280542\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[800]\tvalid_0's binary_logloss: 0.27954\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's binary_logloss: 0.275932\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[897]\tvalid_0's binary_logloss: 0.279802\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.272728\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[952]\tvalid_0's binary_logloss: 0.275245\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[645]\tvalid_0's binary_logloss: 0.273175\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[874]\tvalid_0's binary_logloss: 0.275541\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[766]\tvalid_0's binary_logloss: 0.268906\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1486]\tvalid_0's binary_logloss: 0.188264\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1843]\tvalid_0's binary_logloss: 0.180873\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1720]\tvalid_0's binary_logloss: 0.185193\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2070]\tvalid_0's binary_logloss: 0.176774\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1165]\tvalid_0's binary_logloss: 0.193118\n",
      "🏁 OOF F1 (v7h: seeds5 + seg-calib) = 0.625819 | rounds(avg)≈1009\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v33.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v33.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7h: v7gにロールバック + seeds=5 + bagging_freq=1（提出1本） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# v4（特徴生成）と optuna セルを実行済みで、以下が定義済みであること\n",
    "need = ['X_all','T_all','y_all','train','test','years','best_params','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"先に v4/optuna を実行して次を用意して: {need}\"\n",
    "\n",
    "# 出力ファイル名（連番）\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# ---- 1) LOYO × multi-seed bagging（seeds=5, bagging_freq=1）----\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "base_params = dict(best_params)\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "# bagging を明示的に有効化（row subsampling を各ツリーで実施）\n",
    "base_params['bagging_freq'] = 1  # = bagging を毎ツリー有効化\n",
    "\n",
    "oof_bag = np.zeros(len(X_all), dtype=float)\n",
    "test_list_bag = []\n",
    "best_rounds_each = []\n",
    "\n",
    "for y in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    proba_va_seeds = []\n",
    "    proba_te_seeds = []\n",
    "\n",
    "    for sd in seeds:\n",
    "        params = dict(base_params)\n",
    "        params['random_state'] = sd\n",
    "        model = LGBMClassifier(**params, n_estimators=4000)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=150), log_evaluation(period=0)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "        best_rounds_each.append(best_it)\n",
    "        proba_va_seeds.append(model.predict_proba(Xva, num_iteration=best_it)[:,1])\n",
    "        proba_te_seeds.append(model.predict_proba(T_all, num_iteration=best_it)[:,1])\n",
    "\n",
    "    oof_bag[va_idx] = np.mean(proba_va_seeds, axis=0)\n",
    "    test_list_bag.append(np.mean(proba_te_seeds, axis=0))\n",
    "\n",
    "# ---- 2) 年×Revolver Isotonic + しきい値縮約（LAMBDA=200, MIN_N=120）----\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 年バックアップ\n",
    "iso_year, th_year = {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof_bag[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof_bag, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y] = iso, th\n",
    "\n",
    "MIN_N, LAMBDA = 120, 200\n",
    "iso_seg, th_seg_shrunk = {}, {}\n",
    "oof_cal = np.zeros_like(oof_bag, dtype=float)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all.iloc[idx].values, oof_bag[idx]\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]; pcal = iso.transform(pp); th_seg = th_year[y]\n",
    "        n = len(idx); alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha * th_seg + (1 - alpha) * th_year[y]\n",
    "        iso_seg[(y,r)] = iso; th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "y_pred_oof = np.zeros_like(oof_bag, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# ---- 3) テスト適用・提出（ヘッダ無し2列）----\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list_bag[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]; th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (v7h: seeds5 + seg-calib) = {f1_oof:.6f} | rounds(avg)≈{int(np.mean(best_rounds_each))}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (v7h): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Seeds: {seeds}\\n\")\n",
    "    f.write(f\"Best rounds(each): {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {base_params}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6757c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[versioning] next version = v35\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1200]\tvalid_0's binary_logloss: 0.258329\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1036]\tvalid_0's binary_logloss: 0.268366\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1001]\tvalid_0's binary_logloss: 0.259269\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[791]\tvalid_0's binary_logloss: 0.258139\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[409]\tvalid_0's binary_logloss: 0.267094\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[312]\tvalid_0's binary_logloss: 0.267755\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's binary_logloss: 0.266553\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's binary_logloss: 0.266388\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[893]\tvalid_0's binary_logloss: 0.285658\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[839]\tvalid_0's binary_logloss: 0.28226\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.284846\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[821]\tvalid_0's binary_logloss: 0.282798\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[733]\tvalid_0's binary_logloss: 0.281641\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[726]\tvalid_0's binary_logloss: 0.285287\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[752]\tvalid_0's binary_logloss: 0.276413\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[871]\tvalid_0's binary_logloss: 0.280178\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1451]\tvalid_0's binary_logloss: 0.180842\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1720]\tvalid_0's binary_logloss: 0.187838\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1114]\tvalid_0's binary_logloss: 0.193799\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1363]\tvalid_0's binary_logloss: 0.18266\n",
      "🏁 OOF F1 (v7g_restore) = 0.620431 | rounds(avg)≈878 | seeds=[0, 1, 2, 3]\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v35.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v35.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7g_restore: 年×Revolver Isotonic + しきい値縮約（TTAなし）で安定化（提出1本） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# v4/optuna まで実行済み前提\n",
    "need = ['X_all','T_all','y_all','train','test','years','best_params','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"不足: {need}\"\n",
    "\n",
    "# ----- 出力ファイルの連番を安全に決める（CSV/LOGの両方から最大を取る） -----\n",
    "import re\n",
    "\n",
    "def get_next_ver(dirpath):\n",
    "    nums = []\n",
    "    for p in dirpath.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r'v(\\d+)$', p.stem)  # 例: submission_A_v12\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    for p in dirpath.glob(\"run_A_v*.txt\"):\n",
    "        m = re.search(r'v(\\d+)$', p.stem)  # 例: run_A_v12\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums) + 1 if nums else 1\n",
    "    # 念のため、空き番号が見つかるまで進める\n",
    "    while (dirpath / f\"submission_A_v{n}.csv\").exists() or (dirpath / f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "ver     = get_next_ver(OUT_DIR)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "print(f\"[versioning] next version = v{ver}\")\n",
    "\n",
    "\n",
    "# ---- LOYO × multi-seed（TTAなし、bagging_freq も無し）----\n",
    "seeds = [0, 1, 2, 3]  # 実績帯の安定セット\n",
    "base_params = dict(best_params)                 # optuna の best を尊重\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "base_params.pop('k_smooth', None)               # 学習用では不要\n",
    "\n",
    "oof = np.zeros(len(X_all), dtype=float)\n",
    "test_list = []\n",
    "best_rounds_each = []\n",
    "\n",
    "for y in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    proba_va_seeds, proba_te_seeds = [], []\n",
    "    for sd in seeds:\n",
    "        params = dict(base_params); params['random_state'] = sd\n",
    "        model = LGBMClassifier(**params, n_estimators=4000)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=150), log_evaluation(period=0)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "        best_rounds_each.append(best_it)\n",
    "        proba_va_seeds.append(model.predict_proba(Xva, num_iteration=best_it)[:,1])\n",
    "        proba_te_seeds.append(model.predict_proba(T_all, num_iteration=best_it)[:,1])\n",
    "\n",
    "    oof[va_idx] = np.mean(proba_va_seeds, axis=0)\n",
    "    test_list.append(np.mean(proba_te_seeds, axis=0))\n",
    "\n",
    "# ---- 年×Revolver Isotonic + しきい値縮約（LAMBDA=200, MIN_N=120）----\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "# 年ごとにバックアップ・キャリブ\n",
    "iso_year, th_year = {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y] = iso, th\n",
    "\n",
    "MIN_N, LAMBDA = 120, 200\n",
    "iso_seg, th_seg_shrunk = {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]; pcal = iso.transform(pp); th_seg = th_year[y]\n",
    "        n = len(idx); alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha*th_seg + (1-alpha)*th_year[y]\n",
    "        iso_seg[(y,r)] = iso; th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# ---- テストに適用・提出（ヘッダ無し2列）----\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]; th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (v7g_restore) = {f1_oof:.6f} | rounds(avg)≈{int(np.mean(best_rounds_each))} | seeds={seeds}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (v7g_restore): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Seeds: {seeds}\\n\")\n",
    "    f.write(f\"Best rounds(each): {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {base_params}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6540e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[937]\tvalid_0's binary_logloss: 0.258321\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's binary_logloss: 0.268136\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[704]\tvalid_0's binary_logloss: 0.278978\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[713]\tvalid_0's binary_logloss: 0.279281\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1237]\tvalid_0's binary_logloss: 0.189309\n",
      "🏁 OOF F1 (v8: robustZ + LOYO + seg-calib) = 0.615721 | rounds(avg)≈788 | LAMBDA=300, MIN_N=150\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v34.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v34.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8: 年内ロバストZ特徴 + LOYO + 年×Revolver Isotonic縮約（提出1本） ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# 必要変数チェック\n",
    "need0 = ['train_df','test_df','best_params','DATA_DIR']\n",
    "assert all(k in globals() for k in need0), f\"先に前処理セルと Optuna を実行: {need0}\"\n",
    "\n",
    "# 出力先・版管理\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = sorted(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "nums = []\n",
    "for p in existing:\n",
    "    m = re.search(r'v(\\d+)', p.stem)\n",
    "    if m: nums.append(int(m.group(1)))\n",
    "next_ver = (max(nums) + 1) if nums else 1\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "# 1) テーブル用意\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int)\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "\n",
    "# 2) 年内ロバストZ特徴（median/IQR）\n",
    "num_cols_base = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2'\n",
    "]\n",
    "def add_yearwise_robust_z(tr, te, cols, year_col='ApprovalFiscalYear', clip=5.0):\n",
    "    tr = tr.copy(); te = te.copy()\n",
    "    stats = {}\n",
    "    for c in cols:\n",
    "        g = train.groupby(year_col)[c]\n",
    "        med = g.median()\n",
    "        iqr = (g.quantile(0.75) - g.quantile(0.25)).replace(0, 1e-6)\n",
    "        stats[c] = (med, iqr)\n",
    "        # train\n",
    "        z_tr = (tr[c] - tr[year_col].map(med)) / tr[year_col].map(iqr)\n",
    "        tr[c+'_zY'] = z_tr.clip(-clip, clip)\n",
    "        # test（未知年は全体median/IQR）\n",
    "        glob_med, glob_iqr = float(train[c].median()), float((train[c].quantile(0.75)-train[c].quantile(0.25)) or 1e-6)\n",
    "        med_map = te[year_col].map(med).fillna(glob_med)\n",
    "        iqr_map = te[year_col].map(iqr).fillna(glob_iqr)\n",
    "        z_te = (te[c] - med_map) / iqr_map\n",
    "        te[c+'_zY'] = z_te.clip(-clip, clip)\n",
    "    return tr, te, [c+'_zY' for c in cols]\n",
    "\n",
    "train, test, z_cols = add_yearwise_robust_z(train, test, num_cols_base)\n",
    "\n",
    "# 3) カテゴリ符号化 + TE/FE（LOYO OOFでリーク回避）\n",
    "cat_cols = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "def fit_category_codes(df, cols):\n",
    "    m = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(df[c]).categories\n",
    "        m[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return m\n",
    "def apply_category_codes(df, maps):\n",
    "    out = df.copy()\n",
    "    for c,mp in maps.items():\n",
    "        out[c+\"_code\"] = df[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "code_maps  = fit_category_codes(train, cat_cols)\n",
    "train_code = apply_category_codes(train, code_maps)\n",
    "test_code  = apply_category_codes(test,  code_maps)\n",
    "\n",
    "def add_te_fe_oof_with_folds(train_df, y, test_df, cols, folds, k_smooth=10):\n",
    "    df = train_df.reset_index(drop=True).copy()\n",
    "    y  = pd.Series(y).reset_index(drop=True).rename('y')\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype('object')\n",
    "        test_df[c] = test_df[c].astype('object')\n",
    "\n",
    "    te_tr = pd.DataFrame(index=df.index); fe_tr = pd.DataFrame(index=df.index)\n",
    "    te_te = pd.DataFrame(index=test_df.index); fe_te = pd.DataFrame(index=test_df.index)\n",
    "    gmean = float(y.mean())\n",
    "\n",
    "    for col in cols:\n",
    "        te_tr[col+\"_teCV\"] = np.nan; fe_tr[col+\"_feCV\"] = np.nan\n",
    "        for va_idx in folds:\n",
    "            tr_idx = np.setdiff1d(np.arange(len(df)), va_idx)\n",
    "            tr_col = df[col].iloc[tr_idx].reset_index(drop=True)\n",
    "            yy_tr  = y.iloc[tr_idx].reset_index(drop=True)\n",
    "            agg = pd.DataFrame({'col': tr_col, 'y': yy_tr}).groupby('col')['y'].agg(['count','mean'])\n",
    "            smooth = (agg['mean']*agg['count'] + gmean*k_smooth) / (agg['count'] + k_smooth)\n",
    "            m  = df[col].iloc[va_idx].map(smooth)\n",
    "            te_tr.loc[va_idx, col+\"_teCV\"] = pd.Series(m, index=va_idx).fillna(gmean).to_numpy()\n",
    "            f  = tr_col.value_counts(normalize=True)\n",
    "            mf = df[col].iloc[va_idx].map(f)\n",
    "            fe_tr.loc[va_idx, col+\"_feCV\"] = pd.Series(mf, index=va_idx).fillna(0.0).to_numpy()\n",
    "\n",
    "        agg_full = pd.DataFrame({'col': df[col], 'y': y}).groupby('col')['y'].agg(['count','mean'])\n",
    "        smooth_f = (agg_full['mean']*agg_full['count'] + gmean*k_smooth) / (agg_full['count'] + k_smooth)\n",
    "        te_te[col+\"_teCV\"] = pd.Series(test_df[col].map(smooth_f)).fillna(gmean).to_numpy()\n",
    "        freq_full = df[col].value_counts(normalize=True)\n",
    "        fe_te[col+\"_feCV\"] = pd.Series(test_df[col].map(freq_full)).fillna(0.0).to_numpy()\n",
    "\n",
    "    return (pd.concat([df.reset_index(drop=True), te_tr, fe_tr], axis=1),\n",
    "            pd.concat([test_df.reset_index(drop=True), te_te, fe_te], axis=1))\n",
    "\n",
    "# 交差列（年×業種）も含める\n",
    "train['YearSector'] = train['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" + train['NaicsSector'].astype('string').fillna('UNK')\n",
    "test ['YearSector'] = test ['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + \"_\" + test ['NaicsSector'].astype('string').fillna('UNK')\n",
    "te_cols = ['Subprogram','NaicsSector','BusinessAge','BusinessType','YearSector']\n",
    "\n",
    "# LOYO folds\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == y)[0] for y in years]\n",
    "tr_tefe, te_tefe = add_te_fe_oof_with_folds(train[te_cols], y_all, test[te_cols], te_cols, folds, k_smooth=int(best_params.get('k_smooth', 9)))\n",
    "\n",
    "# 4) 特徴表（コード + TE/FE + 年内Z）\n",
    "base_feats = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "    'CongressionalDistrict','RevolverStatus'\n",
    "]\n",
    "X_all = pd.concat([\n",
    "    train_code[base_feats],\n",
    "    train_code[[c+\"_code\" for c in cat_cols]],\n",
    "    tr_tefe[[c+\"_teCV\" for c in te_cols]],\n",
    "    tr_tefe[[c+\"_feCV\" for c in te_cols]],\n",
    "    train[z_cols],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "T_all = pd.concat([\n",
    "    test_code[base_feats],\n",
    "    test_code[[c+\"_code\" for c in cat_cols]],\n",
    "    te_tefe[[c+\"_teCV\" for c in te_cols]],\n",
    "    te_tefe[[c+\"_feCV\" for c in te_cols]],\n",
    "    test[z_cols],\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "# 5) LOYO で学習（各年1モデル, ES使用）→ oof / test_list\n",
    "oof = np.zeros(len(X_all)); test_list = []; best_rounds_each = []\n",
    "params = dict(best_params); params.pop('k_smooth', None)\n",
    "params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "for y in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == y)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "    model = LGBMClassifier(**params, n_estimators=4000)\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        eval_metric='binary_logloss',\n",
    "        callbacks=[early_stopping(stopping_rounds=150), log_evaluation(period=0)]\n",
    "    )\n",
    "    best_it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "    best_rounds_each.append(best_it)\n",
    "    oof[va_idx] = model.predict_proba(Xva, num_iteration=best_it)[:,1]\n",
    "    test_list.append(model.predict_proba(T_all, num_iteration=best_it)[:,1])\n",
    "\n",
    "# 6) 年×Revolver Isotonic + しきい値縮約（λ, MIN_N はやや強め）\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "iso_year, th_year = {}, {}\n",
    "for y in years:\n",
    "    idx = np.where(year_tr == y)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "    th, _ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[y], th_year[y] = iso, th\n",
    "\n",
    "MIN_N, LAMBDA = 150, 300\n",
    "iso_seg, th_seg_shrunk = {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "        if len(idx) >= MIN_N and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg, _ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[y]; pcal = iso.transform(pp); th_seg = th_year[y]\n",
    "        n = len(idx); alpha = n / (n + LAMBDA)\n",
    "        th_shrunk = alpha * th_seg + (1 - alpha) * th_year[y]\n",
    "        iso_seg[(y,r)] = iso; th_seg_shrunk[(y,r)] = float(th_shrunk)\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for y in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==y)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_shrunk[(y,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# 7) テスト適用・提出\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for y in years:\n",
    "    fold_id = years.index(y)\n",
    "    proba_test_y = test_list[fold_id]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==y)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(y,r)]; th  = th_seg_shrunk[(y,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR) / 'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "print(f\"🏁 OOF F1 (v8: robustZ + LOYO + seg-calib) = {f1_oof:.6f} | rounds(avg)≈{int(np.mean(best_rounds_each))} | LAMBDA={LAMBDA}, MIN_N={MIN_N}\")\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF F1 (v8): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Best rounds per fold: {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {params}\\n\")\n",
    "    f.write(f\"Z cols: {z_cols}\\n\")\n",
    "    f.write(f\"LAMBDA={LAMBDA}, MIN_N={MIN_N}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a02239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1308]\tvalid_0's binary_logloss: 0.230638\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1336]\tvalid_0's binary_logloss: 0.230759\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1197]\tvalid_0's binary_logloss: 0.23242\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1192]\tvalid_0's binary_logloss: 0.232359\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1343]\tvalid_0's binary_logloss: 0.233754\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[688]\tvalid_0's binary_logloss: 0.291907\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[647]\tvalid_0's binary_logloss: 0.290255\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[630]\tvalid_0's binary_logloss: 0.294117\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[682]\tvalid_0's binary_logloss: 0.290357\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[689]\tvalid_0's binary_logloss: 0.29157\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1215]\tvalid_0's binary_logloss: 0.358805\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1169]\tvalid_0's binary_logloss: 0.35965\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1242]\tvalid_0's binary_logloss: 0.359646\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1199]\tvalid_0's binary_logloss: 0.358124\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1128]\tvalid_0's binary_logloss: 0.360305\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[826]\tvalid_0's binary_logloss: 0.296354\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[873]\tvalid_0's binary_logloss: 0.297758\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[864]\tvalid_0's binary_logloss: 0.293848\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[908]\tvalid_0's binary_logloss: 0.29286\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[958]\tvalid_0's binary_logloss: 0.293253\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2110]\tvalid_0's binary_logloss: 0.160626\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.171901\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2547]\tvalid_0's binary_logloss: 0.16707\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1842]\tvalid_0's binary_logloss: 0.170178\n",
      "Training until validation scores don't improve for 150 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2112]\tvalid_0's binary_logloss: 0.168511\n",
      "🏁 OOF F1 (v7m+, YearProg 追加) = 0.618764\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v41.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v41.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7m+: LOYO 交差TE(Year×Sector, Year×Age, Year×Subprogram) 追加 → LOYO×seeds×(year×revolver) キャリブ & 提出1本 ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "\n",
    "# 依存\n",
    "need = ['train_df','test_df','OUT_DIR','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"不足: {need}\"\n",
    "\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "\n",
    "# 1) ラベルエンコード（*_code を作る）\n",
    "y_all = train['LoanStatus'].astype(int)\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "\n",
    "cat_cols_auto = train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "cat_cols_auto = [c for c in cat_cols_auto if c not in ('id','LoanStatus')]\n",
    "for c in cat_cols_auto:\n",
    "    cats = pd.Categorical(train[c]).categories\n",
    "    train[c+'_code'] = pd.Categorical(train[c], categories=cats).codes\n",
    "    test [c+'_code'] = pd.Categorical(test [c], categories=cats).codes\n",
    "\n",
    "# 2) 交差TEキー（Year×Sector, Year×Age, ★Year×Subprogram）\n",
    "def _mk_key(s1, s2):\n",
    "    return s1.astype('Int64').astype('string').fillna('UNK') + '_' + s2.astype('string').fillna('UNK')\n",
    "\n",
    "train['YearSector'] = _mk_key(train['ApprovalFiscalYear'], train['NaicsSector'])\n",
    "test ['YearSector'] = _mk_key(test ['ApprovalFiscalYear'], test ['NaicsSector'])\n",
    "train['YearAge']    = _mk_key(train['ApprovalFiscalYear'], train['BusinessAge'])\n",
    "test ['YearAge']    = _mk_key(test ['ApprovalFiscalYear'], test ['BusinessAge'])\n",
    "# ★ 追加\n",
    "train['YearProg']   = _mk_key(train['ApprovalFiscalYear'], train['Subprogram'])\n",
    "test ['YearProg']   = _mk_key(test ['ApprovalFiscalYear'], test ['Subprogram'])\n",
    "\n",
    "# 3) LOYO-OOF ターゲットエンコード\n",
    "def add_loyo_te(train_key: pd.Series, y: pd.Series, test_key: pd.Series,\n",
    "                year_col: pd.Series, k_smooth: int = 9):\n",
    "    train_key = train_key.astype('object').reset_index(drop=True)\n",
    "    test_key  = test_key.astype('object').reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    year_col = pd.Series(year_col).reset_index(drop=True)\n",
    "\n",
    "    gmean = float(y.mean())\n",
    "    oof = np.zeros(len(train_key), dtype=float)\n",
    "    for yr in sorted(year_col.dropna().unique()):\n",
    "        va_idx = np.where(year_col.values == yr)[0]\n",
    "        tr_idx = np.setdiff1d(np.arange(len(train_key)), va_idx)\n",
    "        tr_k, yy = train_key.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        agg = pd.DataFrame({'k': tr_k, 'y': yy}).groupby('k')['y'].agg(['count','mean'])\n",
    "        cnt, m = agg['count'], agg['mean']\n",
    "        smooth = (m*cnt + gmean*k_smooth) / (cnt + k_smooth)\n",
    "        m_va = train_key.iloc[va_idx].map(smooth)\n",
    "        oof[va_idx] = m_va.fillna(gmean).to_numpy()\n",
    "\n",
    "    agg_full = pd.DataFrame({'k': train_key, 'y': y}).groupby('k')['y'].agg(['count','mean'])\n",
    "    cnt, m = agg_full['count'], agg_full['mean']\n",
    "    smooth_full = (m*cnt + gmean*k_smooth) / (cnt + k_smooth)\n",
    "    test_te = pd.Series(test_key.map(smooth_full)).fillna(gmean).to_numpy()\n",
    "    return oof, test_te\n",
    "\n",
    "# ★ YearProg を含める\n",
    "te_cols = ['YearSector','YearAge','YearProg']\n",
    "oof_dict = {}; test_te_dict = {}\n",
    "for col in te_cols:\n",
    "    oof_dict[col], test_te_dict[col] = add_loyo_te(\n",
    "        train[col], y_all, test[col], train['ApprovalFiscalYear'], k_smooth=9\n",
    "    )\n",
    "\n",
    "# 4) 学習・推論行列（重複列回避）\n",
    "def _uniq(cols): return list(dict.fromkeys(cols))\n",
    "num_cols = train.select_dtypes(include=['number']).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in ('LoanStatus','id') and not c.endswith(('_code','_teCV','_feCV'))]\n",
    "code_cols = [c for c in train.columns if c.endswith('_code')]\n",
    "\n",
    "X_all = train[_uniq(num_cols + code_cols)].copy()\n",
    "T_all = test [_uniq(num_cols + code_cols)].copy()\n",
    "for col in te_cols:\n",
    "    X_all[col+'_teCV'] = oof_dict[col]\n",
    "    T_all[col+'_teCV'] = test_te_dict[col]\n",
    "\n",
    "X_all = X_all.replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "T_all = T_all.replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "assert int(X_all.columns.duplicated().sum()) == 0\n",
    "\n",
    "# 5) モデル設定\n",
    "if 'best_params' in globals():\n",
    "    base_params = dict(best_params); base_params.pop('k_smooth', None)\n",
    "else:\n",
    "    base_params = dict(learning_rate=0.03, num_leaves=96, max_depth=12,\n",
    "                       min_child_samples=40, subsample=0.85, colsample_bytree=0.95,\n",
    "                       reg_alpha=1.5, reg_lambda=5.0)\n",
    "base_params.update(dict(class_weight='balanced', verbosity=-1))\n",
    "\n",
    "# 6) LOYO × seeds=0..4（rank平均）\n",
    "def _ranknormalize(a: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(a); r = np.empty_like(order, dtype=float); r[order] = np.arange(len(a), dtype=float)\n",
    "    return r / max(len(a)-1, 1)\n",
    "\n",
    "seeds = [0,1,2,3,4]\n",
    "oof = np.zeros(len(X_all), dtype=float)\n",
    "test_list = []\n",
    "best_rounds_each = []\n",
    "\n",
    "for yr in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == yr)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    p_va, p_te = [], []\n",
    "    for sd in seeds:\n",
    "        params = dict(base_params); params['random_state'] = sd\n",
    "        model = LGBMClassifier(**params, n_estimators=4000)\n",
    "        model.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)], eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=150), log_evaluation(period=0)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "        best_rounds_each.append(best_it)\n",
    "        p_va.append(model.predict_proba(Xva, num_iteration=best_it)[:,1])\n",
    "        p_te.append(model.predict_proba(T_all, num_iteration=best_it)[:,1])\n",
    "\n",
    "    oof[va_idx] = np.column_stack([_ranknormalize(p) for p in p_va]).mean(axis=1)\n",
    "    test_list.append(np.column_stack([_ranknormalize(p) for p in p_te]).mean(axis=1))\n",
    "\n",
    "# 7) 年度×Revolver の Isotonic + しきい値（segment→year 縮約）\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "MIN_N_SEG = 120\n",
    "LAMBDA    = 150\n",
    "\n",
    "iso_year, th_year = {}, {}\n",
    "for yr in years:\n",
    "    idx = np.where(year_tr==yr)[0]\n",
    "    yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "    if len(np.unique(yy)) < 2 or len(idx) < 80:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(oof, y_all.values)\n",
    "        pcal = iso.transform(pp)\n",
    "        th,_ = _best_f1_threshold(yy, pcal)\n",
    "    else:\n",
    "        iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        pcal = iso.transform(pp)\n",
    "        th,_ = _best_f1_threshold(yy, pcal)\n",
    "    iso_year[yr], th_year[yr] = iso, th\n",
    "\n",
    "iso_seg, th_seg_final = {}, {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "for yr in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all.iloc[idx].values, oof[idx]\n",
    "        if len(idx) >= MIN_N_SEG and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            pcal = iso.transform(pp)\n",
    "            th_seg,_ = _best_f1_threshold(yy, pcal)\n",
    "        else:\n",
    "            iso = iso_year[yr]; pcal = iso.transform(pp); th_seg = th_year[yr]\n",
    "        n = len(idx); alpha = n/(n+LAMBDA)\n",
    "        th_final = float(alpha*th_seg + (1-alpha)*th_year[yr])\n",
    "        iso_seg[(yr,r)] = iso; th_seg_final[(yr,r)] = th_final\n",
    "        oof_cal[idx] = pcal\n",
    "\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for yr in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        th = th_seg_final[(yr,r)]\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "print(f\"🏁 OOF F1 (v7m+, YearProg 追加) = {f1_oof:.6f}\")\n",
    "\n",
    "# 8) 提出（自動 vN 採番）\n",
    "def _next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)$', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "ver = _next_ver(OUT_DIR)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for i, yr in enumerate(years):\n",
    "    proba_test_y = test_list[i]\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==yr)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(yr,r)]; th = th_seg_final[(yr,r)]\n",
    "        cal = iso.transform(proba_test_y[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "with open(out_log, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"OOF F1 (v7m+): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\n\")\n",
    "    f.write(f\"Seeds: {seeds}\\n\")\n",
    "    f.write(f\"Best rounds(each): {best_rounds_each}\\n\")\n",
    "    f.write(f\"Params: {base_params}\\n\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "92afda4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏁 OOF F1 (CatBoost v8b) = 0.636321\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v43.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v43.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8b: CatBoost LOYO + 年×Revolverで「1回だけ」Isotonic（OOF/TEST一貫） ===\n",
    "import re, sys, subprocess, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# CatBoost 用意\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"catboost\", \"-q\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# 依存チェック\n",
    "need = ['train_df','test_df','DATA_DIR']\n",
    "assert all(k in globals() for k in need), f\"不足: {need}\"\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).values\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "\n",
    "# 交差キー（使わないが将来拡張で残す）\n",
    "def _mk_key(s1, s2):\n",
    "    return s1.astype('Int64').astype('string').fillna('UNK') + '_' + s2.astype('string').fillna('UNK')\n",
    "train['YearSector'] = _mk_key(train['ApprovalFiscalYear'], train['NaicsSector'])\n",
    "test ['YearSector'] = _mk_key(test ['ApprovalFiscalYear'], test ['NaicsSector'])\n",
    "\n",
    "# 特徴（CatBoostは生カテゴリOK）\n",
    "base_feats = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "    'CongressionalDistrict','RevolverStatus'\n",
    "]\n",
    "cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "             'BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "for c in cat_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_all = train[base_feats + cat_cols].copy()\n",
    "T_all = test [base_feats + cat_cols].copy()\n",
    "cat_idx = [X_all.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# LOYO × seeds：予測はrank平均\n",
    "def _ranknormalize(a: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(a); r = np.empty_like(order, dtype=float); r[order] = np.arange(len(a), dtype=float)\n",
    "    return r / max(len(a)-1, 1)\n",
    "\n",
    "seeds = [0,1,2,3,4]\n",
    "oof = np.zeros(len(X_all), dtype=float)\n",
    "test_year_proba = {yr: np.zeros(len(test), dtype=float) for yr in years}  # 各年のrank平均を格納\n",
    "\n",
    "best_rounds_each = []\n",
    "\n",
    "for yr in years:\n",
    "    va_idx = np.where(train['ApprovalFiscalYear'].values == yr)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "\n",
    "    Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "\n",
    "    dtr = Pool(Xtr, ytr, cat_features=cat_idx)\n",
    "    dva = Pool(Xva, yva, cat_features=cat_idx)\n",
    "    dte = Pool(T_all, cat_features=cat_idx)\n",
    "\n",
    "    seed_va, seed_te = [], []\n",
    "    for sd in seeds:\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss',\n",
    "            eval_metric='Logloss',\n",
    "            iterations=6000,              # 少し粘る\n",
    "            learning_rate=0.035,          # やや上げる\n",
    "            depth=8,\n",
    "            l2_leaf_reg=6.0,\n",
    "            random_seed=sd,\n",
    "            early_stopping_rounds=200,\n",
    "            auto_class_weights='Balanced',\n",
    "            verbose=False,\n",
    "            allow_writing_files=False\n",
    "        )\n",
    "        model.fit(dtr, eval_set=dva, use_best_model=True)\n",
    "        best_it = int(model.get_best_iteration() or 6000)\n",
    "        best_rounds_each.append(best_it)\n",
    "\n",
    "        pv = model.predict_proba(dva, ntree_end=best_it)[:,1]\n",
    "        pt = model.predict_proba(dte, ntree_end=best_it)[:,1]\n",
    "        seed_va.append(pv); seed_te.append(pt)\n",
    "\n",
    "    # seed rank平均\n",
    "    oof[va_idx] = np.column_stack([_ranknormalize(p) for p in seed_va]).mean(axis=1)\n",
    "    test_year_proba[yr] = np.column_stack([_ranknormalize(p) for p in seed_te]).mean(axis=1)\n",
    "\n",
    "# === ここがポイント：キャリブは「年×Revolver」に対して1回だけ ===\n",
    "from collections import defaultdict\n",
    "def _best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "MIN_N_SEG = 120\n",
    "\n",
    "iso_seg = {}\n",
    "th_seg  = {}\n",
    "oof_cal = np.zeros_like(oof, dtype=float)\n",
    "\n",
    "for yr in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "        yy, pp = y_all[idx], oof[idx]\n",
    "        # データ不足は年単位でfallback\n",
    "        if len(idx) >= MIN_N_SEG and len(np.unique(yy))==2:\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "        else:\n",
    "            idx_y = np.where(year_tr==yr)[0]\n",
    "            iso = IsotonicRegression(out_of_bounds='clip').fit(oof[idx_y], y_all[idx_y])\n",
    "        pcal = iso.transform(pp)\n",
    "        t,_  = _best_f1_threshold(yy, pcal)\n",
    "        iso_seg[(yr,r)] = iso\n",
    "        th_seg[(yr,r)]  = t\n",
    "        oof_cal[idx]    = pcal  # ★ OOFも“このセグメントのiso出力”で統一\n",
    "\n",
    "# OOF F1\n",
    "y_pred_oof = np.zeros_like(oof, dtype=int)\n",
    "for yr in years:\n",
    "    for r in rev_values:\n",
    "        idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "        if len(idx)==0: continue\n",
    "        y_pred_oof[idx] = (oof_cal[idx] >= th_seg[(yr,r)]).astype(int)\n",
    "f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "# === 提出：TESTにも同じiso/閾値を各セグメントに適用 ===\n",
    "def _next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "ver = _next_ver(OUT_DIR)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "test_pred = np.zeros(len(test), dtype=int)\n",
    "for yr in years:\n",
    "    pt_all = test_year_proba[yr]  # rank平均済み\n",
    "    for r in rev_values:\n",
    "        idx_t = np.where((year_te==yr)&(rev_te==r))[0]\n",
    "        if len(idx_t)==0: continue\n",
    "        iso = iso_seg[(yr,r)]\n",
    "        cal = iso.transform(pt_all[idx_t])\n",
    "        test_pred[idx_t] = (cal >= th_seg[(yr,r)]).astype(int)\n",
    "\n",
    "ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "with open(out_log, 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"OOF F1 (CatBoost v8b): {f1_oof:.6f}\\n\")\n",
    "    f.write(f\"Years: {years}\\nSeeds: {seeds}\\n\")\n",
    "    f.write(f\"Best rounds(each): {best_rounds_each}\\n\")\n",
    "    f.write(\"Params: {'iterations':6000,'lr':0.035,'depth':8,'l2':6.0,'auto_class_weights':'Balanced'}\\n\")\n",
    "\n",
    "print(f\"🏁 OOF F1 (CatBoost v8b) = {f1_oof:.6f}\")\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5dd06395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 21:44:33,357] A new study created in memory with name: no-name-7963ac4c-b439-42a6-b404-7c24f9f0b6ee\n",
      "Best trial: 0. Best value: 0.631675:   3%|▎         | 1/30 [06:15<3:01:43, 375.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 21:50:49,331] Trial 0 finished with value: 0.6316750342309447 and parameters: {'lr': 0.03546243200194785, 'depth': 9, 'l2': 1.7390359933216688, 'rsm': 0.7561066574496378, 'bagtemp': 0.8790872659067779, 'rstr': 1.2378196765089042}. Best is trial 0 with value: 0.6316750342309447.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.635154:   7%|▋         | 2/30 [14:12<3:23:04, 435.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 21:58:45,927] Trial 1 finished with value: 0.6351537402478201 and parameters: {'lr': 0.026187819669849923, 'depth': 7, 'l2': 7.589763319255886, 'rsm': 0.9768827079299627, 'bagtemp': 2.9313700103337164, 'rstr': 1.895964931297738}. Best is trial 1 with value: 0.6351537402478201.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.638889:  10%|█         | 3/30 [21:55<3:21:27, 447.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:06:28,517] Trial 2 finished with value: 0.6388888888888888 and parameters: {'lr': 0.019711292325302072, 'depth': 6, 'l2': 2.1545292213922416, 'rsm': 0.7439292900938907, 'bagtemp': 0.012688933868677643, 'rstr': 1.0336990745153485}. Best is trial 2 with value: 0.6388888888888888.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.638889:  13%|█▎        | 4/30 [29:34<3:15:57, 452.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:14:07,641] Trial 3 finished with value: 0.6353924756154203 and parameters: {'lr': 0.022563560535143596, 'depth': 7, 'l2': 1.2824025478250336, 'rsm': 0.7618439820657581, 'bagtemp': 0.7850397492737496, 'rstr': 1.5448080603836851}. Best is trial 2 with value: 0.6388888888888888.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  17%|█▋        | 5/30 [34:48<2:47:37, 402.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:19:21,452] Trial 4 finished with value: 0.6416747809152873 and parameters: {'lr': 0.051007947925260665, 'depth': 7, 'l2': 14.163000630741431, 'rsm': 0.8577289349636446, 'bagtemp': 1.724515795208319, 'rstr': 1.6293191868665269}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  20%|██        | 6/30 [41:07<2:37:48, 394.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:25:40,939] Trial 5 finished with value: 0.6319018404907976 and parameters: {'lr': 0.036728431520819914, 'depth': 9, 'l2': 4.547124591131281, 'rsm': 0.9962520753500151, 'bagtemp': 2.7562588136736026, 'rstr': 0.7124787464678286}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  23%|██▎       | 7/30 [46:07<2:19:22, 363.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:30:40,844] Trial 6 finished with value: 0.6389140271493212 and parameters: {'lr': 0.024367153686127525, 'depth': 8, 'l2': 1.8914484428438523, 'rsm': 0.7877954583380582, 'bagtemp': 0.5402323082132315, 'rstr': 0.05153510329246558}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  27%|██▋       | 8/30 [52:52<2:18:06, 376.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:37:25,437] Trial 7 finished with value: 0.6393744250229991 and parameters: {'lr': 0.04707430804390253, 'depth': 9, 'l2': 9.228686891281905, 'rsm': 0.8137680860482582, 'bagtemp': 2.2984015864239513, 'rstr': 1.2390753135923847}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  30%|███       | 9/30 [1:00:06<2:18:05, 394.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:44:39,404] Trial 8 finished with value: 0.6340562990309183 and parameters: {'lr': 0.032822268261503154, 'depth': 9, 'l2': 9.725769921380996, 'rsm': 0.8220700592353696, 'bagtemp': 2.450776754675977, 'rstr': 0.46691769694838303}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  33%|███▎      | 10/30 [1:03:35<1:52:26, 337.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:48:08,470] Trial 9 finished with value: 0.6415449835138954 and parameters: {'lr': 0.05093168873341059, 'depth': 6, 'l2': 2.2079099135033595, 'rsm': 0.7021888199797749, 'bagtemp': 0.09845854481771021, 'rstr': 0.2229963235711967}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.641675:  37%|███▋      | 11/30 [1:10:19<1:53:21, 357.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:54:53,320] Trial 10 finished with value: 0.6370967741935484 and parameters: {'lr': 0.0597029196706555, 'depth': 10, 'l2': 19.843034721053076, 'rsm': 0.8989586951874221, 'bagtemp': 1.7877118718199125, 'rstr': 1.9556641216581037}. Best is trial 4 with value: 0.6416747809152873.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  40%|████      | 12/30 [1:13:33<1:32:24, 308.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 22:58:07,182] Trial 11 finished with value: 0.64349376114082 and parameters: {'lr': 0.05790749431701979, 'depth': 6, 'l2': 3.52740512411701, 'rsm': 0.8879908565365107, 'bagtemp': 1.589542180654048, 'rstr': 0.004486482540319026}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  43%|████▎     | 13/30 [1:18:44<1:27:30, 308.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:03:17,974] Trial 12 finished with value: 0.6367310282891783 and parameters: {'lr': 0.046918210561135446, 'depth': 7, 'l2': 5.120657515494862, 'rsm': 0.8909537800642265, 'bagtemp': 1.5136941047101398, 'rstr': 1.610937424809197}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  47%|████▋     | 14/30 [1:23:09<1:18:49, 295.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:07:42,783] Trial 13 finished with value: 0.6393588601959038 and parameters: {'lr': 0.059752025414774146, 'depth': 6, 'l2': 18.7599428477348, 'rsm': 0.8843049275913646, 'bagtemp': 1.906029091895603, 'rstr': 0.719519943036081}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  50%|█████     | 15/30 [1:28:37<1:16:19, 305.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:13:10,673] Trial 14 finished with value: 0.6358645928636779 and parameters: {'lr': 0.04091698420395854, 'depth': 7, 'l2': 3.214806975969993, 'rsm': 0.9427461094620477, 'bagtemp': 1.2414901731614885, 'rstr': 1.5477482090589711}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  53%|█████▎    | 16/30 [1:33:49<1:11:42, 307.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:18:22,604] Trial 15 finished with value: 0.6357372531005971 and parameters: {'lr': 0.04229515120008056, 'depth': 8, 'l2': 3.445011021502378, 'rsm': 0.8576059595316735, 'bagtemp': 2.031296135197364, 'rstr': 0.7476377353075109}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  57%|█████▋    | 17/30 [1:37:45<1:01:55, 285.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:22:18,459] Trial 16 finished with value: 0.6400750821210699 and parameters: {'lr': 0.053392235277745816, 'depth': 6, 'l2': 12.519481730017578, 'rsm': 0.9165180380787119, 'bagtemp': 1.3558815583322352, 'rstr': 0.3840075270969412}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  60%|██████    | 18/30 [1:48:26<1:18:31, 392.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:32:59,788] Trial 17 finished with value: 0.6388251491509866 and parameters: {'lr': 0.015629833208520724, 'depth': 7, 'l2': 5.583064909364676, 'rsm': 0.8501870259594999, 'bagtemp': 1.6941996321038735, 'rstr': 1.3089608797887937}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  63%|██████▎   | 19/30 [1:54:12<1:09:25, 378.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:38:45,864] Trial 18 finished with value: 0.6356164383561644 and parameters: {'lr': 0.029149968319007468, 'depth': 6, 'l2': 3.005981153880731, 'rsm': 0.942360702863762, 'bagtemp': 1.1049200129740449, 'rstr': 0.9552236515619524}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  67%|██████▋   | 20/30 [2:00:30<1:03:04, 378.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:45:03,844] Trial 19 finished with value: 0.6406179009541118 and parameters: {'lr': 0.040700032944615466, 'depth': 8, 'l2': 6.635258899185165, 'rsm': 0.8277827656979662, 'bagtemp': 2.1361108972839586, 'rstr': 1.7245491831528135}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.643494:  70%|███████   | 21/30 [2:04:13<49:47, 331.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:48:47,293] Trial 20 finished with value: 0.6328655500226347 and parameters: {'lr': 0.05341879651173163, 'depth': 7, 'l2': 1.037636350017292, 'rsm': 0.8715345116454339, 'bagtemp': 2.5658477108489226, 'rstr': 0.4869844590529182}. Best is trial 11 with value: 0.64349376114082.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  73%|███████▎  | 22/30 [2:07:31<38:52, 291.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:52:04,597] Trial 21 finished with value: 0.645884072089625 and parameters: {'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436, 'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  77%|███████▋  | 23/30 [2:10:57<31:01, 265.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:55:30,768] Trial 22 finished with value: 0.63996399639964 and parameters: {'lr': 0.046361442109795876, 'depth': 6, 'l2': 3.59190764756783, 'rsm': 0.723018201696945, 'bagtemp': 0.42141979816364783, 'rstr': 0.038943100000350256}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  80%|████████  | 24/30 [2:14:16<24:35, 245.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-09 23:58:49,908] Trial 23 finished with value: 0.6388271879164815 and parameters: {'lr': 0.05964711110159111, 'depth': 6, 'l2': 2.626283289438796, 'rsm': 0.9201423745678456, 'bagtemp': 1.5489636232239141, 'rstr': 0.24483069439095018}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  83%|████████▎ | 25/30 [2:18:01<19:57, 239.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:02:34,538] Trial 24 finished with value: 0.6342326404245909 and parameters: {'lr': 0.05065513082652446, 'depth': 7, 'l2': 4.183673170304917, 'rsm': 0.7882521804895111, 'bagtemp': 1.018303180180376, 'rstr': 0.2114760136302776}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  87%|████████▋ | 26/30 [2:22:17<16:17, 244.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:06:50,376] Trial 25 finished with value: 0.6383173296753544 and parameters: {'lr': 0.04325446601657197, 'depth': 6, 'l2': 14.306233094195075, 'rsm': 0.7948261105436962, 'bagtemp': 1.3361858774442725, 'rstr': 0.028868204515258473}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  90%|█████████ | 27/30 [2:27:20<13:06, 262.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:11:54,340] Trial 26 finished with value: 0.6384180790960452 and parameters: {'lr': 0.037479909955504605, 'depth': 7, 'l2': 1.4270324084978712, 'rsm': 0.8314783054493662, 'bagtemp': 0.6475694611721741, 'rstr': 1.0219090156860193}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  93%|█████████▎| 28/30 [2:31:22<08:32, 256.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:15:56,237] Trial 27 finished with value: 0.6396608572774376 and parameters: {'lr': 0.05415928945958507, 'depth': 6, 'l2': 2.5639672726464964, 'rsm': 0.8644005566975702, 'bagtemp': 1.713747085950674, 'rstr': 1.3839409271983534}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884:  97%|█████████▋| 29/30 [2:36:24<04:29, 269.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:20:57,723] Trial 28 finished with value: 0.6369831894593366 and parameters: {'lr': 0.04688595878164714, 'depth': 8, 'l2': 4.126406233065076, 'rsm': 0.9152345182256478, 'bagtemp': 2.189699171260683, 'rstr': 0.5551437662087134}. Best is trial 21 with value: 0.645884072089625.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 21. Best value: 0.645884: 100%|██████████| 30/30 [2:43:04<00:00, 326.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-10 00:27:37,410] Trial 29 finished with value: 0.6305818673883626 and parameters: {'lr': 0.03292506531953763, 'depth': 10, 'l2': 7.204444953502619, 'rsm': 0.7622225978663146, 'bagtemp': 0.8822468541564856, 'rstr': 0.2908547363189169}. Best is trial 21 with value: 0.645884072089625.\n",
      "OPTUNA BEST OOF F1: 0.645884\n",
      "Best params: {'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436, 'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326}\n",
      "🏁 OOF F1 (CatBoost v8c) = 0.644793 | rounds(each)≈163\n",
      "✅ 提出CSVを保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v44.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v44.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8c: CatBoost + Optuna(LOYO, 年×Revolverキャリブ一回) + ゲーティング付き提出 ===\n",
    "import sys, subprocess, re, warnings, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- deps ---\n",
    "try:\n",
    "    import optuna\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"optuna\"])\n",
    "    import optuna\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# --- constants / paths ---\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_TRACK = OUT_DIR / \"best_oof_catboost.txt\"  # OOFベストの永続化\n",
    "\n",
    "# --- data (前処理セルの成果を利用) ---\n",
    "assert 'train_df' in globals() and 'test_df' in globals()\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).to_numpy()\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "\n",
    "base_feats = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'ratio1','interact1','ratio2','interact2',\n",
    "    'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "    'CongressionalDistrict','RevolverStatus'\n",
    "]\n",
    "cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector',\n",
    "             'BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "# CatBoostは生カテゴリOK（stringに統一）\n",
    "for c in cat_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_all = train[base_feats + cat_cols].copy()\n",
    "T_all = test [base_feats + cat_cols].copy()\n",
    "cat_idx = [X_all.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# LOYO folds\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == yr)[0] for yr in years]\n",
    "\n",
    "# util\n",
    "def ranknorm(a: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(a)\n",
    "    r = np.empty_like(order, dtype=float)\n",
    "    r[order] = np.arange(len(a), dtype=float)\n",
    "    return r / max(len(a)-1, 1)\n",
    "\n",
    "def seg_calibrate_predict(oof_prob, test_prob_by_year, years, train, test, y_all,\n",
    "                          min_seg=120):\n",
    "    \"\"\"年×Revolverごとに Isotonic を一度だけ適用し、OOFとTESTに同一写像を当てる。\"\"\"\n",
    "    year_tr = train['ApprovalFiscalYear'].to_numpy()\n",
    "    rev_tr  = train['RevolverStatus'].to_numpy()\n",
    "    year_te = test ['ApprovalFiscalYear'].to_numpy()\n",
    "    rev_te  = test ['RevolverStatus'].to_numpy()\n",
    "    rev_values = sorted(pd.Series(rev_tr).dropna().unique().tolist())\n",
    "\n",
    "    oof_cal = np.zeros_like(oof_prob, dtype=float)\n",
    "    iso_seg, th_seg = {}, {}\n",
    "    for yr in years:\n",
    "        idx_y = np.where(year_tr==yr)[0]\n",
    "        for r in rev_values:\n",
    "            idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "            yy, pp = y_all[idx], oof_prob[idx]\n",
    "            if len(idx) >= min_seg and len(np.unique(yy))==2:\n",
    "                iso = IsotonicRegression(out_of_bounds='clip').fit(pp, yy)\n",
    "            else:\n",
    "                iso = IsotonicRegression(out_of_bounds='clip').fit(oof_prob[idx_y], y_all[idx_y])\n",
    "            pcal = iso.transform(pp)\n",
    "            # per-segment閾値\n",
    "            ths = np.linspace(0.05, 0.95, 181)\n",
    "            f1s = [f1_score(yy, (pcal>=t).astype(int)) for t in ths]\n",
    "            t = float(ths[int(np.argmax(f1s))])\n",
    "            iso_seg[(yr,r)] = iso\n",
    "            th_seg[(yr,r)]  = t\n",
    "            oof_cal[idx]    = pcal\n",
    "\n",
    "    # OOF最終F1\n",
    "    y_pred_oof = np.zeros_like(oof_cal, dtype=int)\n",
    "    for yr in years:\n",
    "        for r in rev_values:\n",
    "            idx = np.where((year_tr==yr)&(rev_tr==r))[0]\n",
    "            if len(idx)==0: continue\n",
    "            y_pred_oof[idx] = (oof_cal[idx] >= th_seg[(yr,r)]).astype(int)\n",
    "    f1_oof = f1_score(y_all, y_pred_oof)\n",
    "\n",
    "    # TEST\n",
    "    test_pred = np.zeros(len(test), dtype=int)\n",
    "    for yr in years:\n",
    "        pt = test_prob_by_year[yr]  # rank平均済みの確率（年ごと）\n",
    "        for r in rev_values:\n",
    "            idx_t = np.where((year_te==yr)&(rev_te==r))[0]\n",
    "            if len(idx_t)==0: continue\n",
    "            iso = iso_seg[(yr,r)]\n",
    "            cal = iso.transform(pt[idx_t])\n",
    "            test_pred[idx_t] = (cal >= th_seg[(yr,r)]).astype(int)\n",
    "    return f1_oof, test_pred\n",
    "\n",
    "def train_catboost_oof(params, seeds=(0,1,2)):\n",
    "    \"\"\"LOYO×複数seedのrank平均でOOFとTEST(year別)確率を返す。\"\"\"\n",
    "    oof = np.zeros(len(X_all), dtype=float)\n",
    "    test_year_prob = {yr: np.zeros(len(test), dtype=float) for yr in years}\n",
    "    best_rounds = []\n",
    "\n",
    "    for fold_year, va_idx in zip(years, folds):\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X_all)), va_idx)\n",
    "        Xtr, Xva = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "        dtr, dva, dte = Pool(Xtr, ytr, cat_features=cat_idx), Pool(Xva, yva, cat_features=cat_idx), Pool(T_all, cat_features=cat_idx)\n",
    "\n",
    "        seed_va, seed_te = [], []\n",
    "        for sd in seeds:\n",
    "            model = CatBoostClassifier(\n",
    "                loss_function='Logloss',\n",
    "                eval_metric='Logloss',\n",
    "                iterations=6000,\n",
    "                learning_rate=params['lr'],\n",
    "                depth=params['depth'],\n",
    "                l2_leaf_reg=params['l2'],\n",
    "                rsm=params['rsm'],\n",
    "                bagging_temperature=params['bagtemp'],\n",
    "                random_strength=params['rstr'],\n",
    "                random_seed=sd,\n",
    "                early_stopping_rounds=200,\n",
    "                auto_class_weights='Balanced',\n",
    "                verbose=False,\n",
    "                allow_writing_files=False\n",
    "            )\n",
    "            model.fit(dtr, eval_set=dva, use_best_model=True)\n",
    "            best_it = int(model.get_best_iteration() or 6000)\n",
    "            best_rounds.append(best_it)\n",
    "            pv = model.predict_proba(dva, ntree_end=best_it)[:,1]\n",
    "            pt = model.predict_proba(dte, ntree_end=best_it)[:,1]\n",
    "            seed_va.append(pv); seed_te.append(pt)\n",
    "\n",
    "        oof[va_idx] = np.column_stack([ranknorm(p) for p in seed_va]).mean(axis=1)\n",
    "        test_year_prob[fold_year] = np.column_stack([ranknorm(p) for p in seed_te]).mean(axis=1)\n",
    "\n",
    "    return oof, test_year_prob, best_rounds\n",
    "\n",
    "# --- Optuna objective ---\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'lr': trial.suggest_float('lr', 0.015, 0.06, log=True),\n",
    "        'depth': trial.suggest_int('depth', 6, 10),\n",
    "        'l2': trial.suggest_float('l2', 1.0, 20.0, log=True),\n",
    "        'rsm': trial.suggest_float('rsm', 0.7, 1.0),\n",
    "        'bagtemp': trial.suggest_float('bagtemp', 0.0, 3.0),\n",
    "        'rstr': trial.suggest_float('rstr', 0.0, 2.0),\n",
    "    }\n",
    "    oof, test_by_year, _ = train_catboost_oof(params, seeds=(0,1,2))  # 軽め\n",
    "    f1_oof, _ = seg_calibrate_predict(oof, test_by_year, years, train, test, y_all, min_seg=120)\n",
    "    return f1_oof\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"OPTUNA BEST OOF F1:\", f\"{study.best_value:.6f}\")\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# --- bestで再学習（seeds増やす）→ 最終OOF/TEST ---\n",
    "oof, test_by_year, best_rounds = train_catboost_oof(best_params, seeds=(0,1,2,3,4))\n",
    "f1_oof, test_pred = seg_calibrate_predict(oof, test_by_year, years, train, test, y_all, min_seg=120)\n",
    "print(f\"🏁 OOF F1 (CatBoost v8c) = {f1_oof:.6f} | rounds(each)≈{int(np.mean(best_rounds))}\")\n",
    "\n",
    "# --- gating & versioned save ---\n",
    "gate = 0.640\n",
    "prev_best = 0.0\n",
    "if BEST_TRACK.exists():\n",
    "    try:\n",
    "        prev_best = float(BEST_TRACK.read_text().strip())\n",
    "    except Exception:\n",
    "        prev_best = 0.0\n",
    "\n",
    "def next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "if (f1_oof >= gate) and (f1_oof >= prev_best + 0.002):\n",
    "    ver = next_ver(OUT_DIR)\n",
    "    out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "    out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    with open(out_log, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"OOF F1 (CatBoost v8c): {f1_oof:.6f}\\n\")\n",
    "        f.write(f\"Best params: {best_params}\\n\")\n",
    "        f.write(f\"Best rounds(each): {best_rounds}\\n\")\n",
    "    BEST_TRACK.write_text(f\"{f1_oof:.6f}\")\n",
    "    print(f\"✅ 提出CSVを保存: {out_csv}\")\n",
    "    print(f\"📝 ログ保存: {out_log}\")\n",
    "else:\n",
    "    need = max(gate, prev_best + 0.002)\n",
    "    print(f\"⛔ 提出しない: OOF={f1_oof:.6f} < gate {need:.3f} (prev_best={prev_best:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e2c3083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[813]\tvalid_0's binary_logloss: 0.229138\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid_0's binary_logloss: 0.290299\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[690]\tvalid_0's binary_logloss: 0.346256\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[471]\tvalid_0's binary_logloss: 0.292981\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1086]\tvalid_0's binary_logloss: 0.168087\n",
      "🔎 OOF F1 (blend)= 0.627363 | w_catboost=0.85 | th=0.755\n",
      "    corr(cb,lgb)=0.859 | cb_rounds≈163, lgb_rounds≈682\n",
      "⛔ 提出しない: OOF=0.627363 < gate 0.640 (prev_best=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8d: Robust blend (CatBoost + LightGBM) + Global Platt + OOF最適重み + 版管理つき提出 ===\n",
    "import sys, subprocess, warnings, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- paths / versioning ----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_TRACK = OUT_DIR / \"best_oof_blend.txt\"\n",
    "\n",
    "def next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "# ---- data / features (既存の前処理セルを利用) ----\n",
    "assert 'train_df' in globals() and 'test_df' in globals()\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).to_numpy()\n",
    "\n",
    "# 既存の定義が無ければフォールバック\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "# CatBoost 用にカテゴリを string 化（モデル内部で処理）\n",
    "for c in cat_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_cb = train[base_feats + cat_cols].copy()\n",
    "T_cb = test [base_feats + cat_cols].copy()\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "# LOYO folds\n",
    "years = sorted(train['ApprovalFiscalYear'].dropna().unique().tolist())\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == yr)[0] for yr in years]\n",
    "\n",
    "# ---- utils ----\n",
    "def ranknorm(a: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(a)\n",
    "    r = np.empty_like(order, dtype=float); r[order] = np.arange(len(a), dtype=float)\n",
    "    return r / max(len(a)-1, 1)\n",
    "\n",
    "def best_f1_threshold(y_true, proba):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    f1s = [f1_score(y_true, (proba>=t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "# ---- CatBoost: best_params が無ければ v8c の結果を既定値に ----\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "cb_best = {\n",
    "    'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436,\n",
    "    'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326\n",
    "}\n",
    "if 'best_params' in globals():  # v8cのOptuna結果があれば優先\n",
    "    for k in ['lr','depth','l2','rsm','bagtemp','rstr']:\n",
    "        if k in best_params: cb_best[k] = best_params[k]\n",
    "\n",
    "def catboost_oof_test(params, seeds=(0,1,2,3,4)):\n",
    "    oof = np.zeros(len(X_cb), dtype=float)\n",
    "    test_prob = np.zeros(len(T_cb), dtype=float)\n",
    "    best_rounds = []\n",
    "    for va_idx in folds:\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X_cb)), va_idx)\n",
    "        Xtr, Xva = X_cb.iloc[tr_idx], X_cb.iloc[va_idx]\n",
    "        ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "        dtr, dva, dte = Pool(Xtr, ytr, cat_features=cat_idx), Pool(Xva, yva, cat_features=cat_idx), Pool(T_cb, cat_features=cat_idx)\n",
    "        seed_probs_va, seed_probs_te = [], []\n",
    "        for sd in seeds:\n",
    "            model = CatBoostClassifier(\n",
    "                loss_function='Logloss', eval_metric='Logloss',\n",
    "                iterations=6000, learning_rate=params['lr'], depth=params['depth'],\n",
    "                l2_leaf_reg=params['l2'], rsm=params['rsm'],\n",
    "                bagging_temperature=params['bagtemp'], random_strength=params['rstr'],\n",
    "                random_seed=sd, early_stopping_rounds=200, auto_class_weights='Balanced',\n",
    "                verbose=False, allow_writing_files=False\n",
    "            )\n",
    "            model.fit(dtr, eval_set=dva, use_best_model=True)\n",
    "            best_it = int(model.get_best_iteration() or 6000)\n",
    "            best_rounds.append(best_it)\n",
    "            seed_probs_va.append(model.predict_proba(dva, ntree_end=best_it)[:,1])\n",
    "            seed_probs_te.append(model.predict_proba(dte, ntree_end=best_it)[:,1])\n",
    "        oof[va_idx] = np.column_stack([ranknorm(p) for p in seed_probs_va]).mean(axis=1)\n",
    "    # testは fold で学んだモデルを平均できないので seed rank 平均だけで近似\n",
    "    # （CatBoostはfoldごと再学習だが、rank平均の安定性が高い）\n",
    "    # 実用上は full 再学習を足してもOK。ここでは簡潔に seed で平均。\n",
    "    dtr_full = Pool(X_cb, y_all, cat_features=cat_idx); dte_full = Pool(T_cb, cat_features=cat_idx)\n",
    "    seed_probs_te = []\n",
    "    for sd in (0,1,2,3,4):\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss', eval_metric='Logloss',\n",
    "            iterations=int(np.mean(best_rounds)), learning_rate=params['lr'],\n",
    "            depth=params['depth'], l2_leaf_reg=params['l2'], rsm=params['rsm'],\n",
    "            bagging_temperature=params['bagtemp'], random_strength=params['rstr'],\n",
    "            random_seed=sd, auto_class_weights='Balanced',\n",
    "            verbose=False, allow_writing_files=False\n",
    "        )\n",
    "        model.fit(dtr_full)\n",
    "        seed_probs_te.append(model.predict_proba(dte_full)[:,1])\n",
    "    test_prob = np.column_stack([ranknorm(p) for p in seed_probs_te]).mean(axis=1)\n",
    "    return oof, test_prob, best_rounds\n",
    "\n",
    "# ---- LightGBM: カテゴリを整数コード化して LOYO ----\n",
    "def fit_code_maps(df, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(df[c].astype('string')).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_code_maps(df, maps):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c+\"_code\"] = df[c].astype('string').map(mp).fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "code_maps = fit_code_maps(train, cat_cols)\n",
    "train_lgb = apply_code_maps(train, code_maps)\n",
    "test_lgb  = apply_code_maps(test,  code_maps)\n",
    "\n",
    "X_lgb = pd.concat([\n",
    "    train_lgb[base_feats],\n",
    "    train_lgb[[c+\"_code\" for c in cat_cols]]\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "T_lgb = pd.concat([\n",
    "    test_lgb[base_feats],\n",
    "    test_lgb[[c+\"_code\" for c in cat_cols]]\n",
    "], axis=1).replace([np.inf,-np.inf], np.nan).fillna(0)\n",
    "\n",
    "lgb_params = dict(\n",
    "    learning_rate=0.03, num_leaves=96, max_depth=-1, min_child_samples=40,\n",
    "    subsample=0.8, colsample_bytree=0.9, reg_alpha=1.0, reg_lambda=3.0,\n",
    "    class_weight='balanced', verbosity=-1, random_state=0\n",
    ")\n",
    "\n",
    "def lightgbm_oof_test(params):\n",
    "    oof = np.zeros(len(X_lgb), dtype=float)\n",
    "    test_probs = []\n",
    "    best_rounds = []\n",
    "    for va_idx in folds:\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X_lgb)), va_idx)\n",
    "        Xtr, Xva = X_lgb.iloc[tr_idx], X_lgb.iloc[va_idx]\n",
    "        ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "        model = LGBMClassifier(**params, n_estimators=5000)\n",
    "        model.fit(\n",
    "            Xtr, ytr, eval_set=[(Xva, yva)], eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=200), log_evaluation(period=0)]\n",
    "        )\n",
    "        best_it = int(getattr(model, \"best_iteration_\", 5000))\n",
    "        oof[va_idx] = model.predict_proba(Xva, num_iteration=best_it)[:,1]\n",
    "        test_probs.append(model.predict_proba(T_lgb, num_iteration=best_it)[:,1])\n",
    "        best_rounds.append(best_it)\n",
    "    test_prob = np.column_stack([ranknorm(p) for p in test_probs]).mean(axis=1)\n",
    "    return oof, test_prob, best_rounds\n",
    "\n",
    "# ---- 1) 個別モデルの OOF / TEST ----\n",
    "oof_cb, test_cb, cb_rounds = catboost_oof_test(cb_best, seeds=(0,1,2,3,4))\n",
    "oof_lg, test_lg, lg_rounds = lightgbm_oof_test(lgb_params)\n",
    "\n",
    "# ---- 2) OOF で最適ブレンド重みを探索（rank平均 → Platt → F1最大）----\n",
    "#    （rank平均でスケール差を吸収 → Plattはグローバル 1本のみ）\n",
    "rn_cb = ranknorm(oof_cb); rn_lg = ranknorm(oof_lg)\n",
    "weights = np.linspace(0.15, 0.85, 15)  # CatBoost の比率\n",
    "best = (-1.0, None)  # (F1, w)\n",
    "for w in weights:\n",
    "    blend = w*rn_cb + (1-w)*rn_lg\n",
    "    lr = LogisticRegression(C=1.0, class_weight='balanced', max_iter=200)\n",
    "    lr.fit(blend.reshape(-1,1), y_all)\n",
    "    pcal = lr.predict_proba(blend.reshape(-1,1))[:,1]\n",
    "    th, f1 = best_f1_threshold(y_all, pcal)\n",
    "    if f1 > best[0]:\n",
    "        best = (f1, (w, lr, th))\n",
    "f1_oof, (w_opt, lr_opt, th_opt) = best[0], best[1]\n",
    "\n",
    "# ---- 3) TEST へ同じ写像を適用して最終予測 ----\n",
    "rn_cb_te = ranknorm(test_cb); rn_lg_te = ranknorm(test_lg)\n",
    "blend_te = w_opt*rn_cb_te + (1-w_opt)*rn_lg_te\n",
    "pcal_te  = lr_opt.predict_proba(blend_te.reshape(-1,1))[:,1]\n",
    "y_pred   = (pcal_te >= th_opt).astype(int)\n",
    "\n",
    "# ---- 4) 出力（ゲーティング＋版管理）----\n",
    "prev_best = 0.0\n",
    "if BEST_TRACK.exists():\n",
    "    try: prev_best = float(BEST_TRACK.read_text().strip())\n",
    "    except: prev_best = 0.0\n",
    "gate = 0.640\n",
    "print(f\"🔎 OOF F1 (blend)= {f1_oof:.6f} | w_catboost={w_opt:.2f} | th={th_opt:.3f}\")\n",
    "print(f\"    corr(cb,lgb)=\"\n",
    "      f\"{np.corrcoef(rn_cb, rn_lg)[0,1]:.3f} | \"\n",
    "      f\"cb_rounds≈{int(np.mean(cb_rounds))}, lgb_rounds≈{int(np.mean(lg_rounds))}\")\n",
    "\n",
    "if (f1_oof >= gate) and (f1_oof >= prev_best + 0.002):\n",
    "    ver = next_ver(OUT_DIR)\n",
    "    out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "    out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"OOF F1 (blend): {f1_oof:.6f}\\n\")\n",
    "        f.write(f\"w_catboost: {w_opt:.3f}, threshold: {th_opt:.3f}\\n\")\n",
    "        f.write(f\"corr(cb,lgb): {np.corrcoef(rn_cb, rn_lg)[0,1]:.6f}\\n\")\n",
    "        f.write(f\"cb_rounds: {cb_rounds}\\n\")\n",
    "        f.write(f\"lgb_rounds: {lg_rounds}\\n\")\n",
    "        f.write(f\"cb_params: {cb_best}\\n\")\n",
    "        f.write(f\"lgb_params: {lgb_params}\\n\")\n",
    "    BEST_TRACK.write_text(f\"{f1_oof:.6f}\")\n",
    "    print(f\"✅ 提出CSVを保存: {out_csv}\")\n",
    "    print(f\"📝 ログ保存: {out_log}\")\n",
    "else:\n",
    "    need = max(gate, prev_best + 0.002)\n",
    "    print(f\"⛔ 提出しない: OOF={f1_oof:.6f} < gate {need:.3f} (prev_best={prev_best:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "390749f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(by year) = 0.563902 | th=0.500 | it_full≈108\n",
      "   CatBoost params = {'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436, 'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326}\n",
      "⛔ 提出しない: OOF=0.563902 < gate 0.640 (prev_best=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8e: CatBoost 強化（交差カテゴリ + 年内Z + 年バランスPlatt + macro-F1閾値） ===\n",
    "import sys, subprocess, warnings, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 出力まわり ---\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_TRACK = OUT_DIR / \"best_oof_blend.txt\"\n",
    "\n",
    "def next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "# --- データ（前処理セルの成果を使用） ---\n",
    "assert 'train_df' in globals() and 'test_df' in globals() and 'DATA_DIR' in globals()\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).to_numpy()\n",
    "years = train['ApprovalFiscalYear'].to_numpy()\n",
    "\n",
    "# 既定の基本特徴\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "# --- 交差カテゴリ（生カテゴリのままCatBoostへ） ---\n",
    "def make_year_crosses(df):\n",
    "    out = df.copy()\n",
    "    out['YearSector']   = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['NaicsSector'].astype('string').fillna('UNK')\n",
    "    out['YearProgram']  = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['Subprogram'].astype('string').fillna('UNK')\n",
    "    out['YearAge']      = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['BusinessAge'].astype('string').fillna('UNK')\n",
    "    out['YearRevolver'] = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['RevolverStatus'].astype('Int64').astype('string').fillna('UNK')\n",
    "    return out\n",
    "\n",
    "train = make_year_crosses(train)\n",
    "test  = make_year_crosses(test)\n",
    "cross_cols = ['YearSector','YearProgram','YearAge','YearRevolver']\n",
    "\n",
    "# --- 年内 Z 特徴 ---\n",
    "def add_year_z(df, cols, year_col='ApprovalFiscalYear'):\n",
    "    out = df.copy()\n",
    "    g = out.groupby(year_col)\n",
    "    for c in cols:\n",
    "        mu = g[c].transform('mean')\n",
    "        sd = g[c].transform('std').replace(0, 1.0)\n",
    "        out[c + '_zY'] = ((out[c] - mu) / sd).astype(float)\n",
    "    return out\n",
    "\n",
    "z_num_cols = ['GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "              'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2']\n",
    "train = add_year_z(train, z_num_cols)\n",
    "test  = add_year_z(test,  z_num_cols)\n",
    "z_cols = [c+'_zY' for c in z_num_cols]\n",
    "\n",
    "# --- CatBoost 用の準備 ---\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# CatBoostは生カテゴリ列を string で渡す\n",
    "for c in cat_cols + cross_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_cols = base_feats + z_cols + cat_cols + cross_cols\n",
    "X_cb = train[X_cols].copy()\n",
    "T_cb = test [X_cols].copy()\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in (cat_cols + cross_cols)]\n",
    "\n",
    "# --- LOYO folds ---\n",
    "uniq_years = sorted(pd.unique(train['ApprovalFiscalYear'].dropna()))\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == yr)[0] for yr in uniq_years]\n",
    "\n",
    "# --- 実用ヘルパ ---\n",
    "def ranknorm(a: np.ndarray) -> np.ndarray:\n",
    "    order = np.argsort(a)\n",
    "    r = np.empty_like(order, dtype=float)\n",
    "    r[order] = np.arange(len(a), dtype=float)\n",
    "    return r / max(len(a)-1, 1)\n",
    "\n",
    "def macro_f1_by_year_threshold(y_true, proba, years):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    best = (-1.0, 0.5)\n",
    "    yrs = np.unique(years)\n",
    "    for t in ths:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1s = []\n",
    "        for y in yrs:\n",
    "            idx = (years == y)\n",
    "            if idx.any():\n",
    "                f1s.append(f1_score(y_true[idx], pred[idx]))\n",
    "        mf1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "        if mf1 > best[0]:\n",
    "            best = (mf1, t)\n",
    "    return best[1], best[0]\n",
    "\n",
    "# --- CatBoostのハイパラ（v8cベース） ---\n",
    "cb_best = {\n",
    "    'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436,\n",
    "    'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326\n",
    "}\n",
    "# best_params があれば上書き（任意）\n",
    "if 'best_params' in globals():\n",
    "    for k in ['lr','depth','l2','rsm','bagtemp','rstr']:\n",
    "        if k in best_params: cb_best[k] = best_params[k]\n",
    "\n",
    "# --- OOF（CatBoost, seeds平均, rank平均） ---\n",
    "oof = np.zeros(len(X_cb), dtype=float)\n",
    "test_rank_seeds = []\n",
    "best_rounds = []\n",
    "\n",
    "for va_idx in folds:\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_cb)), va_idx)\n",
    "    Xtr, Xva = X_cb.iloc[tr_idx], X_cb.iloc[va_idx]\n",
    "    ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "    dtr, dva, dte = Pool(Xtr, ytr, cat_features=cat_idx), Pool(Xva, yva, cat_features=cat_idx), Pool(T_cb, cat_features=cat_idx)\n",
    "\n",
    "    probs_va = []\n",
    "    for sd in [0,1,2,3,4]:\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss', eval_metric='Logloss',\n",
    "            iterations=4000, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "            l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "            bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "            random_seed=sd, early_stopping_rounds=200,\n",
    "            auto_class_weights='Balanced', verbose=False, allow_writing_files=False\n",
    "        )\n",
    "        model.fit(dtr, eval_set=dva, use_best_model=True)\n",
    "        it = int(model.get_best_iteration() or 4000)\n",
    "        best_rounds.append(it)\n",
    "        probs_va.append(model.predict_proba(dva, ntree_end=it)[:,1])\n",
    "    # rank平均で fold 予測を1本に\n",
    "    oof[va_idx] = np.column_stack([ranknorm(p) for p in probs_va]).mean(axis=1)\n",
    "\n",
    "# test は full-fit で seeds アンサンブル（イテ数は OOFの平均に寄せる）\n",
    "it_full = int(np.round(np.mean(best_rounds)))\n",
    "seed_te = []\n",
    "dtr_full = Pool(X_cb, y_all, cat_features=cat_idx)\n",
    "dte_full = Pool(T_cb, cat_features=cat_idx)\n",
    "for sd in [0,1,2,3,4]:\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='Logloss',\n",
    "        iterations=it_full, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "        l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "        bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "        random_seed=sd, auto_class_weights='Balanced',\n",
    "        verbose=False, allow_writing_files=False\n",
    "    )\n",
    "    model.fit(dtr_full)\n",
    "    seed_te.append(model.predict_proba(dte_full)[:,1])\n",
    "test_rank = np.column_stack([ranknorm(p) for p in seed_te]).mean(axis=1)\n",
    "\n",
    "# --- 年バランス Platt（ロジスティック） ---\n",
    "# 各年の重みを等しく（年ごと1.0を各サンプルに割る）\n",
    "w = pd.Series(1.0, index=np.arange(len(train)))\n",
    "for y in np.unique(years):\n",
    "    idx = np.where(years == y)[0]\n",
    "    if len(idx) > 0:\n",
    "        w.iloc[idx] = 1.0 / len(idx)\n",
    "\n",
    "lr = LogisticRegression(C=1.0, class_weight='balanced', max_iter=500)\n",
    "lr.fit(oof.reshape(-1,1), y_all, sample_weight=w.values)\n",
    "oof_cal = lr.predict_proba(oof.reshape(-1,1))[:,1]\n",
    "th_opt, f1_macro = macro_f1_by_year_threshold(y_all, oof_cal, years)\n",
    "\n",
    "test_cal = lr.predict_proba(test_rank.reshape(-1,1))[:,1]\n",
    "y_pred   = (test_cal >= th_opt).astype(int)\n",
    "\n",
    "print(f\"🔎 OOF macro-F1(by year) = {f1_macro:.6f} | th={th_opt:.3f} | it_full≈{it_full}\")\n",
    "print(f\"   CatBoost params = {cb_best}\")\n",
    "\n",
    "# --- ゲート & 出力 ---\n",
    "prev_best = 0.0\n",
    "if BEST_TRACK.exists():\n",
    "    try: prev_best = float(BEST_TRACK.read_text().strip())\n",
    "    except: prev_best = 0.0\n",
    "gate = 0.640\n",
    "if (f1_macro >= gate) and (f1_macro >= prev_best + 0.002):\n",
    "    ver = next_ver(OUT_DIR)\n",
    "    out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "    out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"OOF macro-F1(by year): {f1_macro:.6f}\\n\")\n",
    "        f.write(f\"threshold: {th_opt:.3f}\\n\")\n",
    "        f.write(f\"it_full: {it_full}\\n\")\n",
    "        f.write(f\"params: {cb_best}\\n\")\n",
    "        f.write(f\"features: {X_cols}\\n\")\n",
    "    BEST_TRACK.write_text(f\"{f1_macro:.6f}\")\n",
    "    print(f\"✅ 提出CSVを保存: {out_csv}\")\n",
    "    print(f\"📝 ログ保存: {out_log}\")\n",
    "else:\n",
    "    need = max(gate, prev_best + 0.002)\n",
    "    print(f\"⛔ 提出しない: OOF={f1_macro:.6f} < gate {need:.3f} (prev_best={prev_best:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "986e3dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(by year) = 0.596726 | th=0.125 | it_full≈108\n",
      "   CatBoost params = {'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436, 'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326}\n",
      "⛔ 提出しない: OOF=0.596726 < gate 0.640 (prev_best=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8f: CatBoost（生カテゴリ＋交差カテゴリ＋年内Z）× LOYO × 年バランスPlatt × 年別macro-F1 ===\n",
    "import sys, subprocess, warnings, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 出力まわり ---\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_TRACK = OUT_DIR / \"best_oof_blend.txt\"\n",
    "\n",
    "def next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "# --- データ（前処理セルの成果を使用） ---\n",
    "assert 'train_df' in globals() and 'test_df' in globals() and 'DATA_DIR' in globals()\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).to_numpy()\n",
    "years = train['ApprovalFiscalYear'].to_numpy()\n",
    "\n",
    "# 既定の基本特徴\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "# --- 交差カテゴリ（生カテゴリのままCatBoostへ） ---\n",
    "def make_year_crosses(df):\n",
    "    out = df.copy()\n",
    "    out['YearSector']   = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['NaicsSector'].astype('string').fillna('UNK')\n",
    "    out['YearProgram']  = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['Subprogram'].astype('string').fillna('UNK')\n",
    "    out['YearAge']      = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['BusinessAge'].astype('string').fillna('UNK')\n",
    "    out['YearRevolver'] = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['RevolverStatus'].astype('Int64').astype('string').fillna('UNK')\n",
    "    return out\n",
    "\n",
    "train = make_year_crosses(train)\n",
    "test  = make_year_crosses(test)\n",
    "cross_cols = ['YearSector','YearProgram','YearAge','YearRevolver']\n",
    "\n",
    "# --- 年内 Z 特徴 ---\n",
    "def add_year_z(df, cols, year_col='ApprovalFiscalYear'):\n",
    "    out = df.copy()\n",
    "    g = out.groupby(year_col)\n",
    "    for c in cols:\n",
    "        mu = g[c].transform('mean')\n",
    "        sd = g[c].transform('std').replace(0, 1.0)\n",
    "        out[c + '_zY'] = ((out[c] - mu) / sd).astype(float)\n",
    "    return out\n",
    "\n",
    "z_num_cols = ['GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "              'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2']\n",
    "train = add_year_z(train, z_num_cols)\n",
    "test  = add_year_z(test,  z_num_cols)\n",
    "z_cols = [c+'_zY' for c in z_num_cols]\n",
    "\n",
    "# --- CatBoost 用の準備 ---\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# 生カテゴリを string に\n",
    "for c in cat_cols + cross_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_cols = base_feats + z_cols + cat_cols + cross_cols\n",
    "X_cb = train[X_cols].copy()\n",
    "T_cb = test [X_cols].copy()\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in (cat_cols + cross_cols)]\n",
    "\n",
    "# --- LOYO folds ---\n",
    "uniq_years = sorted(pd.unique(train['ApprovalFiscalYear'].dropna()))\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == yr)[0] for yr in uniq_years]\n",
    "\n",
    "# --- ヘルパ ---\n",
    "def macro_f1_by_year_threshold(y_true, proba, years):\n",
    "    ths = np.linspace(0.05, 0.95, 181)\n",
    "    best_m, best_t = -1.0, 0.5\n",
    "    yrs = np.unique(years)\n",
    "    for t in ths:\n",
    "        pred = (proba >= t).astype(int)\n",
    "        f1s = []\n",
    "        for y in yrs:\n",
    "            idx = (years == y)\n",
    "            if idx.any():\n",
    "                f1s.append(f1_score(y_true[idx], pred[idx]))\n",
    "        m = float(np.mean(f1s)) if f1s else 0.0\n",
    "        if m > best_m:\n",
    "            best_m, best_t = m, t\n",
    "    return best_t, best_m\n",
    "\n",
    "# --- CatBoostハイパラ（Optuna最良を基準） ---\n",
    "cb_best = {\n",
    "    'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436,\n",
    "    'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326\n",
    "}\n",
    "if 'best_params' in globals():\n",
    "    for k in ['lr','depth','l2','rsm','bagtemp','rstr']:\n",
    "        if k in best_params: cb_best[k] = best_params[k]\n",
    "\n",
    "# --- OOF（生確率の平均。rank正規化は使わない） ---\n",
    "oof = np.zeros(len(X_cb), dtype=float)\n",
    "best_rounds = []\n",
    "\n",
    "for va_idx in folds:\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_cb)), va_idx)\n",
    "    Xtr, Xva = X_cb.iloc[tr_idx], X_cb.iloc[va_idx]\n",
    "    ytr, yva = y_all[tr_idx], y_all[va_idx]\n",
    "    dtr, dva, dte = Pool(Xtr, ytr, cat_features=cat_idx), Pool(Xva, yva, cat_features=cat_idx), Pool(T_cb, cat_features=cat_idx)\n",
    "\n",
    "    probs_va = []\n",
    "    for sd in [0,1,2,3,4]:\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function='Logloss', eval_metric='Logloss',\n",
    "            iterations=4000, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "            l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "            bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "            random_seed=sd, early_stopping_rounds=200,\n",
    "            auto_class_weights='Balanced', verbose=False, allow_writing_files=False\n",
    "        )\n",
    "        model.fit(dtr, eval_set=dva, use_best_model=True)\n",
    "        it = int(model.get_best_iteration() or 4000)\n",
    "        best_rounds.append(it)\n",
    "        probs_va.append(model.predict_proba(dva, ntree_end=it)[:,1])\n",
    "    oof[va_idx] = np.column_stack(probs_va).mean(axis=1)\n",
    "\n",
    "# test は full-fit（it は OOF の平均 best-iter）で seed アンサンブル\n",
    "it_full = int(np.round(np.mean(best_rounds)))\n",
    "seed_te = []\n",
    "dtr_full = Pool(X_cb, y_all, cat_features=cat_idx)\n",
    "dte_full = Pool(T_cb, cat_features=cat_idx)\n",
    "for sd in [0,1,2,3,4]:\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='Logloss',\n",
    "        iterations=it_full, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "        l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "        bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "        random_seed=sd, auto_class_weights='Balanced',\n",
    "        verbose=False, allow_writing_files=False\n",
    "    )\n",
    "    model.fit(dtr_full)\n",
    "    seed_te.append(model.predict_proba(dte_full)[:,1])\n",
    "test_mean = np.column_stack(seed_te).mean(axis=1)\n",
    "\n",
    "# --- 年バランス Platt（ロジスティック） ---\n",
    "w = pd.Series(1.0, index=np.arange(len(train)))\n",
    "for y in np.unique(years):\n",
    "    idx = np.where(years == y)[0]\n",
    "    if len(idx) > 0:\n",
    "        w.iloc[idx] = 1.0 / len(idx)\n",
    "\n",
    "# ※ class_weight は使わず、年バランスの sample_weight のみ\n",
    "lr = LogisticRegression(C=0.5, class_weight=None, max_iter=500, solver=\"lbfgs\")\n",
    "lr.fit(oof.reshape(-1,1), y_all, sample_weight=w.values)\n",
    "oof_cal = lr.predict_proba(oof.reshape(-1,1))[:,1]\n",
    "th_opt, f1_macro = macro_f1_by_year_threshold(y_all, oof_cal, years)\n",
    "\n",
    "test_cal = lr.predict_proba(test_mean.reshape(-1,1))[:,1]\n",
    "y_pred   = (test_cal >= th_opt).astype(int)\n",
    "\n",
    "print(f\"🔎 OOF macro-F1(by year) = {f1_macro:.6f} | th={th_opt:.3f} | it_full≈{it_full}\")\n",
    "print(f\"   CatBoost params = {cb_best}\")\n",
    "\n",
    "# --- ゲート & 出力 ---\n",
    "prev_best = 0.0\n",
    "if BEST_TRACK.exists():\n",
    "    try: prev_best = float(BEST_TRACK.read_text().strip())\n",
    "    except: prev_best = 0.0\n",
    "gate = 0.640\n",
    "if (f1_macro >= gate) and (f1_macro >= prev_best + 0.002):\n",
    "    ver = next_ver(OUT_DIR)\n",
    "    out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "    out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"OOF macro-F1(by year): {f1_macro:.6f}\\n\")\n",
    "        f.write(f\"threshold: {th_opt:.3f}\\n\")\n",
    "        f.write(f\"it_full: {it_full}\\n\")\n",
    "        f.write(f\"params: {cb_best}\\n\")\n",
    "        f.write(f\"features: {X_cols}\\n\")\n",
    "    BEST_TRACK.write_text(f\"{f1_macro:.6f}\")\n",
    "    print(f\"✅ 提出CSVを保存: {out_csv}\")\n",
    "    print(f\"📝 ログ保存: {out_log}\")\n",
    "else:\n",
    "    need = max(gate, prev_best + 0.002)\n",
    "    print(f\"⛔ 提出しない: OOF={f1_macro:.6f} < gate {need:.3f} (prev_best={prev_best:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f7bf7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using LGB params**: {'learning_rate': np.float64(0.01402150923749871), 'num_leaves': 55, 'max_depth': 11, 'min_child_samples': 14, 'subsample': 0.85, 'colsample_bytree': 0.95, 'reg_alpha': 1.7, 'reg_lambda': 5.0, 'class_weight': 'balanced', 'verbosity': -1}\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2254]\tvalid_0's binary_logloss: 0.255337\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1575]\tvalid_0's binary_logloss: 0.253181\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2131]\tvalid_0's binary_logloss: 0.255513\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1188]\tvalid_0's binary_logloss: 0.288993\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1025]\tvalid_0's binary_logloss: 0.292374\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1189]\tvalid_0's binary_logloss: 0.292182\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1659]\tvalid_0's binary_logloss: 0.314768\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1642]\tvalid_0's binary_logloss: 0.309431\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1822]\tvalid_0's binary_logloss: 0.311273\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1162]\tvalid_0's binary_logloss: 0.26532\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1086]\tvalid_0's binary_logloss: 0.265003\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1346]\tvalid_0's binary_logloss: 0.265668\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3188]\tvalid_0's binary_logloss: 0.195839\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2340]\tvalid_0's binary_logloss: 0.189738\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3209]\tvalid_0's binary_logloss: 0.186984\n",
      "🔎 OOF F1 (stack) = 0.606691 | th=0.770 | corr(cb,lgb)=0.884\n",
      "   OOF F1 (cb)=0.586813 | (lgb)=0.589368\n",
      "   rounds≈ cb:106 / lgb:1787\n",
      "   params_cb={'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436, 'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326}\n",
      "   params_lgb={'learning_rate': np.float64(0.01402150923749871), 'num_leaves': 55, 'max_depth': 11, 'min_child_samples': 14, 'subsample': 0.85, 'colsample_bytree': 0.95, 'reg_alpha': 1.7, 'reg_lambda': 5.0, 'class_weight': 'balanced', 'verbosity': -1}\n",
      "⛔ 提出しない: OOF=0.606691 < gate 0.640 (prev_best=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# === A-line v8g: CatBoost(生カテゴリ) + LightGBM(整数コード) スタッキング\n",
    "# LOYO OOF → Platt(全体) → 全体F1で閾値最適化 → 1本提出 ===\n",
    "import sys, subprocess, warnings, re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- 出力先 ----------\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_TRACK = OUT_DIR / \"best_oof_blend.txt\"\n",
    "\n",
    "def next_ver(dirpath: Path):\n",
    "    nums = []\n",
    "    for p in list(dirpath.glob(\"submission_A_v*.csv\")) + list(dirpath.glob(\"run_A_v*.txt\")):\n",
    "        m = re.search(r'v(\\d+)', p.stem)\n",
    "        if m: nums.append(int(m.group(1)))\n",
    "    n = max(nums)+1 if nums else 1\n",
    "    while (dirpath/f\"submission_A_v{n}.csv\").exists() or (dirpath/f\"run_A_v{n}.txt\").exists():\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "# ---------- データ（前処理セルのtrain_df/test_dfを利用） ----------\n",
    "assert 'train_df' in globals() and 'test_df' in globals()\n",
    "train = train_df.copy().reset_index(drop=True)\n",
    "test  = test_df.copy().reset_index(drop=True)\n",
    "y_all = train['LoanStatus'].astype(int).to_numpy()\n",
    "\n",
    "# 既定の基本特徴\n",
    "if 'base_feats' not in globals():\n",
    "    base_feats = [\n",
    "        'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "        'ratio1','interact1','ratio2','interact2',\n",
    "        'ApprovalFiscalYear','InitialInterestRate','TermInMonths',\n",
    "        'CongressionalDistrict','RevolverStatus'\n",
    "    ]\n",
    "if 'cat_cols' not in globals():\n",
    "    cat_cols  = ['Subprogram','FixedOrVariableInterestInd','NaicsSector','BusinessType','BusinessAge','CollateralInd']\n",
    "\n",
    "# 交差カテゴリ\n",
    "def make_year_crosses(df):\n",
    "    out = df.copy()\n",
    "    out['YearSector']   = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['NaicsSector'].astype('string').fillna('UNK')\n",
    "    out['YearProgram']  = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['Subprogram'].astype('string').fillna('UNK')\n",
    "    out['YearAge']      = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['BusinessAge'].astype('string').fillna('UNK')\n",
    "    out['YearRevolver'] = out['ApprovalFiscalYear'].astype('Int64').astype('string').fillna('UNK') + '_' + out['RevolverStatus'].astype('Int64').astype('string').fillna('UNK')\n",
    "    return out\n",
    "train = make_year_crosses(train)\n",
    "test  = make_year_crosses(test)\n",
    "cross_cols = ['YearSector','YearProgram','YearAge','YearRevolver']\n",
    "\n",
    "# 年内Z\n",
    "def add_year_z(df, cols, year_col='ApprovalFiscalYear'):\n",
    "    out = df.copy()\n",
    "    g = out.groupby(year_col)\n",
    "    for c in cols:\n",
    "        mu = g[c].transform('mean')\n",
    "        sd = g[c].transform('std').replace(0, 1.0)\n",
    "        out[c + '_zY'] = ((out[c] - mu) / sd).astype(float)\n",
    "    return out\n",
    "z_num_cols = ['GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "              'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2']\n",
    "train = add_year_z(train, z_num_cols)\n",
    "test  = add_year_z(test,  z_num_cols)\n",
    "z_cols = [c+'_zY' for c in z_num_cols]\n",
    "\n",
    "# ---------- CatBoost 準備 ----------\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"catboost\"])\n",
    "    from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "for c in cat_cols + cross_cols:\n",
    "    train[c] = train[c].astype('string')\n",
    "    test[c]  = test[c].astype('string')\n",
    "\n",
    "X_cols_cb = base_feats + z_cols + cat_cols + cross_cols\n",
    "X_cb = train[X_cols_cb].copy()\n",
    "T_cb = test [X_cols_cb].copy()\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in (cat_cols + cross_cols)]\n",
    "\n",
    "cb_best = {\n",
    "    'lr': 0.048769058293601134, 'depth': 6, 'l2': 2.4822218413698436,\n",
    "    'rsm': 0.714998523211578, 'bagtemp': 0.01788813282482593, 'rstr': 0.03237341746516326\n",
    "}\n",
    "if 'best_params' in globals():  # もしOptunaの結果があれば上書き\n",
    "    for k in ['lr','depth','l2','rsm','bagtemp','rstr']:\n",
    "        if k in best_params: cb_best[k] = best_params[k]\n",
    "\n",
    "# ---------- LightGBM 用の整数コード ----------\n",
    "def fit_code_maps(df, cols):\n",
    "    maps = {}\n",
    "    for c in cols:\n",
    "        cats = pd.Categorical(df[c]).categories\n",
    "        maps[c] = {v:i for i,v in enumerate(cats)}\n",
    "    return maps\n",
    "\n",
    "def apply_code_maps(df, maps, suffix=\"_code\"):\n",
    "    out = df.copy()\n",
    "    for c, mp in maps.items():\n",
    "        out[c+suffix] = out[c].map(mp).astype('Int32').fillna(-1).astype(int)\n",
    "    return out\n",
    "\n",
    "code_maps = fit_code_maps(train, cat_cols + cross_cols)\n",
    "train_lgb = apply_code_maps(train, code_maps)\n",
    "test_lgb  = apply_code_maps(test,  code_maps)\n",
    "\n",
    "X_cols_lgb = base_feats + z_cols + [c+\"_code\" for c in (cat_cols + cross_cols)]\n",
    "X_lgb = train_lgb[X_cols_lgb].copy()\n",
    "T_lgb = test_lgb [X_cols_lgb].copy()\n",
    "\n",
    "# --- LGBM パラメータ（安全デフォルトに rs_lgb を上書きする方式） ---\n",
    "lgb_base = dict(\n",
    "    learning_rate=0.024,     # 安全デフォルト\n",
    "    num_leaves=96,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=45,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.95,\n",
    "    reg_alpha=1.7,\n",
    "    reg_lambda=5.0,\n",
    "    class_weight='balanced',\n",
    "    verbosity=-1\n",
    ")\n",
    "# rs_lgb があれば上書き（足りない項目はデフォルトが残る）\n",
    "if 'rs_lgb' in globals():\n",
    "    lgb_base.update(rs_lgb.best_params_)\n",
    "\n",
    "# 以降で二重指定を避けるために必ず外す\n",
    "lgb_base.pop('n_estimators', None)\n",
    "\n",
    "print(\"**Using LGB params**:\", lgb_base)\n",
    "\n",
    "\n",
    "# ---------- LOYO folds ----------\n",
    "uniq_years = sorted(pd.unique(train['ApprovalFiscalYear'].dropna()))\n",
    "folds = [np.where(train['ApprovalFiscalYear'].values == yr)[0] for yr in uniq_years]\n",
    "\n",
    "# ---------- 学習（OOF） ----------\n",
    "seeds = [0,1,2]\n",
    "oof_cb = np.zeros(len(X_cb)); oof_lgb = np.zeros(len(X_lgb))\n",
    "best_rounds_cb = []; best_rounds_lgb = []\n",
    "\n",
    "for va_idx in folds:\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_cb)), va_idx)\n",
    "    Xtr_cb, Xva_cb = X_cb.iloc[tr_idx], X_cb.iloc[va_idx]\n",
    "    ytr, yva       = y_all[tr_idx], y_all[va_idx]\n",
    "    Xtr_lgb, Xva_lgb = X_lgb.iloc[tr_idx], X_lgb.iloc[va_idx]\n",
    "\n",
    "    dtr_cb, dva_cb = Pool(Xtr_cb, ytr, cat_features=cat_idx), Pool(Xva_cb, yva, cat_features=cat_idx)\n",
    "\n",
    "    probs_cb, probs_lgb = [], []\n",
    "    for sd in seeds:\n",
    "        # CatBoost\n",
    "        cb = CatBoostClassifier(\n",
    "            loss_function='Logloss', eval_metric='Logloss',\n",
    "            iterations=4000, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "            l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "            bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "            random_seed=sd, early_stopping_rounds=200,\n",
    "            auto_class_weights='Balanced', verbose=False, allow_writing_files=False\n",
    "        )\n",
    "        cb.fit(dtr_cb, eval_set=dva_cb, use_best_model=True)\n",
    "        it = int(cb.get_best_iteration() or 4000)\n",
    "        best_rounds_cb.append(it)\n",
    "        probs_cb.append(cb.predict_proba(dva_cb, ntree_end=it)[:,1])\n",
    "\n",
    "        # LightGBM（n_estimators の二重指定を回避）\n",
    "        params = dict(lgb_base); params['random_state'] = sd\n",
    "        params.pop('n_estimators', None)\n",
    "        lgb = LGBMClassifier(**params, n_estimators=4000)\n",
    "        lgb.fit(\n",
    "            Xtr_lgb, ytr,\n",
    "            eval_set=[(Xva_lgb, yva)], eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=200), log_evaluation(period=0)]\n",
    "        )\n",
    "        it_l = int(getattr(lgb, \"best_iteration_\", 4000))\n",
    "        best_rounds_lgb.append(it_l)\n",
    "        probs_lgb.append(lgb.predict_proba(Xva_lgb, num_iteration=it_l)[:,1])\n",
    "\n",
    "    oof_cb [va_idx] = np.column_stack(probs_cb ).mean(axis=1)\n",
    "    oof_lgb[va_idx] = np.column_stack(probs_lgb).mean(axis=1)\n",
    "\n",
    "# ---------- スタッキング（Platt, 全体F1で閾値） ----------\n",
    "S_oof = np.column_stack([oof_cb, oof_lgb])\n",
    "meta = LogisticRegression(C=2.0, class_weight='balanced', max_iter=1000, solver='lbfgs')\n",
    "meta.fit(S_oof, y_all)\n",
    "oof_blend = meta.predict_proba(S_oof)[:,1]\n",
    "\n",
    "ths = np.linspace(0.05, 0.95, 181)\n",
    "f1s = [f1_score(y_all, (oof_blend>=t).astype(int)) for t in ths]\n",
    "best_i = int(np.argmax(f1s)); th_opt = float(ths[best_i]); oof_f1 = float(f1s[best_i])\n",
    "\n",
    "corr = np.corrcoef(oof_cb, oof_lgb)[0,1]\n",
    "print(f\"🔎 OOF F1 (stack) = {oof_f1:.6f} | th={th_opt:.3f} | corr(cb,lgb)={corr:.3f}\")\n",
    "print(f\"   OOF F1 (cb)={f1_score(y_all,(oof_cb>=0.5).astype(int)):.6f} | (lgb)={f1_score(y_all,(oof_lgb>=0.5).astype(int)):.6f}\")\n",
    "print(f\"   rounds≈ cb:{int(np.mean(best_rounds_cb))} / lgb:{int(np.mean(best_rounds_lgb))}\")\n",
    "print(f\"   params_cb={cb_best}\")\n",
    "print(f\"   params_lgb={lgb_base}\")\n",
    "\n",
    "# ---------- full fit → test 予測 ----------\n",
    "it_cb_full  = int(np.round(np.mean(best_rounds_cb)))\n",
    "it_lgb_full = int(np.round(np.mean(best_rounds_lgb)))\n",
    "\n",
    "# CatBoost full\n",
    "dtr_full_cb = Pool(X_cb, y_all, cat_features=cat_idx)\n",
    "dte_full_cb = Pool(T_cb, cat_features=cat_idx)\n",
    "te_cb_list = []\n",
    "for sd in seeds:\n",
    "    cb = CatBoostClassifier(\n",
    "        loss_function='Logloss', eval_metric='Logloss',\n",
    "        iterations=it_cb_full, learning_rate=cb_best['lr'], depth=cb_best['depth'],\n",
    "        l2_leaf_reg=cb_best['l2'], rsm=cb_best['rsm'],\n",
    "        bagging_temperature=cb_best['bagtemp'], random_strength=cb_best['rstr'],\n",
    "        random_seed=sd, auto_class_weights='Balanced',\n",
    "        verbose=False, allow_writing_files=False\n",
    "    )\n",
    "    cb.fit(dtr_full_cb)\n",
    "    te_cb_list.append(cb.predict_proba(dte_full_cb)[:,1])\n",
    "test_cb = np.column_stack(te_cb_list).mean(axis=1)\n",
    "\n",
    "# LightGBM full（ここも二重指定回避）\n",
    "te_lgb_list = []\n",
    "for sd in seeds:\n",
    "    params = dict(lgb_base); params['random_state'] = sd\n",
    "    params.pop('n_estimators', None)\n",
    "    lgb = LGBMClassifier(**params, n_estimators=it_lgb_full)\n",
    "    lgb.fit(X_lgb, y_all)\n",
    "    te_lgb_list.append(lgb.predict_proba(T_lgb)[:,1])\n",
    "test_lgb = np.column_stack(te_lgb_list).mean(axis=1)\n",
    "\n",
    "# stack & threshold\n",
    "S_test = np.column_stack([test_cb, test_lgb])\n",
    "test_blend = meta.predict_proba(S_test)[:,1]\n",
    "y_pred = (test_blend >= th_opt).astype(int)\n",
    "\n",
    "# ---------- ゲート & 出力 ----------\n",
    "prev_best = 0.0\n",
    "if BEST_TRACK.exists():\n",
    "    try: prev_best = float(BEST_TRACK.read_text().strip())\n",
    "    except: prev_best = 0.0\n",
    "gate = 0.640\n",
    "if (oof_f1 >= gate) and (oof_f1 >= prev_best + 0.002):\n",
    "    ver = next_ver(OUT_DIR)\n",
    "    out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "    out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "    ids = test['id'] if 'id' in test.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"OOF F1 (stack): {oof_f1:.6f}\\n\")\n",
    "        f.write(f\"threshold: {th_opt:.3f}\\n\")\n",
    "        f.write(f\"corr(cb,lgb): {corr:.6f}\\n\")\n",
    "        f.write(f\"rounds(full): cb={it_cb_full}, lgb={it_lgb_full}\\n\")\n",
    "        f.write(f\"params_cb: {cb_best}\\n\")\n",
    "        f.write(f\"params_lgb: {lgb_base}\\n\")\n",
    "        f.write(f\"features_cb: {X_cols_cb}\\n\")\n",
    "        f.write(f\"features_lgb: {X_cols_lgb}\\n\")\n",
    "    BEST_TRACK.write_text(f\"{oof_f1:.6f}\")\n",
    "    print(f\"✅ 提出CSVを保存: {out_csv}\")\n",
    "    print(f\"📝 ログ保存: {out_log}\")\n",
    "else:\n",
    "    need = max(gate, prev_best + 0.002)\n",
    "    print(f\"⛔ 提出しない: OOF={oof_f1:.6f} < gate {need:.3f} (prev_best={prev_best:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "33e4cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using LGB params**: {'learning_rate': np.float64(0.01402150923749871), 'num_leaves': 55, 'max_depth': 11, 'min_child_samples': 14, 'subsample': 0.8476028533782086, 'colsample_bytree': 0.9644797168301948, 'reg_alpha': 1.7250263945237354, 'reg_lambda': 4.982743504628338, 'class_weight': 'balanced', 'verbosity': -1}\n"
     ]
    }
   ],
   "source": [
    "# --- LGBM パラメータ（安全デフォルトに rs_lgb/optuna結果を上書き） ---\n",
    "lgb_base = dict(\n",
    "    learning_rate=0.024781124923563432,\n",
    "    num_leaves=93,\n",
    "    max_depth=12,\n",
    "    min_child_samples=45,\n",
    "    subsample=0.8476028533782086,\n",
    "    colsample_bytree=0.9644797168301948,\n",
    "    reg_alpha=1.7250263945237354,\n",
    "    reg_lambda=4.982743504628338,\n",
    "    class_weight='balanced',\n",
    "    verbosity=-1\n",
    ")\n",
    "# rs_lgb があれば差分だけ上書き（不足は上の安全値が残る）\n",
    "if 'rs_lgb' in globals():\n",
    "    lgb_base.update(rs_lgb.best_params_)\n",
    "\n",
    "# 二重指定の事故防止\n",
    "lgb_base.pop('n_estimators', None)\n",
    "\n",
    "print(\"**Using LGB params**:\", lgb_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9e89c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Calibrated-blend OOF F1 = 0.613985 | best_w(cb)=0.625 | best_th=0.340\n",
      "✅ submission を保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\submission_A_v45.csv\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v45.txt\n"
     ]
    }
   ],
   "source": [
    "# === ブレンド修正セル：等単調校正 → 重み＆しきい値グリッド最適化 ===\n",
    "import numpy as np\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1) 各モデルを OOF で等単調校正\n",
    "iso_cb  = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_lgb = IsotonicRegression(out_of_bounds='clip')\n",
    "oof_cb_cal  = iso_cb.fit_transform(oof_cb,  y_all)\n",
    "oof_lgb_cal = iso_lgb.fit_transform(oof_lgb, y_all)\n",
    "\n",
    "test_cb_cal  = iso_cb.transform(test_cb)\n",
    "test_lgb_cal = iso_lgb.transform(test_lgb)\n",
    "\n",
    "# 2) 重み w と 閾値 th を同時にグリッド探索（軽い粗探索）\n",
    "weights = np.linspace(0.00, 1.00, 41)      # 0.00, 0.025, …, 1.00\n",
    "ths     = np.linspace(0.10, 0.90, 161)     # 0.10〜0.90を細かく\n",
    "best = (-1, -1, -1.0)                      # (best_f1, best_w, best_th)\n",
    "\n",
    "for w in weights:\n",
    "    blend_oof = w * oof_cb_cal + (1.0 - w) * oof_lgb_cal\n",
    "    # 先に粗く最良近傍を探す\n",
    "    for th in ths:\n",
    "        f1 = f1_score(y_all, (blend_oof >= th).astype(int))\n",
    "        if f1 > best[0]:\n",
    "            best = (f1, w, th)\n",
    "\n",
    "best_f1, best_w, best_th = best\n",
    "print(f\"🔧 Calibrated-blend OOF F1 = {best_f1:.6f} | best_w(cb)={best_w:.3f} | best_th={best_th:.3f}\")\n",
    "\n",
    "# 3) test も同じ重みでブレンドし、最良しきい値で二値化\n",
    "blend_test = best_w * test_cb_cal + (1.0 - best_w) * test_lgb_cal\n",
    "y_test_pred = (blend_test >= best_th).astype(int)\n",
    "\n",
    "# 4) 提出（いつもの versioning/gating を流用）\n",
    "from pathlib import Path\n",
    "import re, pandas as pd\n",
    "\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "def _ver(pathlist):\n",
    "    mx = 0\n",
    "    for p in pathlist:\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return mx + 1 if mx > 0 else 1\n",
    "next_ver = _ver(existing)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{next_ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{next_ver}.txt\"\n",
    "\n",
    "ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "pd.DataFrame({'id': ids, 'LoanStatus': y_test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Calibrated-blend OOF F1: {best_f1:.6f}\\n\")\n",
    "    f.write(f\"Best weight (cb): {best_w:.6f}\\n\")\n",
    "    f.write(f\"Best threshold: {best_th:.6f}\\n\")\n",
    "    f.write(f\"Rounds≈ cb:{'N/A' if 'best_rounds_cb' not in globals() else int(np.mean(best_rounds_cb))} / \"\n",
    "            f\"lgb:{'N/A' if 'best_rounds_lgb' not in globals() else int(np.mean(best_rounds_lgb))}\\n\")\n",
    "\n",
    "print(f\"✅ submission を保存: {out_csv}\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f956234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(by year) = 0.622394\n",
      "⛔ 提出しない: macro-F1=0.622394 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y: 年度ごとに 等単調校正(Isotonic) → 重みw & しきい値th 最適化 → 1本提出 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 必須変数チェック\n",
    "required = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df']\n",
    "missing = [v for v in required if v not in globals()]\n",
    "assert not missing, f\"必要な変数が未定義: {missing}\"\n",
    "\n",
    "years = train_df['ApprovalFiscalYear'].astype(int).values\n",
    "years_unique = sorted(np.unique(years))\n",
    "assert len(years_unique) >= 3, \"年度ユニーク数が少なすぎます\"\n",
    "\n",
    "# 1) モデルごとに等単調校正（全OOFで1本ずつ）\n",
    "iso_cb  = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_lgb = IsotonicRegression(out_of_bounds='clip')\n",
    "oof_cb_cal  = iso_cb.fit_transform(oof_cb,  y_all)\n",
    "oof_lgb_cal = iso_lgb.fit_transform(oof_lgb, y_all)\n",
    "test_cb_cal  = iso_cb.transform(test_cb)\n",
    "test_lgb_cal = iso_lgb.transform(test_lgb)\n",
    "\n",
    "# 2) 年度ごとに w, th を最適化（macro-F1 を意識）\n",
    "w_grid  = np.linspace(0.00, 1.00, 41)     # 0.00〜1.00, step=0.025\n",
    "th_grid = np.linspace(0.10, 0.60, 101)    # 0.10〜0.60, step=0.005\n",
    "\n",
    "best_by_year = {}  # year -> (best_f1, best_w, best_th)\n",
    "oof_preds = np.zeros_like(y_all, dtype=int)\n",
    "\n",
    "for yy in years_unique:\n",
    "    idx = np.where(years == yy)[0]\n",
    "    if len(idx) < 120:  # サンプルが少なすぎる年は全体最適の代替に\n",
    "        best_by_year[yy] = None\n",
    "        continue\n",
    "    yy_cb  = oof_cb_cal[idx]\n",
    "    yy_lgb = oof_lgb_cal[idx]\n",
    "    yy_y   = y_all.iloc[idx].values if hasattr(y_all, \"iloc\") else np.array(y_all)[idx]\n",
    "\n",
    "    best = (-1.0, 0.5, 0.3)\n",
    "    for w in w_grid:\n",
    "        blend = w*yy_cb + (1.0-w)*yy_lgb\n",
    "        # しきい値サーチ\n",
    "        # ベクトル化で高速化\n",
    "        B = blend.reshape(-1,1) >= th_grid.reshape(1,-1)\n",
    "        f1s = [f1_score(yy_y, B[:,j].astype(int)) for j in range(B.shape[1])]\n",
    "        j = int(np.argmax(f1s))\n",
    "        if f1s[j] > best[0]:\n",
    "            best = (float(f1s[j]), float(w), float(th_grid[j]))\n",
    "\n",
    "    best_by_year[yy] = best\n",
    "    # OOF予測を書き込む（あとでmacro-F1算出）\n",
    "    blend = best[1]*yy_cb + (1.0-best[1])*yy_lgb\n",
    "    oof_preds[idx] = (blend >= best[2]).astype(int)\n",
    "\n",
    "# 3) macro-F1（年度平均）を表示\n",
    "f1s_year = []\n",
    "for yy in years_unique:\n",
    "    idx = np.where(years == yy)[0]\n",
    "    if len(idx)==0: continue\n",
    "    f1s_year.append(f1_score(y_all.iloc[idx], oof_preds[idx]) if hasattr(y_all,\"iloc\")\n",
    "                    else f1_score(np.array(y_all)[idx], oof_preds[idx]))\n",
    "macro_f1 = float(np.mean(f1s_year))\n",
    "print(f\"🔎 OOF macro-F1(by year) = {macro_f1:.6f}\")\n",
    "\n",
    "# 4) テスト側も年度ごとに最適 w,th で二値化\n",
    "test_years = test_df['ApprovalFiscalYear'].astype(int).values\n",
    "test_pred = np.zeros(len(test_df), dtype=int)\n",
    "\n",
    "# フォールバック（小サンプル年などNoneだった場合用）：全体での最良w,th\n",
    "# 全体w,thを粗探索（安全網）\n",
    "blend_cb, blend_lgb = oof_cb_cal, oof_lgb_cal\n",
    "best_global = (-1.0, 0.5, 0.3)\n",
    "for w in w_grid:\n",
    "    g_blend = w*blend_cb + (1.0-w)*blend_lgb\n",
    "    B = g_blend.reshape(-1,1) >= th_grid.reshape(1,-1)\n",
    "    f1s = [f1_score(y_all, B[:,j].astype(int)) for j in range(B.shape[1])]\n",
    "    j = int(np.argmax(f1s))\n",
    "    if f1s[j] > best_global[0]:\n",
    "        best_global = (float(f1s[j]), float(w), float(th_grid[j]))\n",
    "\n",
    "for yy in np.unique(test_years):\n",
    "    jdx = np.where(test_years == yy)[0]\n",
    "    w_th = best_by_year.get(yy, None)\n",
    "    if (w_th is None) or (w_th[0] < 0):\n",
    "        w_use, th_use = best_global[1], best_global[2]\n",
    "    else:\n",
    "        w_use, th_use = w_th[1], w_th[2]\n",
    "    blend_t = w_use*test_cb_cal[jdx] + (1.0-w_use)*test_lgb_cal[jdx]\n",
    "    test_pred[jdx] = (blend_t >= th_use).astype(int)\n",
    "\n",
    "# 5) 提出（バージョン自動付番 / gate あり）\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "def next_version(paths):\n",
    "    mx = 0\n",
    "    for p in paths:\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return mx + 1 if mx>0 else 1\n",
    "ver = next_version(existing)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "# gate: 0.640 以上のみ出力（上書き回避）\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    print(f\"✅ submission を保存: {out_csv}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "# ログ\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(by year): {macro_f1:.6f}\\n\")\n",
    "    for yy in years_unique:\n",
    "        f.write(f\"Y{yy}: {best_by_year.get(yy)}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1c1a2f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(year×rev) = 0.615671\n",
      "Segments: [(np.int64(2020), np.int64(0)), (np.int64(2020), np.int64(1)), (np.int64(2021), np.int64(0)), (np.int64(2021), np.int64(1)), (np.int64(2022), np.int64(0))] ... total=10 | MIN_N=200, LAMBDA=300\n",
      "⛔ 提出しない: macro-F1=0.615671 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7yr: 年度×Revolver セグメント校正 + セグメント別 w,th 最適化 → 1本提出 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 必須変数チェック\n",
    "required = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df']\n",
    "missing = [v for v in required if v not in globals()]\n",
    "assert not missing, f\"未定義: {missing}\"\n",
    "\n",
    "# セグメントキー（年度×Revolver）\n",
    "years = train_df['ApprovalFiscalYear'].astype(int).values\n",
    "revs  = train_df['RevolverStatus'].astype(int).values\n",
    "keys  = list(zip(years, revs))\n",
    "uniq_keys = sorted(set(keys))\n",
    "test_keys = list(zip(test_df['ApprovalFiscalYear'].astype(int).values,\n",
    "                     test_df['RevolverStatus'].astype(int).values))\n",
    "\n",
    "# ハイパラ（必要ならここだけ触る）\n",
    "MIN_N   = 200     # この件数未満のセグメントは年単位 or 全体にフォールバック\n",
    "LAMBDA  = 300     # セグメント校正の縮約強度（n/(n+LAMBDA)）\n",
    "W_GRID  = np.linspace(0.00, 1.00, 41)   # ブレンド重み探索\n",
    "TH_GRID = np.linspace(0.10, 0.60, 101)  # しきい値探索\n",
    "\n",
    "# 1) まず全体でモデル別に等単調校正\n",
    "iso_cb_global  = IsotonicRegression(out_of_bounds='clip').fit(oof_cb,  y_all)\n",
    "iso_lgb_global = IsotonicRegression(out_of_bounds='clip').fit(oof_lgb, y_all)\n",
    "oof_cb_g  = iso_cb_global.transform(oof_cb)\n",
    "oof_lgb_g = iso_lgb_global.transform(oof_lgb)\n",
    "test_cb_g  = iso_cb_global.transform(test_cb)\n",
    "test_lgb_g = iso_lgb_global.transform(test_lgb)\n",
    "\n",
    "# 2) セグメント毎に「局所等単調校正」を学習し、n/(n+LAMBDA)で大域とブレンドして縮約\n",
    "def segment_calibrate(proba, proba_seg, y_seg, iso_global):\n",
    "    if len(y_seg) < MIN_N:\n",
    "        # セグメントが小さい → 全体校正のみ\n",
    "        return iso_global.transform(proba_seg), 0.0\n",
    "    iso_seg = IsotonicRegression(out_of_bounds='clip').fit(proba_seg, y_seg)\n",
    "    p_local  = iso_seg.transform(proba_seg)\n",
    "    p_global = iso_global.transform(proba_seg)\n",
    "    w = len(y_seg) / (len(y_seg) + LAMBDA)\n",
    "    return (w * p_local + (1.0 - w) * p_global), w\n",
    "\n",
    "# 3) セグメント別に w, th を最適化（OOFで F1 最大）\n",
    "y_true = np.array(y_all).astype(int)\n",
    "oof_pred_lbl = np.zeros(len(y_true), dtype=int)\n",
    "seg_info = {}  # (year,rev) -> dict\n",
    "\n",
    "for seg in uniq_keys:\n",
    "    idx = np.where((years==seg[0]) & (revs==seg[1]))[0]\n",
    "    if len(idx) == 0: \n",
    "        continue\n",
    "\n",
    "    # 局所+縮約等単調\n",
    "    cb_cal, w_cb = segment_calibrate(oof_cb,  oof_cb[idx],  y_true[idx], iso_cb_global)\n",
    "    lgb_cal, w_l = segment_calibrate(oof_lgb, oof_lgb[idx], y_true[idx], iso_lgb_global)\n",
    "\n",
    "    # ブレンド & しきい値サーチ\n",
    "    best = (-1.0, 0.5, 0.3)  # f1, w, th\n",
    "    for w in W_GRID:\n",
    "        blend = w*cb_cal + (1.0-w)*lgb_cal\n",
    "        B = blend.reshape(-1,1) >= TH_GRID.reshape(1,-1)\n",
    "        # ベクトル化（簡明のためループ）\n",
    "        f1s = [f1_score(y_true[idx], B[:,j].astype(int)) for j in range(B.shape[1])]\n",
    "        j = int(np.argmax(f1s))\n",
    "        if f1s[j] > best[0]:\n",
    "            best = (float(f1s[j]), float(w), float(TH_GRID[j]))\n",
    "\n",
    "    # OOF ラベル確定\n",
    "    oof_pred_lbl[idx] = (best[1]*cb_cal + (1.0-best[1])*lgb_cal >= best[2]).astype(int)\n",
    "\n",
    "    seg_info[seg] = {\n",
    "        'n': int(len(idx)),\n",
    "        'w_cb': float(w_cb), 'w_lgb': float(w_l),\n",
    "        'best_f1': best[0], 'blend_w': best[1], 'thr': best[2]\n",
    "    }\n",
    "\n",
    "# 4) 年度 macro-F1 を集計\n",
    "macro_f1 = []\n",
    "for yy in sorted(np.unique(years)):\n",
    "    yy_idx = np.where(years==yy)[0]\n",
    "    macro_f1.append(f1_score(y_true[yy_idx], oof_pred_lbl[yy_idx]) if len(yy_idx)>0 else 0.0)\n",
    "macro_f1 = float(np.mean(macro_f1))\n",
    "print(f\"🔎 OOF macro-F1(year×rev) = {macro_f1:.6f}\")\n",
    "print(f\"Segments: {uniq_keys[:5]} ... total={len(uniq_keys)} | MIN_N={MIN_N}, LAMBDA={LAMBDA}\")\n",
    "\n",
    "# 5) テスト予測：セグメント別の校正→ブレンド→しきい値\n",
    "test_pred = np.zeros(len(test_df), dtype=int)\n",
    "for seg in uniq_keys:\n",
    "    jdx = np.where((np.array(test_keys)==seg).all(axis=1))[0] if len(test_keys)>0 else []\n",
    "    if len(jdx)==0: \n",
    "        continue\n",
    "\n",
    "    # セグメント側の校正（OOF時と同じ縮約重みで test 側も）\n",
    "    idx = np.where((years==seg[0]) & (revs==seg[1]))[0]\n",
    "    # もし学習側セグメントが無い/小さい → 全体校正 & 全体最適へフォールバック\n",
    "    if seg not in seg_info or seg_info[seg]['n'] < MIN_N:\n",
    "        cb_t = test_cb_g[jdx]\n",
    "        lgb_t = test_lgb_g[jdx]\n",
    "        # フォールバック：全体最適 w,th を再計算（軽く）\n",
    "        # 全体blend最適（粗探索）\n",
    "        best_global = (-1.0, 0.5, 0.3)\n",
    "        for w in W_GRID:\n",
    "            blend = w*oof_cb_g + (1.0-w)*oof_lgb_g\n",
    "            B = blend.reshape(-1,1) >= TH_GRID.reshape(1,-1)\n",
    "            f1s = [f1_score(y_true, B[:,k].astype(int)) for k in range(B.shape[1])]\n",
    "            k = int(np.argmax(f1s))\n",
    "            if f1s[k] > best_global[0]:\n",
    "                best_global = (float(f1s[k]), float(w), float(TH_GRID[k]))\n",
    "        test_pred[jdx] = (best_global[1]*cb_t + (1.0-best_global[1])*lgb_t >= best_global[2]).astype(int)\n",
    "        continue\n",
    "\n",
    "    info = seg_info[seg]\n",
    "    # セグメント縮約の「w_cb, w_lgb」を使って test を校正\n",
    "    # 縮約： p_seg = w*iso_seg(test_seg) + (1-w)*iso_global(test_seg)\n",
    "    # → iso_seg が必要だが、簡略化のため OOF 時と同じ係数で\n",
    "    #    学習データのセグメントで再フィットして適用する\n",
    "    idx = np.where((years==seg[0]) & (revs==seg[1]))[0]\n",
    "    # 再学習\n",
    "    iso_cb_seg  = IsotonicRegression(out_of_bounds='clip').fit(oof_cb[idx],  y_true[idx]) if info['n']>=MIN_N else None\n",
    "    iso_lgb_seg = IsotonicRegression(out_of_bounds='clip').fit(oof_lgb[idx], y_true[idx]) if info['n']>=MIN_N else None\n",
    "\n",
    "    cb_local  = iso_cb_seg.transform(test_cb[jdx])   if iso_cb_seg  else test_cb_g[jdx]\n",
    "    lgb_local = iso_lgb_seg.transform(test_lgb[jdx]) if iso_lgb_seg else test_lgb_g[jdx]\n",
    "    cb_t = info['w_cb'] * cb_local  + (1.0 - info['w_cb']) * test_cb_g[jdx]\n",
    "    lgb_t= info['w_lgb']* lgb_local + (1.0 - info['w_lgb'])* test_lgb_g[jdx]\n",
    "\n",
    "    test_pred[jdx] = (info['blend_w']*cb_t + (1.0-info['blend_w'])*lgb_t >= info['thr']).astype(int)\n",
    "\n",
    "# 6) 出力（ゲートあり、連番）\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "existing = list(OUT_DIR.glob(\"submission_A_v*.csv\"))\n",
    "def next_version(paths):\n",
    "    mx = 0\n",
    "    for p in paths:\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return mx + 1 if mx>0 else 1\n",
    "ver = next_version(existing)\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': test_pred}).to_csv(out_csv, index=False, header=False)\n",
    "    print(f\"✅ submission を保存: {out_csv}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(year×rev): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"MIN_N={MIN_N}, LAMBDA={LAMBDA}\\n\")\n",
    "    for k in uniq_keys:\n",
    "        f.write(f\"{k}: {seg_info.get(k)}\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7408ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year baseline th (around which we search): {np.int64(2020): 0.68, np.int64(2021): 0.64, np.int64(2022): 0.675, np.int64(2023): 0.69, np.int64(2024): 0.695}\n",
      "✅ grid best: best_macro=0.620780 | MIN_N=80, LAMBDA=300, BAND=±0.08\n",
      "⛔ 提出しない: macro-F1=0.620780 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7yr_platt: year×revolver 縮約 + Platt校正 + 年別th近傍探索 + 粗グリッド ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 必須変数チェック\n",
    "req = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [r for r in req if r not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "revs   = train_df['RevolverStatus'].astype(int).to_numpy()\n",
    "keys   = list(zip(years, revs))\n",
    "uniq_keys = sorted(set(keys))\n",
    "\n",
    "test_years = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "test_revs  = test_df['RevolverStatus'].astype(int).to_numpy()\n",
    "test_keys  = list(zip(test_years, test_revs))\n",
    "\n",
    "# ---- Platt 校正（1次元ロジスティック）\n",
    "def platt_fit(proba, y):\n",
    "    m = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000)\n",
    "    # 1次元特徴として確率を入れる\n",
    "    X = proba.reshape(-1,1)\n",
    "    m.fit(X, y)\n",
    "    return m\n",
    "\n",
    "def platt_tr(model, proba):\n",
    "    return model.predict_proba(proba.reshape(-1,1))[:,1]\n",
    "\n",
    "# ---- 出力ユーティリティ\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def next_version(globpat=\"submission_A_v*.csv\"):\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(globpat):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx>0 else 1\n",
    "\n",
    "# ---- 年別のベース（global blend）から最適しきい値を先に取る\n",
    "W_GRID_BASE  = np.linspace(0.0, 1.0, 21)\n",
    "TH_GRID_BASE = np.linspace(0.05, 0.70, 132)\n",
    "\n",
    "# global Platt（モデル別）\n",
    "pl_cb_g  = platt_fit(oof_cb,  y_true); oof_cb_g  = platt_tr(pl_cb_g,  oof_cb)\n",
    "pl_lgb_g = platt_fit(oof_lgb, y_true); oof_lgb_g = platt_tr(pl_lgb_g, oof_lgb)\n",
    "\n",
    "year_list = sorted(np.unique(years))\n",
    "year_thr  = {}\n",
    "year_w    = {}\n",
    "for yy in year_list:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    if len(idx)==0: continue\n",
    "    best = (-1.0, 0.5, 0.3)  # f1, w, th\n",
    "    for w in W_GRID_BASE:\n",
    "        blend = w*oof_cb_g[idx] + (1.0-w)*oof_lgb_g[idx]\n",
    "        B = blend.reshape(-1,1) >= TH_GRID_BASE.reshape(1,-1)\n",
    "        f1s = [f1_score(y_true[idx], B[:,j]) for j in range(B.shape[1])]\n",
    "        j = int(np.argmax(f1s))\n",
    "        if f1s[j] > best[0]:\n",
    "            best = (float(f1s[j]), float(w), float(TH_GRID_BASE[j]))\n",
    "    year_thr[yy] = best[2]\n",
    "    year_w[yy]   = best[1]\n",
    "\n",
    "print(\"year baseline th (around which we search):\", {y:round(t,3) for y,t in year_thr.items()})\n",
    "\n",
    "# ---- セグメント最適化（縮約 + Platt）を粗グリッドで探索\n",
    "MIN_N_LIST  = [80, 120, 160, 200, 240]\n",
    "LAMBDA_LIST = [100, 200, 300, 400, 600]\n",
    "BAND_LIST   = [0.04, 0.06, 0.08]  # 年ベースthの±帯域\n",
    "\n",
    "W_GRID_SEG  = np.linspace(0.0, 1.0, 21)\n",
    "\n",
    "best_cfg = None\n",
    "best_macro = -1\n",
    "best_pred_test = None\n",
    "best_log = None\n",
    "\n",
    "for MIN_N in MIN_N_LIST:\n",
    "    for LAMBDA in LAMBDA_LIST:\n",
    "        for BAND in BAND_LIST:\n",
    "            # セグメント毎の Platt 再学習（縮約： n/(n+LAMBDA)）\n",
    "            oof_lbl = np.zeros_like(y_true)\n",
    "            # 再学習した Platt モデルをキャッシュ（速度対策）\n",
    "            pl_cache_cb  = {}\n",
    "            pl_cache_lgb = {}\n",
    "\n",
    "            for seg in uniq_keys:\n",
    "                yy, rv = seg\n",
    "                idx = np.where((years==yy) & (revs==rv))[0]\n",
    "                if len(idx)==0: \n",
    "                    continue\n",
    "\n",
    "                # セグメントが十分なら局所Platt → 縮約、少ないなら global のみ\n",
    "                if len(idx) >= MIN_N:\n",
    "                    if seg not in pl_cache_cb:\n",
    "                        pl_cache_cb[seg]  = platt_fit(oof_cb[idx],  y_true[idx])\n",
    "                        pl_cache_lgb[seg] = platt_fit(oof_lgb[idx], y_true[idx])\n",
    "                    p_cb_loc  = platt_tr(pl_cache_cb[seg],  oof_cb[idx])\n",
    "                    p_lgb_loc = platt_tr(pl_cache_lgb[seg], oof_lgb[idx])\n",
    "                    w_sh = len(idx) / (len(idx) + LAMBDA)\n",
    "                    p_cb  = w_sh * p_cb_loc  + (1.0 - w_sh) * oof_cb_g[idx]\n",
    "                    p_lgb = w_sh * p_lgb_loc + (1.0 - w_sh) * oof_lgb_g[idx]\n",
    "                else:\n",
    "                    p_cb  = oof_cb_g[idx]\n",
    "                    p_lgb = oof_lgb_g[idx]\n",
    "\n",
    "                # 年別ベースの近傍でのみ探索（安定化）\n",
    "                th_base = year_thr.get(int(yy), 0.30)\n",
    "                THS = np.clip(np.linspace(th_base-BAND, th_base+BAND, 61), 0.02, 0.98)\n",
    "\n",
    "                best = (-1.0, 0.5, th_base)\n",
    "                for w in W_GRID_SEG:\n",
    "                    blend = w*p_cb + (1.0-w)*p_lgb\n",
    "                    B = blend.reshape(-1,1) >= THS.reshape(1,-1)\n",
    "                    f1s = [f1_score(y_true[idx], B[:,j]) for j in range(B.shape[1])]\n",
    "                    j = int(np.argmax(f1s))\n",
    "                    if f1s[j] > best[0]:\n",
    "                        best = (float(f1s[j]), float(w), float(THS[j]))\n",
    "                oof_lbl[idx] = (best[1]*p_cb + (1.0-best[1])*p_lgb >= best[2]).astype(int)\n",
    "\n",
    "            # 年度 macro-F1 を評価\n",
    "            mfs = []\n",
    "            for yy in year_list:\n",
    "                ii = np.where(years==yy)[0]\n",
    "                if len(ii)>0:\n",
    "                    mfs.append(f1_score(y_true[ii], oof_lbl[ii]))\n",
    "            macro = float(np.mean(mfs)) if mfs else 0.0\n",
    "\n",
    "            if macro > best_macro:\n",
    "                best_macro = macro\n",
    "                best_cfg = (MIN_N, LAMBDA, BAND)\n",
    "\n",
    "                # ===== テスト側も同設定で予測を作る =====\n",
    "                test_pred = np.zeros(len(test_df), dtype=int)\n",
    "                for seg in uniq_keys:\n",
    "                    yy, rv = seg\n",
    "                    idx = np.where((years==yy) & (revs==rv))[0]\n",
    "                    jdx = np.where((test_years==yy) & (test_revs==rv))[0]\n",
    "                    if len(jdx)==0: \n",
    "                        continue\n",
    "\n",
    "                    if len(idx) >= MIN_N:\n",
    "                        pl_cb  = pl_cache_cb.get(seg)  or platt_fit(oof_cb[idx],  y_true[idx])\n",
    "                        pl_lgb = pl_cache_lgb.get(seg) or platt_fit(oof_lgb[idx], y_true[idx])\n",
    "                        p_cb_loc_t  = platt_tr(pl_cb,  test_cb[jdx])\n",
    "                        p_lgb_loc_t = platt_tr(pl_lgb, test_lgb[jdx])\n",
    "                        w_sh = len(idx) / (len(idx) + LAMBDA)\n",
    "                        p_cb_t  = w_sh * p_cb_loc_t  + (1.0 - w_sh) * platt_tr(pl_cb_g,  test_cb[jdx])\n",
    "                        p_lgb_t = w_sh * p_lgb_loc_t + (1.0 - w_sh) * platt_tr(pl_lgb_g, test_lgb[jdx])\n",
    "                    else:\n",
    "                        p_cb_t  = platt_tr(pl_cb_g,  test_cb[jdx])\n",
    "                        p_lgb_t = platt_tr(pl_lgb_g, test_lgb[jdx])\n",
    "\n",
    "                    # OOFで得た最適 w,th を再検索せずに「年ベース近傍で」再取得\n",
    "                    th_base = year_thr.get(int(yy), 0.30)\n",
    "                    THS = np.clip(np.linspace(th_base-BAND, th_base+BAND, 61), 0.02, 0.98)\n",
    "                    best = (-1.0, 0.5, th_base)\n",
    "                    for w in W_GRID_SEG:\n",
    "                        blend = w*(\n",
    "                            platt_tr(pl_cb_g,  oof_cb[idx]) if len(idx)>0 else np.zeros(1)\n",
    "                        )  # ダミー。下で使わないため形保ち\n",
    "                    # ここは OOFから w を保持していないため、簡便に「w は年のベース」を使う\n",
    "                    w_default = year_w.get(int(yy), 0.5)\n",
    "                    test_pred[jdx] = (w_default*p_cb_t + (1.0-w_default)*p_lgb_t >= th_base).astype(int)\n",
    "\n",
    "                best_pred_test = test_pred.copy()\n",
    "                best_log = f\"best_macro={best_macro:.6f} | MIN_N={MIN_N}, LAMBDA={LAMBDA}, BAND=±{BAND}\"\n",
    "\n",
    "print(\"✅ grid best:\", best_log)\n",
    "\n",
    "# ===== 出力（ゲート判定付き） =====\n",
    "ver = next_version()\n",
    "out_csv = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "out_log = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "gate = 0.640\n",
    "\n",
    "if best_macro >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': best_pred_test}).to_csv(out_csv, index=False, header=False)\n",
    "    print(f\"🚀 提出CSV保存: {out_csv}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={best_macro:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(out_log, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(best_log + \"\\n\")\n",
    "print(f\"📝 ログ保存: {out_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6a6f0505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(v7y_stack) = 0.556482\n",
      "year thresholds: {2020: 0.6, 2021: 0.58, 2022: 0.565, 2023: 0.5, 2024: 0.545}\n",
      "⛔ 提出しない: macro-F1=0.556482 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y_stack: 年ダミー＋Revolver付きのロジスティック・スタッキング（LOYO） ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 必須チェック\n",
    "need = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "revs   = train_df['RevolverStatus'].astype(int).to_numpy()\n",
    "uniq_years = sorted(np.unique(years))\n",
    "\n",
    "# ---- メタ特徴を作る（確率 × 多項式 × 交互作用 + 年ダミー + Revolver） ----\n",
    "def build_meta(o_cb, o_lgb, df):\n",
    "    M = pd.DataFrame({\n",
    "        'cb':  o_cb,\n",
    "        'lgb': o_lgb,\n",
    "    }, index=df.index)\n",
    "    M['cb2']  = M['cb']  ** 2\n",
    "    M['lgb2'] = M['lgb'] ** 2\n",
    "    M['int']  = M['cb'] * M['lgb']\n",
    "    M['rev']  = df['RevolverStatus'].astype(int).to_numpy()\n",
    "    YOH = pd.get_dummies(df['ApprovalFiscalYear'].astype(int), prefix='Y', drop_first=False)\n",
    "    return pd.concat([M, YOH], axis=1)\n",
    "\n",
    "X_meta = build_meta(pd.Series(oof_cb, index=train_df.index),\n",
    "                    pd.Series(oof_lgb, index=train_df.index),\n",
    "                    train_df)\n",
    "T_meta = build_meta(pd.Series(test_cb, index=test_df.index),\n",
    "                    pd.Series(test_lgb, index=test_df.index),\n",
    "                    test_df)\n",
    "\n",
    "# ---- LOYO でメタLRを学習（年ごと外だし） ----\n",
    "oof_meta = np.zeros(len(X_meta))\n",
    "test_meta = np.zeros(len(T_meta))\n",
    "for yy in uniq_years:\n",
    "    va_idx = np.where(years == yy)[0]\n",
    "    tr_idx = np.setdiff1d(np.arange(len(X_meta)), va_idx)\n",
    "    Xtr, Xva = X_meta.iloc[tr_idx], X_meta.iloc[va_idx]\n",
    "    ytr      = y_true[tr_idx]\n",
    "\n",
    "    lr = LogisticRegression(\n",
    "        C=0.5, class_weight='balanced', max_iter=5000, solver='lbfgs'\n",
    "    )\n",
    "    lr.fit(Xtr, ytr)\n",
    "    oof_meta[va_idx] = lr.predict_proba(Xva)[:,1]\n",
    "\n",
    "    # 同じ年のテストにも適用\n",
    "    jdx = np.where(test_df['ApprovalFiscalYear'].astype(int).to_numpy() == yy)[0]\n",
    "    if len(jdx):\n",
    "        test_meta[jdx] = lr.predict_proba(T_meta.iloc[jdx])[:,1]\n",
    "\n",
    "# ---- 年別に“狭い帯域”で閾値最適化（0.30～0.60, step=0.005） ----\n",
    "THS = np.arange(0.30, 0.601, 0.005)\n",
    "year_th = {}\n",
    "f1s = []\n",
    "for yy in uniq_years:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    if len(idx)==0: continue\n",
    "    p = oof_meta[idx]\n",
    "    y = y_true[idx]\n",
    "    F = [f1_score(y, (p>=t).astype(int)) for t in THS]\n",
    "    j = int(np.argmax(F))\n",
    "    f1s.append(F[j])\n",
    "    year_th[yy] = float(THS[j])\n",
    "\n",
    "macro_f1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "print(f\"🔎 OOF macro-F1(v7y_stack) = {macro_f1:.6f}\")\n",
    "print(\"year thresholds:\", {int(k): round(v,3) for k,v in year_th.items()})\n",
    "\n",
    "# ---- テスト予測（年ごとにその閾値を適用） ----\n",
    "y_pred_test = np.zeros(len(test_meta), dtype=int)\n",
    "ty = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    if len(jdx)==0: continue\n",
    "    th = year_th.get(yy, 0.5)\n",
    "    y_pred_test[jdx] = (test_meta[jdx] >= th).astype(int)\n",
    "\n",
    "# ---- 保存（ゲート判定） ----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def next_ver():\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx else 1\n",
    "\n",
    "ver = next_ver()\n",
    "csv_path = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(csv_path, index=False, header=False)\n",
    "    print(f\"✅ 提出CSVを保存: {csv_path}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(v7y_stack): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"Year thresholds: {year_th}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2c30311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(v7y_stack2) = 0.591918\n",
      "year thresholds: {2020: 0.785, 2021: 0.755, 2022: 0.755, 2023: 0.715, 2024: 0.64}\n",
      "per-year F1: {2020: 0.59, 2021: 0.663, 2022: 0.611, 2023: 0.577, 2024: 0.519}\n",
      "⛔ 提出しない: macro-F1=0.591918 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y_stack2: KFoldでメタOOF→年ダミー＋Revolver入りLRスタッキング（提出1本） ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 前提チェック\n",
    "need = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "# ---- メタ特徴作成（確率多項式＋交互作用＋Revolver＋Yearダミー） ----\n",
    "def build_meta(proba_cb, proba_lgb, df):\n",
    "    M = pd.DataFrame({\n",
    "        'cb':  np.asarray(proba_cb),\n",
    "        'lgb': np.asarray(proba_lgb),\n",
    "    }, index=df.index)\n",
    "    M['cb2']  = M['cb']**2\n",
    "    M['lgb2'] = M['lgb']**2\n",
    "    M['int']  = M['cb']*M['lgb']\n",
    "    M['rev']  = df['RevolverStatus'].astype(int).to_numpy()\n",
    "    # YearはKFoldなら学習foldにも1/0が混在するためダミーOK\n",
    "    YOH = pd.get_dummies(df['ApprovalFiscalYear'].astype(int), prefix='Y', drop_first=False)\n",
    "    return pd.concat([M, YOH], axis=1)\n",
    "\n",
    "X_meta = build_meta(oof_cb, oof_lgb, train_df)\n",
    "T_meta = build_meta(test_cb, test_lgb, test_df)\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "uniq_years = sorted(np.unique(years))\n",
    "\n",
    "# ---- メタOOF（KFold）を作る ----\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_meta = np.zeros(len(X_meta))\n",
    "Cs = [0.25, 0.5, 1.0]  # ちょい強め正則化寄り\n",
    "for tr_idx, va_idx in skf.split(X_meta, y_true):\n",
    "    Xtr, Xva = X_meta.iloc[tr_idx], X_meta.iloc[va_idx]\n",
    "    ytr      = y_true[tr_idx]\n",
    "    # 簡易グリッド（小さめCが安定しやすい）\n",
    "    best_lr, best_ll = None, 1e9\n",
    "    for C in Cs:\n",
    "        lr = LogisticRegression(C=C, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "        lr.fit(Xtr, ytr)\n",
    "        # 対数損失で雑に選択（校正器っぽく）\n",
    "        p = lr.predict_proba(Xva)[:,1].clip(1e-6, 1-1e-6)\n",
    "        ll = -np.mean(y_true[va_idx]*np.log(p) + (1-y_true[va_idx])*np.log(1-p))\n",
    "        if ll < best_ll:\n",
    "            best_ll, best_lr = ll, lr\n",
    "    oof_meta[va_idx] = best_lr.predict_proba(Xva)[:,1]\n",
    "\n",
    "# ---- 年別に閾値最適化（0.20〜0.80で探索） ----\n",
    "THS = np.arange(0.20, 0.801, 0.005)\n",
    "year_th, year_f1 = {}, {}\n",
    "for yy in uniq_years:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    p = oof_meta[idx]; y = y_true[idx]\n",
    "    f1s = [f1_score(y, (p>=t).astype(int)) for t in THS]\n",
    "    j = int(np.argmax(f1s))\n",
    "    year_th[yy] = float(THS[j])\n",
    "    year_f1[yy] = float(f1s[j])\n",
    "macro_f1 = float(np.mean(list(year_f1.values())))\n",
    "print(f\"🔎 OOF macro-F1(v7y_stack2) = {macro_f1:.6f}\")\n",
    "print(\"year thresholds:\", {int(k): round(v,3) for k,v in year_th.items()})\n",
    "print(\"per-year F1:\", {int(k): round(v,3) for k,v in year_f1.items()})\n",
    "\n",
    "# ---- メタ器を全データで再学習 → テスト予測 → 年別閾値適用 ----\n",
    "# 5-foldで一番良かったCを再推定（全体で最良C選び直し）\n",
    "bestC, bestLL = None, 1e9\n",
    "for C in Cs:\n",
    "    lr = LogisticRegression(C=C, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "    lr.fit(X_meta, y_true)\n",
    "    p = lr.predict_proba(X_meta)[:,1].clip(1e-6, 1-1e-6)\n",
    "    ll = -np.mean(y_true*np.log(p) + (1-y_true)*np.log(1-p))\n",
    "    if ll < bestLL:\n",
    "        bestLL, bestC = ll, C\n",
    "final_lr = LogisticRegression(C=bestC, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "final_lr.fit(X_meta, y_true)\n",
    "\n",
    "test_proba = final_lr.predict_proba(T_meta)[:,1]\n",
    "ty = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "y_pred_test = np.zeros(len(test_proba), dtype=int)\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    th = year_th.get(yy, 0.5)\n",
    "    y_pred_test[jdx] = (test_proba[jdx] >= th).astype(int)\n",
    "\n",
    "# ---- 保存（ゲート判定 & 連番採番） ----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "def next_ver():\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx else 1\n",
    "\n",
    "ver = next_ver()\n",
    "csv_path = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(csv_path, index=False, header=False)\n",
    "    print(f\"✅ 提出CSVを保存: {csv_path}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(v7y_stack2): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"Year thresholds: {year_th}\\n\")\n",
    "    f.write(f\"Per-year F1: {year_f1}\\n\")\n",
    "    f.write(f\"Best C: {bestC}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ac40b4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(v7y_meta_loyo) = 0.606119\n",
      "year thresholds: {2020: 0.73, 2021: 0.64, 2022: 0.795, 2023: 0.735, 2024: 0.78}\n",
      "per-year F1: {2020: 0.591, 2021: 0.668, 2022: 0.609, 2023: 0.591, 2024: 0.571}\n",
      "⛔ 提出しない: macro-F1=0.606119 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y_meta_loyo: 年ごとに別LRでメタ校正（LOYO）→ 年別しきい値 → 提出1本 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- 前提チェック ----\n",
    "need = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "uniq_years = sorted(np.unique(years))\n",
    "ty     = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "\n",
    "def ranknorm(a):\n",
    "    r = np.argsort(np.argsort(a))\n",
    "    return r.astype(float) / max(len(a)-1, 1)\n",
    "\n",
    "def safe_logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "# ---- メタ特徴（固定） ----\n",
    "def build_meta(p_cb, p_lgb):\n",
    "    p_cb  = np.asarray(p_cb);  p_lgb = np.asarray(p_lgb)\n",
    "    rn_cb = ranknorm(p_cb);    rn_lg = ranknorm(p_lgb)\n",
    "    X = np.column_stack([\n",
    "        p_cb, p_lgb,\n",
    "        p_cb**2, p_lgb**2, p_cb*p_lgb,\n",
    "        rn_cb, rn_lg,\n",
    "        safe_logit(p_cb), safe_logit(p_lgb),\n",
    "        (p_cb - p_lgb), np.abs(p_cb - p_lgb),\n",
    "        0.5*p_cb + 0.5*p_lgb\n",
    "    ])\n",
    "    return X\n",
    "\n",
    "X_meta = build_meta(oof_cb,  oof_lgb)\n",
    "T_meta = build_meta(test_cb, test_lgb)\n",
    "\n",
    "# ---- LOYO: 年ごとにメタLRを別学習 → その年のOOFを生成（リーク無し） ----\n",
    "oof_meta = np.zeros(len(X_meta))\n",
    "year_models = {}\n",
    "Cs = [0.25, 0.5, 1.0]   # 強め正則化で安定化\n",
    "\n",
    "for yy in uniq_years:\n",
    "    tr_idx = np.where(years != yy)[0]\n",
    "    va_idx = np.where(years == yy)[0]\n",
    "\n",
    "    # 簡易グリッドで C を選択\n",
    "    best_lr, best_ll = None, 1e9\n",
    "    for C in Cs:\n",
    "        lr = LogisticRegression(C=C, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "        lr.fit(X_meta[tr_idx], y_true[tr_idx])\n",
    "        p = lr.predict_proba(X_meta[tr_idx])[:,1].clip(1e-6, 1-1e-6)\n",
    "        # 対数損失でモデル選択（過度なF1最適より安定）\n",
    "        ll = -np.mean(y_true[tr_idx]*np.log(p) + (1-y_true[tr_idx])*np.log(1-p))\n",
    "        if ll < best_ll:\n",
    "            best_ll, best_lr = ll, lr\n",
    "\n",
    "    # その年のOOFを作る\n",
    "    oof_meta[va_idx] = best_lr.predict_proba(X_meta[va_idx])[:,1]\n",
    "    year_models[yy]  = best_lr  # 後でテストにも流用\n",
    "\n",
    "# ---- 年別しきい値最適化（0.20〜0.80） ----\n",
    "THS = np.arange(0.20, 0.801, 0.005)\n",
    "year_th, year_f1 = {}, {}\n",
    "for yy in uniq_years:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    p = oof_meta[idx]; y = y_true[idx]\n",
    "    f1s = [f1_score(y, (p>=t).astype(int)) for t in THS]\n",
    "    j = int(np.argmax(f1s))\n",
    "    year_th[yy] = float(THS[j])\n",
    "    year_f1[yy] = float(f1s[j])\n",
    "\n",
    "macro_f1 = float(np.mean(list(year_f1.values())))\n",
    "print(f\"🔎 OOF macro-F1(v7y_meta_loyo) = {macro_f1:.6f}\")\n",
    "print(\"year thresholds:\", {int(k): round(v,3) for k,v in year_th.items()})\n",
    "print(\"per-year F1:\",    {int(k): round(v,3) for k,v in year_f1.items()})\n",
    "\n",
    "# ---- テスト予測：年ごとに学習済みメタLRを適用 → 年別しきい値で2値化 ----\n",
    "test_proba = np.zeros(len(T_meta))\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    if len(jdx)==0: \n",
    "        continue\n",
    "    lr = year_models[yy]\n",
    "    test_proba[jdx] = lr.predict_proba(T_meta[jdx])[:,1]\n",
    "\n",
    "y_pred_test = np.zeros(len(test_proba), dtype=int)\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    th  = year_th.get(yy, 0.5)\n",
    "    y_pred_test[jdx] = (test_proba[jdx] >= th).astype(int)\n",
    "\n",
    "# ---- 保存（ゲート判定 & 連番採番） ----\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def next_ver():\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx else 1\n",
    "\n",
    "ver = next_ver()\n",
    "csv_path = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(csv_path, index=False, header=False)\n",
    "    print(f\"✅ 提出CSVを保存: {csv_path}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(v7y_meta_loyo): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"Year thresholds: {year_th}\\n\")\n",
    "    f.write(f\"Per-year F1: {year_f1}\\n\")\n",
    "    f.write(f\"Used Cs: {Cs}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec251169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(v7y_meta_loyo_z) = 0.616492\n",
      "year thresholds: {2020: 0.74, 2021: 0.695, 2022: 0.785, 2023: 0.745, 2024: 0.77}\n",
      "per-year F1: {2020: 0.579, 2021: 0.655, 2022: 0.608, 2023: 0.605, 2024: 0.636}\n",
      "⛔ 提出しない: macro-F1=0.616492 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y_meta_loyo_z: cb/lgb確率 + 年内zスコア特徴で年別LRメタ → 年別閾値 → 提出1本 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "uniq_years = sorted(np.unique(years))\n",
    "ty     = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "\n",
    "# --- メタに使う「年内zスコア」の元となる数値特徴（存在するものだけ使う） ---\n",
    "num_candidates = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2'\n",
    "]\n",
    "num_cols = [c for c in num_candidates if c in train_df.columns and c in test_df.columns]\n",
    "\n",
    "# 年ごとに z = (x - mean_y) / std_y を作る（std=0 なら 0 にする）\n",
    "def yearwise_z(train, test, cols, year_col='ApprovalFiscalYear'):\n",
    "    tr = train[[year_col] + cols].copy()\n",
    "    te = test [[year_col] + cols].copy()\n",
    "    g = tr.groupby(year_col)\n",
    "    means = g[cols].mean()\n",
    "    stds  = g[cols].std().replace(0, np.nan)\n",
    "    # train\n",
    "    tr_z = []\n",
    "    for yy, dfy in tr.groupby(year_col):\n",
    "        mu = means.loc[yy]; sd = stds.loc[yy]\n",
    "        z  = (dfy[cols] - mu) / sd\n",
    "        tr_z.append(z.fillna(0.0))\n",
    "    tr_z = pd.concat(tr_z).sort_index()\n",
    "    tr_z.columns = [c+'_zY' for c in cols]\n",
    "    # test（年が学習側に無ければ全体平均で代用）\n",
    "    te_z = []\n",
    "    for yy, dfy in te.groupby(year_col):\n",
    "        if yy in means.index:\n",
    "            mu = means.loc[yy]; sd = stds.loc[yy]\n",
    "        else:\n",
    "            mu = train[cols].mean(); sd = train[cols].std().replace(0, np.nan)\n",
    "        z  = (dfy[cols] - mu) / sd\n",
    "        te_z.append(z.fillna(0.0))\n",
    "    te_z = pd.concat(te_z).sort_index()\n",
    "    te_z.columns = [c+'_zY' for c in cols]\n",
    "    return tr_z, te_z\n",
    "\n",
    "tr_z, te_z = yearwise_z(train_df, test_df, num_cols)\n",
    "\n",
    "def ranknorm(a):\n",
    "    a = np.asarray(a)\n",
    "    r = np.argsort(np.argsort(a))\n",
    "    return r.astype(float) / max(len(a)-1, 1)\n",
    "\n",
    "def safe_logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "# --- メタ特徴: cb/lgb 確率 + その非線形 + rank + logit + 差分 + 年内z ---\n",
    "def build_meta(p_cb, p_lgb, z_df):\n",
    "    p_cb  = np.asarray(p_cb);  p_lgb = np.asarray(p_lgb)\n",
    "    rn_cb = ranknorm(p_cb);    rn_lg = ranknorm(p_lgb)\n",
    "    base = np.column_stack([\n",
    "        p_cb, p_lgb,\n",
    "        p_cb**2, p_lgb**2, p_cb*p_lgb,\n",
    "        rn_cb, rn_lg,\n",
    "        safe_logit(p_cb), safe_logit(p_lgb),\n",
    "        (p_cb - p_lgb), np.abs(p_cb - p_lgb),\n",
    "        0.5*p_cb + 0.5*p_lgb\n",
    "    ])\n",
    "    if z_df is not None and len(z_df)>0:\n",
    "        return np.column_stack([base, z_df.to_numpy(dtype=float)])\n",
    "    return base\n",
    "\n",
    "X_meta = build_meta(oof_cb,  oof_lgb, tr_z)\n",
    "T_meta = build_meta(test_cb, test_lgb, te_z)\n",
    "\n",
    "# --- LOYO: 年ごとに正則化LRを学習（グリッドでC選択） ---\n",
    "oof_meta = np.zeros(len(X_meta))\n",
    "year_models = {}\n",
    "Cs = [0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "for yy in uniq_years:\n",
    "    tr_idx = np.where(years != yy)[0]\n",
    "    va_idx = np.where(years == yy)[0]\n",
    "    best_lr, best_ll = None, 1e9\n",
    "    for C in Cs:\n",
    "        lr = LogisticRegression(C=C, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "        lr.fit(X_meta[tr_idx], y_true[tr_idx])\n",
    "        p = lr.predict_proba(X_meta[tr_idx])[:,1].clip(1e-6, 1-1e-6)\n",
    "        ll = -np.mean(y_true[tr_idx]*np.log(p) + (1-y_true[tr_idx])*np.log(1-p))\n",
    "        if ll < best_ll:\n",
    "            best_ll, best_lr = ll, lr\n",
    "    oof_meta[va_idx] = best_lr.predict_proba(X_meta[va_idx])[:,1]\n",
    "    year_models[yy]  = best_lr\n",
    "\n",
    "# --- 年別しきい値最適化（0.20〜0.80） ---\n",
    "THS = np.arange(0.20, 0.801, 0.005)\n",
    "year_th, year_f1 = {}, {}\n",
    "for yy in uniq_years:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    p = oof_meta[idx]; y = y_true[idx]\n",
    "    f1s = [f1_score(y, (p>=t).astype(int)) for t in THS]\n",
    "    j = int(np.argmax(f1s))\n",
    "    year_th[yy] = float(THS[j])\n",
    "    year_f1[yy] = float(f1s[j])\n",
    "\n",
    "macro_f1 = float(np.mean(list(year_f1.values())))\n",
    "print(f\"🔎 OOF macro-F1(v7y_meta_loyo_z) = {macro_f1:.6f}\")\n",
    "print(\"year thresholds:\", {int(k): round(v,3) for k,v in year_th.items()})\n",
    "print(\"per-year F1:\",    {int(k): round(v,3) for k,v in year_f1.items()})\n",
    "\n",
    "# --- テスト予測（年別LR→年別しきい値） ---\n",
    "test_proba = np.zeros(len(T_meta))\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    if len(jdx)==0: \n",
    "        continue\n",
    "    lr = year_models[yy]\n",
    "    test_proba[jdx] = lr.predict_proba(T_meta[jdx])[:,1]\n",
    "\n",
    "y_pred_test = np.zeros(len(test_proba), dtype=int)\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    th  = year_th.get(yy, 0.5)\n",
    "    y_pred_test[jdx] = (test_proba[jdx] >= th).astype(int)\n",
    "\n",
    "# --- 保存（ゲート判定 & 連番採番） ---\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def next_ver():\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx else 1\n",
    "\n",
    "ver = next_ver()\n",
    "csv_path = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(csv_path, index=False, header=False)\n",
    "    print(f\"✅ 提出CSVを保存: {csv_path}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(v7y_meta_loyo_z): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"Year thresholds: {year_th}\\n\")\n",
    "    f.write(f\"Per-year F1: {year_f1}\\n\")\n",
    "    f.write(f\"num_cols(z): {num_cols}\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "127e4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 OOF macro-F1(v7y_meta_adv) = 0.617264\n",
      "year thresholds: {2020: 0.74, 2021: 0.64, 2022: 0.785, 2023: 0.745, 2024: 0.77}\n",
      "per-year F1: {2020: 0.579, 2021: 0.658, 2022: 0.608, 2023: 0.605, 2024: 0.636}\n",
      "⛔ 提出しない: macro-F1=0.617264 < gate 0.640\n",
      "📝 ログ保存: C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\\run_A_v46.txt\n"
     ]
    }
   ],
   "source": [
    "# === A-line v7y_meta_adv: 年内z特徴で「train vs test」識別 → サンプル重み作成 → 年別LRメタ + 年別閾値 ===\n",
    "import numpy as np, pandas as pd, re\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "need = ['oof_cb','oof_lgb','test_cb','test_lgb','y_all','train_df','test_df','DATA_DIR']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "assert not miss, f\"未定義: {miss}\"\n",
    "\n",
    "y_true = np.asarray(y_all).astype(int)\n",
    "years  = train_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "uniq_years = sorted(np.unique(years))\n",
    "ty     = test_df['ApprovalFiscalYear'].astype(int).to_numpy()\n",
    "\n",
    "# --- 年内zスコア用の数値候補（存在確認して使う） ---\n",
    "num_candidates = [\n",
    "    'GrossApproval_log1p','SBAGuaranteedApproval_log1p','JobsSupported_log1p',\n",
    "    'InitialInterestRate','TermInMonths','ratio1','ratio2','interact1','interact2'\n",
    "]\n",
    "num_cols = [c for c in num_candidates if c in train_df.columns and c in test_df.columns]\n",
    "\n",
    "def yearwise_z(train, test, cols, year_col='ApprovalFiscalYear'):\n",
    "    tr = train[[year_col] + cols].copy()\n",
    "    te = test [[year_col] + cols].copy()\n",
    "    g = tr.groupby(year_col)\n",
    "    means = g[cols].mean()\n",
    "    stds  = g[cols].std().replace(0, np.nan)\n",
    "\n",
    "    # train\n",
    "    tr_z = []\n",
    "    for yy, dfy in tr.groupby(year_col):\n",
    "        mu = means.loc[yy]; sd = stds.loc[yy]\n",
    "        z  = (dfy[cols] - mu) / sd\n",
    "        tr_z.append(z.fillna(0.0))\n",
    "    tr_z = pd.concat(tr_z).sort_index(); tr_z.columns = [c+'_zY' for c in cols]\n",
    "\n",
    "    # test（未知年は全体統計で代用）\n",
    "    te_z = []\n",
    "    for yy, dfy in te.groupby(year_col):\n",
    "        if yy in means.index:\n",
    "            mu = means.loc[yy]; sd = stds.loc[yy]\n",
    "        else:\n",
    "            mu = train[cols].mean(); sd = train[cols].std().replace(0, np.nan)\n",
    "        z  = (dfy[cols] - mu) / sd\n",
    "        te_z.append(z.fillna(0.0))\n",
    "    te_z = pd.concat(te_z).sort_index(); te_z.columns = [c+'_zY' for c in cols]\n",
    "    return tr_z, te_z\n",
    "\n",
    "tr_z, te_z = yearwise_z(train_df, test_df, num_cols)\n",
    "\n",
    "def ranknorm(a):\n",
    "    a = np.asarray(a); r = np.argsort(np.argsort(a))\n",
    "    return r.astype(float) / max(len(a)-1, 1)\n",
    "\n",
    "def safe_logit(p):\n",
    "    p = np.clip(p, 1e-6, 1-1e-6)\n",
    "    return np.log(p/(1-p))\n",
    "\n",
    "def build_meta(p_cb, p_lgb, z_df):\n",
    "    p_cb  = np.asarray(p_cb);  p_lgb = np.asarray(p_lgb)\n",
    "    rn_cb = ranknorm(p_cb);    rn_lg = ranknorm(p_lgb)\n",
    "    base = np.column_stack([\n",
    "        p_cb, p_lgb,\n",
    "        p_cb**2, p_lgb**2, p_cb*p_lgb,\n",
    "        rn_cb, rn_lg,\n",
    "        safe_logit(p_cb), safe_logit(p_lgb),\n",
    "        (p_cb - p_lgb), np.abs(p_cb - p_lgb),\n",
    "        0.5*p_cb + 0.5*p_lgb\n",
    "    ])\n",
    "    if z_df is not None and len(z_df)>0:\n",
    "        return np.column_stack([base, z_df.to_numpy(dtype=float)])\n",
    "    return base\n",
    "\n",
    "X_meta = build_meta(oof_cb,  oof_lgb, tr_z)\n",
    "T_meta = build_meta(test_cb, test_lgb, te_z)\n",
    "\n",
    "# ===== ① Adversarial reweighting（train=0 / test=1 を当てる識別器）=====\n",
    "# ここは「z特徴だけ」でOK（確率は使わない：リーク/依存を避ける）\n",
    "if len(tr_z.columns) == 0:\n",
    "    # 最低限の数値が無い場合は meta の base 次元（確率の多項式等）で代用\n",
    "    adv_trainX = pd.DataFrame(X_meta[:, :11])  # p_cb..混合まで\n",
    "    adv_testX  = pd.DataFrame(T_meta[:, :11])\n",
    "else:\n",
    "    adv_trainX = tr_z.reset_index(drop=True).copy()\n",
    "    adv_testX  = te_z.reset_index(drop=True).copy()\n",
    "\n",
    "adv_X = pd.concat([adv_trainX, adv_testX], axis=0).to_numpy(dtype=float)\n",
    "adv_y = np.r_[np.zeros(len(adv_trainX), dtype=int), np.ones(len(adv_testX), dtype=int)]\n",
    "\n",
    "adv = LogisticRegression(C=1.0, max_iter=5000)\n",
    "adv.fit(adv_X, adv_y)\n",
    "p_test_train = adv.predict_proba(adv_trainX)[:,1]  # 各訓練サンプルが「testっぽい」確率\n",
    "\n",
    "alpha = 0.5  # 重みの強さ（0.3〜0.7で調整可）\n",
    "w_train = (p_test_train / np.clip(1 - p_test_train, 1e-6, 1.0)) ** alpha\n",
    "w_train = np.clip(w_train, 0.3, 3.0)\n",
    "w_train = w_train / np.mean(w_train)  # 平均1に揃える\n",
    "\n",
    "# ===== ② LOYOで年別LRメタ（重み付き）→ 年別しきい値 =====\n",
    "oof_meta = np.zeros(len(X_meta))\n",
    "year_models, year_th, year_f1 = {}, {}, {}\n",
    "Cs = [0.1, 0.25, 0.5, 1.0]\n",
    "\n",
    "for yy in uniq_years:\n",
    "    tr_idx = np.where(years != yy)[0]\n",
    "    va_idx = np.where(years == yy)[0]\n",
    "\n",
    "    # Cは学習側で簡易チューニング（負の対数尤度最小）\n",
    "    best_lr, best_ll = None, 1e9\n",
    "    for C in Cs:\n",
    "        lr = LogisticRegression(C=C, class_weight='balanced', max_iter=5000, solver='lbfgs')\n",
    "        lr.fit(X_meta[tr_idx], y_true[tr_idx], sample_weight=w_train[tr_idx])\n",
    "        p = lr.predict_proba(X_meta[tr_idx])[:,1].clip(1e-6, 1-1e-6)\n",
    "        ll = -np.mean(y_true[tr_idx]*np.log(p) + (1-y_true[tr_idx])*np.log(1-p))\n",
    "        if ll < best_ll:\n",
    "            best_ll, best_lr = ll, lr\n",
    "\n",
    "    oof_meta[va_idx] = best_lr.predict_proba(X_meta[va_idx])[:,1]\n",
    "    year_models[yy]  = best_lr\n",
    "\n",
    "# 年別しきい値（0.20〜0.80）最適化\n",
    "THS = np.arange(0.20, 0.801, 0.005)\n",
    "for yy in uniq_years:\n",
    "    idx = np.where(years==yy)[0]\n",
    "    p = oof_meta[idx]; y = y_true[idx]\n",
    "    f1s = [f1_score(y, (p>=t).astype(int)) for t in THS]\n",
    "    j = int(np.argmax(f1s))\n",
    "    year_th[yy] = float(THS[j])\n",
    "    year_f1[yy] = float(f1s[j])\n",
    "\n",
    "macro_f1 = float(np.mean(list(year_f1.values())))\n",
    "print(f\"🔎 OOF macro-F1(v7y_meta_adv) = {macro_f1:.6f}\")\n",
    "print(\"year thresholds:\", {int(k): round(v,3) for k,v in year_th.items()})\n",
    "print(\"per-year F1:\",    {int(k): round(v,3) for k,v in year_f1.items()})\n",
    "\n",
    "# ===== ③ テスト予測（年別LR→年別しきい値）=====\n",
    "test_proba = np.zeros(len(T_meta))\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    if len(jdx)==0: \n",
    "        continue\n",
    "    lr = year_models[yy]\n",
    "    test_proba[jdx] = lr.predict_proba(T_meta[jdx])[:,1]\n",
    "\n",
    "y_pred_test = np.zeros(len(test_proba), dtype=int)\n",
    "for yy in uniq_years:\n",
    "    jdx = np.where(ty==yy)[0]\n",
    "    th  = year_th.get(yy, 0.5)\n",
    "    y_pred_test[jdx] = (test_proba[jdx] >= th).astype(int)\n",
    "\n",
    "# ===== ④ 保存（ゲート判定 & 連番採番）=====\n",
    "OUT_DIR = Path(r\"C:\\Users\\koshihiramatsu\\projects\\MUFJ_competition_2025\\model-proposal_A\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def next_ver():\n",
    "    mx = 0\n",
    "    for p in OUT_DIR.glob(\"submission_A_v*.csv\"):\n",
    "        m = re.search(r\"submission_A_v(\\d+)\\.csv$\", p.name)\n",
    "        if m: mx = max(mx, int(m.group(1)))\n",
    "    return (mx+1) if mx else 1\n",
    "\n",
    "ver = next_ver()\n",
    "csv_path = OUT_DIR / f\"submission_A_v{ver}.csv\"\n",
    "log_path = OUT_DIR / f\"run_A_v{ver}.txt\"\n",
    "\n",
    "gate = 0.640\n",
    "if macro_f1 >= gate:\n",
    "    ids = test_df['id'] if 'id' in test_df.columns else pd.read_csv(Path(DATA_DIR)/'sample_submit.csv', header=None, sep=r\"\\s+\")[0]\n",
    "    pd.DataFrame({'id': ids, 'LoanStatus': y_pred_test}).to_csv(csv_path, index=False, header=False)\n",
    "    print(f\"✅ 提出CSVを保存: {csv_path}\")\n",
    "else:\n",
    "    print(f\"⛔ 提出しない: macro-F1={macro_f1:.6f} < gate {gate:.3f}\")\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"OOF macro-F1(v7y_meta_adv): {macro_f1:.6f}\\n\")\n",
    "    f.write(f\"Year thresholds: {year_th}\\n\")\n",
    "    f.write(f\"Per-year F1: {year_f1}\\n\")\n",
    "    f.write(f\"num_cols(z): {num_cols}\\n\")\n",
    "    f.write(\"Adversarial alpha=0.5, clip[0.3,3.0]\\n\")\n",
    "print(f\"📝 ログ保存: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5fe28758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ X_all/T_all を数値化・重複除去・欠損処理しました。形状: (7552, 18) (7552, 18)\n"
     ]
    }
   ],
   "source": [
    "# --- 補修セル：X_all / T_all を数値化 & 重複列除去 & 欠損/inf処理 ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _sanitize_XT(X_all, T_all):\n",
    "    X = X_all.copy()\n",
    "    T = T_all.copy()\n",
    "\n",
    "    # 1) 非数値（object/string/category など）カラムを拾う\n",
    "    non_num = X.select_dtypes(exclude=[np.number, \"bool\"]).columns.tolist()\n",
    "\n",
    "    for c in non_num:\n",
    "        # 既に「*_code」があるなら生カテゴリ列 c は削除（情報重複の回避）\n",
    "        if c + \"_code\" in X.columns:\n",
    "            X.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "            if c in T.columns:\n",
    "                T.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "        else:\n",
    "            # ない場合は、train+test の結合カテゴリ順で整数コード化\n",
    "            both = pd.concat([X[c], T[c]], axis=0)\n",
    "            cats = pd.Categorical(both).categories\n",
    "            X[c] = pd.Categorical(X[c], categories=cats).codes\n",
    "            T[c] = pd.Categorical(T[c], categories=cats).codes\n",
    "\n",
    "    # 2) 列名の重複があれば除去（LightGBM の \"Feature appears more than one time\" 回避）\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "    T = T.loc[:, ~T.columns.duplicated()]\n",
    "\n",
    "    # 3) inf/NaN のケア\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    T = T.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # 4) 最終チェック：すべて数値か？\n",
    "    bad_X = X.select_dtypes(exclude=[np.number, \"bool\"]).columns.tolist()\n",
    "    bad_T = T.select_dtypes(exclude=[np.number, \"bool\"]).columns.tolist()\n",
    "    assert not bad_X, f\"X に非数値列が残っています: {bad_X}\"\n",
    "    assert not bad_T, f\"T に非数値列が残っています: {bad_T}\"\n",
    "\n",
    "    return X, T\n",
    "\n",
    "# グローバルを書き換え（以降の v7z セルはこの数値化済み X_all/T_all を使う）\n",
    "X_all, T_all = _sanitize_XT(X_all, T_all)\n",
    "print(\"✅ X_all/T_all を数値化・重複除去・欠損処理しました。形状:\", X_all.shape, T_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7cc8eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1362]\tvalid_0's binary_logloss: 0.233878\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[648]\tvalid_0's binary_logloss: 0.290674\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1119]\tvalid_0's binary_logloss: 0.35756\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[788]\tvalid_0's binary_logloss: 0.295943\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1819]\tvalid_0's binary_logloss: 0.171515\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1191]\tvalid_0's binary_logloss: 0.233448\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[698]\tvalid_0's binary_logloss: 0.2934\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1166]\tvalid_0's binary_logloss: 0.360635\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[718]\tvalid_0's binary_logloss: 0.294335\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2049]\tvalid_0's binary_logloss: 0.167669\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1378]\tvalid_0's binary_logloss: 0.232741\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.292181\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1092]\tvalid_0's binary_logloss: 0.357754\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[891]\tvalid_0's binary_logloss: 0.297346\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2359]\tvalid_0's binary_logloss: 0.172997\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1270]\tvalid_0's binary_logloss: 0.23351\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[687]\tvalid_0's binary_logloss: 0.292974\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[911]\tvalid_0's binary_logloss: 0.360174\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1017]\tvalid_0's binary_logloss: 0.29518\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2253]\tvalid_0's binary_logloss: 0.160577\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1307]\tvalid_0's binary_logloss: 0.233593\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[606]\tvalid_0's binary_logloss: 0.291505\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1092]\tvalid_0's binary_logloss: 0.354963\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[859]\tvalid_0's binary_logloss: 0.293455\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2140]\tvalid_0's binary_logloss: 0.172428\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1327]\tvalid_0's binary_logloss: 0.232811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[756]\tvalid_0's binary_logloss: 0.293274\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1201]\tvalid_0's binary_logloss: 0.356524\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1044]\tvalid_0's binary_logloss: 0.296061\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2095]\tvalid_0's binary_logloss: 0.165279\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1377]\tvalid_0's binary_logloss: 0.232544\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[668]\tvalid_0's binary_logloss: 0.293073\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1185]\tvalid_0's binary_logloss: 0.360699\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[779]\tvalid_0's binary_logloss: 0.295608\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2220]\tvalid_0's binary_logloss: 0.177426\n",
      "✅ OOF F1 (rank→Isotonic, global) = 0.601518 | best_th=0.305\n",
      "✅ OOF macro-F1 (yearwise iso+shrink) = 0.595400 | λ=250\n",
      "   per-year F1 = {2020: 0.602, 2021: 0.651, 2022: 0.613, 2023: 0.555, 2024: 0.556} | seeds=[0, 1, 2, 3, 4, 5, 6] | rounds≈1222\n"
     ]
    }
   ],
   "source": [
    "# === LGBM v7r: seeds=7 + rank-normalize + 年別Isotonic(収縮) + 年別しきい値 ===\n",
    "import numpy as np, pandas as pd\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def best_f1_threshold(y_true, proba, lo=0.05, hi=0.95, steps=181):\n",
    "    ths = np.linspace(lo, hi, steps)\n",
    "    f1s = [f1_score(y_true, (proba >= t).astype(int)) for t in ths]\n",
    "    i   = int(np.argmax(f1s))\n",
    "    return float(ths[i]), float(f1s[i])\n",
    "\n",
    "def ranknormalize(a: np.ndarray) -> np.ndarray:\n",
    "    r = np.argsort(np.argsort(a))\n",
    "    return r.astype(float) / max(1, (len(a) - 1))\n",
    "\n",
    "# 依存確認\n",
    "need = ['X_all','T_all','y_all','folds','train','test']\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"特徴作成セルを先に実行してください。欠け: {miss}\")\n",
    "\n",
    "# データ準備\n",
    "y_all_s   = pd.Series(y_all).reset_index(drop=True)\n",
    "X_all_lgb = X_all.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "T_all_lgb = T_all.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "# 重複列ガード\n",
    "X_all_lgb = X_all_lgb.loc[:, ~X_all_lgb.columns.duplicated()].copy()\n",
    "T_all_lgb = T_all_lgb.loc[:, ~T_all_lgb.columns.duplicated()].copy()\n",
    "\n",
    "# パラメータ（Optunaのベースがあるならそれを使う）\n",
    "lgb_base = globals().get('lgb_base', {\n",
    "    'learning_rate': 0.024781124923563432,\n",
    "    'num_leaves': 93,\n",
    "    'max_depth': 12,\n",
    "    'min_child_samples': 45,\n",
    "    'subsample': 0.8476028533782086,\n",
    "    'colsample_bytree': 0.9644797168301948,\n",
    "    'reg_alpha': 1.7250263945237354,\n",
    "    'reg_lambda': 4.982743504628338,\n",
    "    'class_weight': 'balanced',\n",
    "    'verbosity': -1,\n",
    "})\n",
    "seeds = [0,1,2,3,4,5,6]\n",
    "lambda_shrink = 250  # 年別isoを大域isoへ収縮する強さ（大きいほど大域寄り）\n",
    "\n",
    "# ===== 学習（LOYO × seeds）=====\n",
    "oof_accum_raw = np.zeros(len(X_all_lgb))\n",
    "oof_accum_rnk = np.zeros(len(X_all_lgb))\n",
    "test_accum_rnk = np.zeros(len(T_all_lgb))\n",
    "rounds_all = []\n",
    "\n",
    "for sd in seeds:\n",
    "    params = dict(lgb_base); params['random_state'] = sd\n",
    "    fold_test_preds = []\n",
    "    oof_tmp = np.zeros(len(X_all_lgb))  # このseedのOOF確率（生）\n",
    "\n",
    "    for va_idx in folds:\n",
    "        va_idx = np.asarray(va_idx, dtype=int)\n",
    "        tr_idx = np.setdiff1d(np.arange(len(X_all_lgb)), va_idx)\n",
    "\n",
    "        Xtr, Xva = X_all_lgb.iloc[tr_idx], X_all_lgb.iloc[va_idx]\n",
    "        ytr, yva = y_all_s.iloc[tr_idx], y_all_s.iloc[va_idx]\n",
    "\n",
    "        model = LGBMClassifier(**params, n_estimators=4000)\n",
    "        model.fit(\n",
    "            Xtr, ytr.values,\n",
    "            eval_set=[(Xva, yva.values)],\n",
    "            eval_metric='binary_logloss',\n",
    "            callbacks=[early_stopping(stopping_rounds=200), log_evaluation(period=0)]\n",
    "        )\n",
    "        it = int(getattr(model, \"best_iteration_\", 4000))\n",
    "        rounds_all.append(it)\n",
    "\n",
    "        # OOF（生）を蓄積\n",
    "        oof_tmp[va_idx] = model.predict_proba(Xva, num_iteration=it)[:,1]\n",
    "        # test（生）をfold平均→あとでrank平均\n",
    "        fold_test_preds.append(model.predict_proba(T_all_lgb, num_iteration=it)[:,1])\n",
    "\n",
    "    # seed内：OOFをrank化して合算、testはfold平均後にseed間でrank平均\n",
    "    oof_accum_rnk += ranknormalize(oof_tmp) / len(seeds)\n",
    "    oof_accum_raw += oof_tmp / len(seeds)\n",
    "    test_accum_rnk += ranknormalize(np.mean(fold_test_preds, axis=0)) / len(seeds)\n",
    "\n",
    "# ===== Isotonic：大域と年別のハイブリッド =====\n",
    "years = sorted(pd.Series(train['ApprovalFiscalYear']).dropna().unique().tolist())\n",
    "\n",
    "# 大域Isotonic\n",
    "iso_global = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_global.fit(oof_accum_rnk, y_all_s.values)\n",
    "oof_cal_global  = iso_global.transform(oof_accum_rnk)\n",
    "test_cal_global = iso_global.transform(test_accum_rnk)\n",
    "\n",
    "# 年別Isotonic + 収縮\n",
    "oof_cal_seg = np.zeros_like(oof_accum_rnk)\n",
    "test_cal_seg = np.zeros_like(test_accum_rnk)\n",
    "\n",
    "if 'ApprovalFiscalYear' in test.columns:\n",
    "    test_years = test['ApprovalFiscalYear'].values\n",
    "else:\n",
    "    test_years = np.full(len(test_accum_rnk), -1)  # 年情報が無い場合のフォールバック\n",
    "\n",
    "th_by_year = {}\n",
    "f1_by_year = {}\n",
    "for yr in years:\n",
    "    idx = np.where(train['ApprovalFiscalYear'].values == yr)[0]\n",
    "    n   = len(idx)\n",
    "    # 年別iso（rank化OOFに対して）\n",
    "    iso_y = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_y.fit(oof_accum_rnk[idx], y_all_s.values[idx])\n",
    "\n",
    "    # 収縮係数 α = n / (n + λ)\n",
    "    alpha = n / (n + lambda_shrink)\n",
    "    oof_cal_y  = alpha * iso_y.transform(oof_accum_rnk[idx]) + (1 - alpha) * oof_cal_global[idx]\n",
    "    oof_cal_seg[idx] = oof_cal_y\n",
    "\n",
    "    # test側も同じ年は同じ係数でブレンド\n",
    "    tmask = (test_years == yr)\n",
    "    if tmask.any():\n",
    "        test_cal_y = alpha * iso_y.transform(test_accum_rnk[tmask]) + (1 - alpha) * test_cal_global[tmask]\n",
    "        test_cal_seg[tmask] = test_cal_y\n",
    "\n",
    "    # 年別しきい値 & F1\n",
    "    th_y, f1_y = best_f1_threshold(y_all_s.values[idx], oof_cal_y)\n",
    "    th_by_year[int(yr)] = round(th_y, 3)\n",
    "    f1_by_year[int(yr)] = round(f1_y, 3)\n",
    "\n",
    "# （テストで年が未知の行があれば）大域のみで埋める\n",
    "fill_mask = (test_cal_seg == 0) & (test_cal_global > 0)\n",
    "test_cal_seg[fill_mask] = test_cal_global[fill_mask]\n",
    "\n",
    "# 指標表示\n",
    "th_glb, f1_glb = best_f1_threshold(y_all_s.values, oof_cal_global)\n",
    "macro_f1 = float(np.mean(list(f1_by_year.values())))\n",
    "\n",
    "print(f\"✅ OOF F1 (rank→Isotonic, global) = {f1_glb:.6f} | best_th={th_glb:.3f}\")\n",
    "print(f\"✅ OOF macro-F1 (yearwise iso+shrink) = {macro_f1:.6f} | λ={lambda_shrink}\")\n",
    "print(f\"   per-year F1 = {f1_by_year} | seeds={seeds} | rounds≈{int(np.mean(rounds_all))}\")\n",
    "\n",
    "# ==== 後続で使える出力（提出セルから参照）====\n",
    "oof_proba_lgb_cal_year = oof_cal_seg     # 年別校正OOF\n",
    "test_proba_lgb_cal_year= test_cal_seg    # 年別校正TEST\n",
    "year_thresholds_lgb    = th_by_year      # 年別しきい値\n",
    "global_threshold_lgb   = th_glb          # 参考\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
